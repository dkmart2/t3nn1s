{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install -q openpyxl\n",
    "%pip install -q pyarrow"
   ],
   "id": "2b1b048aca7e15d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initial Functions Setup\n",
    "def normalize_name(name):\n",
    "    \"\"\"Normalize tennis player names for matching\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    name = str(name).replace('.', '').lower()\n",
    "    parts = name.split()\n",
    "    if len(parts) < 2:\n",
    "        return name.replace(' ', '_')\n",
    "    if len(parts[-1]) == 1:  # Last part is single letter (first initial)\n",
    "        last_name = parts[-2]\n",
    "        first_initial = parts[-1]\n",
    "    else:  # Handle \"First Lastname\" format\n",
    "        last_name = parts[-1]\n",
    "        first_initial = parts[0][0] if parts[0] else ''\n",
    "    return f\"{last_name}_{first_initial}\"\n",
    "\n",
    "def normalize_jeff_name(name):\n",
    "    \"\"\"Normalize Jeff's player names for matching\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    name = str(name).lower()\n",
    "    parts = name.split()\n",
    "    if len(parts) < 2:\n",
    "        return name.replace(' ', '_')\n",
    "    last_name = parts[-1]\n",
    "    first_initial = parts[0][0] if parts[0] else ''\n",
    "    return f\"{last_name}_{first_initial}\"\n",
    "\n",
    "def normalize_tournament_name(name):\n",
    "    \"\"\"Normalize tournament names\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    name = str(name).lower()\n",
    "    name = name.replace('masters cup', 'masters')\n",
    "    name = name.replace('atp finals', 'masters')\n",
    "    name = name.replace('wta finals', 'masters')\n",
    "    return name.strip()\n",
    "\n",
    "def load_excel_data(file_path):\n",
    "    \"\"\"Load data from Excel file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        if 'Date' not in df.columns:\n",
    "            print(f\"Warning: No Date column in {file_path}\")\n",
    "            return pd.DataFrame()\n",
    "        print(f\"Loaded {len(df)} matches from {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_tournament_tier_weight(tournament_name: str) -> float:\n",
    "    \"\"\"Classify tournament tier and return weight\"\"\"\n",
    "    if pd.isna(tournament_name):\n",
    "        return 0.5\n",
    "    tournament_lower = tournament_name.lower()\n",
    "    if any(slam in tournament_lower for slam in ['roland garros', 'wimbledon', 'australian open', 'us open']):\n",
    "        return 1.0\n",
    "    masters_events = ['indian wells', 'miami', 'monte carlo', 'madrid', 'rome', 'canada', 'cincinnati', 'shanghai', 'paris masters']\n",
    "    if any(masters in tournament_lower for masters in masters_events):\n",
    "        return 0.9\n",
    "    atp500_events = ['stuttgart', 'barcelona', 'hamburg', 'halle', 'queens', 'washington', 'dubai', 'rotterdam']\n",
    "    if any(event in tournament_lower for event in atp500_events):\n",
    "        return 0.7\n",
    "    if any(lower_tier in tournament_lower for lower_tier in ['itf', 'challenger', 'juniors']):\n",
    "        return 0.2\n",
    "    return 0.5\n",
    "\n",
    "def calculate_recency_weight(match_date, reference_date='2025-07-01'):\n",
    "    \"\"\"Calculate exponential decay weight based on match recency\"\"\"\n",
    "    try:\n",
    "        if isinstance(match_date, str):\n",
    "            match_dt = datetime.strptime(match_date, '%Y%m%d')\n",
    "        else:\n",
    "            match_dt = match_date\n",
    "        ref_dt = datetime.strptime(reference_date, '%Y-%m-%d')\n",
    "        days_ago = (ref_dt - match_dt).days\n",
    "        return np.exp(-0.0005 * days_ago)\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def load_jeff_comprehensive_data():\n",
    "    \"\"\"Load all of Jeff's comprehensive tennis data\"\"\"\n",
    "    base_path = os.path.expanduser(\"~/Desktop/data/Jeff 6.14.25\")\n",
    "    data = {'men': {}, 'women': {}}\n",
    "    files = {\n",
    "        'matches': 'charting-{}-matches.csv',\n",
    "        'points_2020s': 'charting-{}-points-2020s.csv',\n",
    "        'overview': 'charting-{}-stats-Overview.csv',\n",
    "        'serve_basics': 'charting-{}-stats-ServeBasics.csv',\n",
    "        'return_outcomes': 'charting-{}-stats-ReturnOutcomes.csv',\n",
    "        'return_depth': 'charting-{}-stats-ReturnDepth.csv',\n",
    "        'key_points_serve': 'charting-{}-stats-KeyPointsServe.csv',\n",
    "        'key_points_return': 'charting-{}-stats-KeyPointsReturn.csv',\n",
    "        'net_points': 'charting-{}-stats-NetPoints.csv',\n",
    "        'rally': 'charting-{}-stats-Rally.csv',\n",
    "        'serve_direction': 'charting-{}-stats-ServeDirection.csv',\n",
    "        'serve_influence': 'charting-{}-stats-ServeInfluence.csv',\n",
    "        'shot_direction': 'charting-{}-stats-ShotDirection.csv',\n",
    "        'shot_dir_outcomes': 'charting-{}-stats-ShotDirOutcomes.csv',\n",
    "        'shot_types': 'charting-{}-stats-ShotTypes.csv',\n",
    "        'snv': 'charting-{}-stats-SnV.csv',\n",
    "        'sv_break_split': 'charting-{}-stats-SvBreakSplit.csv',\n",
    "        'sv_break_total': 'charting-{}-stats-SvBreakTotal.csv'\n",
    "    }\n",
    "\n",
    "    for gender in ['men', 'women']:\n",
    "        gender_path = os.path.join(base_path, gender)\n",
    "        if os.path.exists(gender_path):\n",
    "            for key, filename_template in files.items():\n",
    "                filename = filename_template.format('m' if gender == 'men' else 'w')\n",
    "                file_path = os.path.join(gender_path, filename)\n",
    "                if os.path.exists(file_path):\n",
    "                    df = pd.read_csv(file_path, low_memory=False)\n",
    "                    if 'player' in df.columns:\n",
    "                        df['Player_canonical'] = df['player'].apply(normalize_jeff_name)\n",
    "                    data[gender][key] = df\n",
    "                    print(f\"Loaded {gender}/{filename}: {len(df)} records\")\n",
    "    return data\n",
    "\n",
    "def load_all_tennis_data():\n",
    "    \"\"\"Load tennis data from all years\"\"\"\n",
    "    base_path = os.path.expanduser(\"~/Desktop/data\")\n",
    "    all_data = []\n",
    "\n",
    "    for gender_name, gender_code in [(\"tennisdata_men\", \"M\"), (\"tennisdata_women\", \"W\")]:\n",
    "        gender_path = os.path.join(base_path, gender_name)\n",
    "        if os.path.exists(gender_path):\n",
    "            for year in range(2020, 2026):\n",
    "                file_path = os.path.join(gender_path, f\"{year}_{gender_code.lower()}.xlsx\")\n",
    "                if os.path.exists(file_path):\n",
    "                    df = load_excel_data(file_path)\n",
    "                    if not df.empty and 'Date' in df.columns:\n",
    "                        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "                        df['gender'] = gender_code\n",
    "                        df['year'] = df['Date'].dt.year\n",
    "                        all_data.append(df)\n",
    "\n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ],
   "id": "36b752654a8127c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime, date, timedelta\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import shutil\n",
    "from unidecode import unidecode\n",
    "\n",
    "# API Configuration\n",
    "# --- APIâ€‘Tennis authentication ---\n",
    "API_KEY = \"adfc70491c47895e5fffdc6428bbf36a561989d4bffcfa9ecfba8d91e947b4fb\"\n",
    "BASE = \"https://api.api-tennis.com/tennis/\"\n",
    "\n",
    "def call(method: str, **params):\n",
    "    q = {\"method\": method, \"APIkey\": API_KEY, **params}\n",
    "    r = requests.get(BASE, params=q, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "    if str(j.get(\"error\", \"0\")) != \"0\":\n",
    "        raise RuntimeError(j)\n",
    "    return j[\"result\"]\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"Normalize tennis player names for matching\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    name = str(name).replace('.', '').lower()\n",
    "    parts = name.split()\n",
    "    if len(parts) < 2:\n",
    "        return name.replace(' ', '_')\n",
    "    if len(parts[-1]) == 1:  # Last part is single letter (first initial)\n",
    "        last_name = parts[-2]\n",
    "        first_initial = parts[-1]\n",
    "    else:  # Handle \"First Lastname\" format\n",
    "        last_name = parts[-1]\n",
    "        first_initial = parts[0][0] if parts[0] else ''\n",
    "    return f\"{last_name}_{first_initial}\"\n",
    "\n",
    "def normalize_jeff_name(name):\n",
    "    \"\"\"Normalize Jeff's player names for matching\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    name = str(name).lower()\n",
    "    parts = name.split()\n",
    "    if len(parts) < 2:\n",
    "        return name.replace(' ', '_')\n",
    "    last_name = parts[-1]\n",
    "    first_initial = parts[0][0] if parts[0] else ''\n",
    "    return f\"{last_name}_{first_initial}\"\n",
    "\n",
    "def normalize_tournament_name(name):\n",
    "    \"\"\"Normalize tournament names\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    name = str(name).lower()\n",
    "    name = name.replace('masters cup', 'masters')\n",
    "    name = name.replace('atp finals', 'masters')\n",
    "    name = name.replace('wta finals', 'masters')\n",
    "    return name.strip()\n",
    "\n",
    "def load_excel_data(file_path):\n",
    "    \"\"\"Load data from Excel file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        if 'Date' not in df.columns:\n",
    "            print(f\"Warning: No Date column in {file_path}\")\n",
    "            return pd.DataFrame()\n",
    "        print(f\"Loaded {len(df)} matches from {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_tournament_tier_weight(tournament_name: str) -> float:\n",
    "    \"\"\"Classify tournament tier and return weight\"\"\"\n",
    "    if pd.isna(tournament_name):\n",
    "        return 0.5\n",
    "    tournament_lower = tournament_name.lower()\n",
    "    if any(slam in tournament_lower for slam in ['roland garros', 'wimbledon', 'australian open', 'us open']):\n",
    "        return 1.0\n",
    "    masters_events = ['indian wells', 'miami', 'monte carlo', 'madrid', 'rome', 'canada', 'cincinnati', 'shanghai', 'paris masters']\n",
    "    if any(masters in tournament_lower for masters in masters_events):\n",
    "        return 0.9\n",
    "    atp500_events = ['stuttgart', 'barcelona', 'hamburg', 'halle', 'queens', 'washington', 'dubai', 'rotterdam']\n",
    "    if any(event in tournament_lower for event in atp500_events):\n",
    "        return 0.7\n",
    "    if any(lower_tier in tournament_lower for lower_tier in ['itf', 'challenger', 'juniors']):\n",
    "        return 0.2\n",
    "    return 0.5\n",
    "\n",
    "def calculate_recency_weight(match_date, reference_date='2025-07-01'):\n",
    "    \"\"\"Calculate exponential decay weight based on match recency\"\"\"\n",
    "    try:\n",
    "        if isinstance(match_date, str):\n",
    "            match_dt = datetime.strptime(match_date, '%Y%m%d')\n",
    "        else:\n",
    "            match_dt = match_date\n",
    "        ref_dt = datetime.strptime(reference_date, '%Y-%m-%d')\n",
    "        days_ago = (ref_dt - match_dt).days\n",
    "        return np.exp(-0.0005 * days_ago)\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def load_jeff_comprehensive_data():\n",
    "    \"\"\"Load all of Jeff's comprehensive tennis data\"\"\"\n",
    "    base_path = os.path.expanduser(\"~/Desktop/data/Jeff 6.14.25\")\n",
    "    data = {'men': {}, 'women': {}}\n",
    "    files = {\n",
    "        'matches': 'charting-{}-matches.csv',\n",
    "        'points_2020s': 'charting-{}-points-2020s.csv',\n",
    "        'overview': 'charting-{}-stats-Overview.csv',\n",
    "        'serve_basics': 'charting-{}-stats-ServeBasics.csv',\n",
    "        'return_outcomes': 'charting-{}-stats-ReturnOutcomes.csv',\n",
    "        'return_depth': 'charting-{}-stats-ReturnDepth.csv',\n",
    "        'key_points_serve': 'charting-{}-stats-KeyPointsServe.csv',\n",
    "        'key_points_return': 'charting-{}-stats-KeyPointsReturn.csv',\n",
    "        'net_points': 'charting-{}-stats-NetPoints.csv',\n",
    "        'rally': 'charting-{}-stats-Rally.csv',\n",
    "        'serve_direction': 'charting-{}-stats-ServeDirection.csv',\n",
    "        'serve_influence': 'charting-{}-stats-ServeInfluence.csv',\n",
    "        'shot_direction': 'charting-{}-stats-ShotDirection.csv',\n",
    "        'shot_dir_outcomes': 'charting-{}-stats-ShotDirOutcomes.csv',\n",
    "        'shot_types': 'charting-{}-stats-ShotTypes.csv',\n",
    "        'snv': 'charting-{}-stats-SnV.csv',\n",
    "        'sv_break_split': 'charting-{}-stats-SvBreakSplit.csv',\n",
    "        'sv_break_total': 'charting-{}-stats-SvBreakTotal.csv'\n",
    "    }\n",
    "\n",
    "    for gender in ['men', 'women']:\n",
    "        gender_path = os.path.join(base_path, gender)\n",
    "        if os.path.exists(gender_path):\n",
    "            for key, filename_template in files.items():\n",
    "                filename = filename_template.format('m' if gender == 'men' else 'w')\n",
    "                file_path = os.path.join(gender_path, filename)\n",
    "                if os.path.exists(file_path):\n",
    "                    df = pd.read_csv(file_path, low_memory=False)\n",
    "                    if 'player' in df.columns:\n",
    "                        df['Player_canonical'] = df['player'].apply(normalize_jeff_name)\n",
    "                    data[gender][key] = df\n",
    "                    print(f\"Loaded {gender}/{filename}: {len(df)} records\")\n",
    "    return data\n",
    "\n",
    "def load_all_tennis_data():\n",
    "    \"\"\"Load tennis data from all years\"\"\"\n",
    "    base_path = os.path.expanduser(\"~/Desktop/data\")\n",
    "    all_data = []\n",
    "\n",
    "    for gender_name, gender_code in [(\"tennisdata_men\", \"M\"), (\"tennisdata_women\", \"W\")]:\n",
    "        gender_path = os.path.join(base_path, gender_name)\n",
    "        if os.path.exists(gender_path):\n",
    "            for year in range(2020, 2026):\n",
    "                file_path = os.path.join(gender_path, f\"{year}_{gender_code.lower()}.xlsx\")\n",
    "                if os.path.exists(file_path):\n",
    "                    df = load_excel_data(file_path)\n",
    "                    if not df.empty and 'Date' in df.columns:\n",
    "                        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "                        df['gender'] = gender_code\n",
    "                        df['year'] = df['Date'].dt.year\n",
    "                        all_data.append(df)\n",
    "\n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_fallback_defaults(gender_key):\n",
    "    \"\"\"Fallback defaults when no Jeff data available\"\"\"\n",
    "    base_defaults = {\n",
    "        'serve_pts': 80, 'aces': 6, 'double_faults': 3, 'first_serve_pct': 0.62,\n",
    "        'first_serve_won': 35, 'second_serve_won': 16, 'break_points_saved': 4,\n",
    "        'return_pts_won': 30, 'winners_total': 28, 'winners_fh': 16, 'winners_bh': 12,\n",
    "        'unforced_errors': 28, 'unforced_fh': 16, 'unforced_bh': 12,\n",
    "        'serve_wide_pct': 0.3, 'serve_t_pct': 0.4, 'serve_body_pct': 0.3,\n",
    "        'return_deep_pct': 0.4, 'return_shallow_pct': 0.3, 'return_very_deep_pct': 0.2,\n",
    "        'key_points_serve_won_pct': 0.6, 'key_points_aces_pct': 0.05, 'key_points_first_in_pct': 0.55,\n",
    "        'key_points_return_won_pct': 0.35, 'key_points_return_winners': 0.02,\n",
    "        'net_points_won_pct': 0.65, 'net_winners_pct': 0.3, 'passed_at_net_pct': 0.3,\n",
    "        'rally_server_winners_pct': 0.15, 'rally_server_unforced_pct': 0.2,\n",
    "        'rally_returner_winners_pct': 0.1, 'rally_returner_unforced_pct': 0.25,\n",
    "        'shot_crosscourt_pct': 0.5, 'shot_down_line_pct': 0.25, 'shot_inside_out_pct': 0.15,\n",
    "        'serve_volley_frequency': 0.02, 'serve_volley_success_pct': 0.6,\n",
    "        'return_error_net_pct': 0.1, 'return_error_wide_pct': 0.05,\n",
    "        'aggression_index': 0.5, 'consistency_index': 0.5, 'pressure_performance': 0.5, 'net_game_strength': 0.5\n",
    "    }\n",
    "\n",
    "    if gender_key == 'women':\n",
    "        base_defaults.update({\n",
    "            'serve_pts': 75, 'aces': 4, 'first_serve_pct': 0.60,\n",
    "            'first_serve_won': 32, 'second_serve_won': 15,\n",
    "            'serve_volley_frequency': 0.01, 'net_points_won_pct': 0.60\n",
    "        })\n",
    "\n",
    "    return base_defaults\n",
    "\n",
    "def calculate_comprehensive_weighted_defaults(jeff_data):\n",
    "    \"\"\"Calculate weighted defaults from all Jeff datasets\"\"\"\n",
    "    print(\"Calculating comprehensive weighted defaults from Jeff's data...\")\n",
    "    defaults = {'men': {}, 'women': {}}\n",
    "\n",
    "    for gender in ['men', 'women']:\n",
    "        if gender not in jeff_data:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing {gender}'s comprehensive data...\")\n",
    "        matches_df = jeff_data[gender].get('matches')\n",
    "        if matches_df is None:\n",
    "            print(f\"No matches data for {gender}\")\n",
    "            continue\n",
    "\n",
    "        gender_defaults = {}\n",
    "\n",
    "        # Overview stats - basic serving/returning\n",
    "        if 'overview' in jeff_data[gender]:\n",
    "            overview_df = jeff_data[gender]['overview']\n",
    "            match_totals = overview_df[overview_df['set'] == 'Total'].copy()\n",
    "\n",
    "            # Simple defaults from median values\n",
    "            if len(match_totals) > 0:\n",
    "                gender_defaults.update({\n",
    "                    'serve_pts': float(match_totals['serve_pts'].median()) if 'serve_pts' in match_totals.columns else 77.0,\n",
    "                    'aces': float(match_totals['aces'].median()) if 'aces' in match_totals.columns else 5.0,\n",
    "                    'double_faults': float(match_totals['dfs'].median()) if 'dfs' in match_totals.columns else 3.0,\n",
    "                    'first_serve_pct': 0.62,  # reasonable default\n",
    "                    'first_serve_won': float(match_totals['first_won'].median()) if 'first_won' in match_totals.columns else 35.0,\n",
    "                    'second_serve_won': float(match_totals['second_won'].median()) if 'second_won' in match_totals.columns else 16.0,\n",
    "                    'break_points_saved': float(match_totals['bp_saved'].median()) if 'bp_saved' in match_totals.columns else 4.0,\n",
    "                    'return_pts_won': float(match_totals['return_pts_won'].median()) if 'return_pts_won' in match_totals.columns else 30.0,\n",
    "                    'winners_total': float(match_totals['winners'].median()) if 'winners' in match_totals.columns else 28.0,\n",
    "                    'winners_fh': float(match_totals['winners_fh'].median()) if 'winners_fh' in match_totals.columns else 16.0,\n",
    "                    'winners_bh': float(match_totals['winners_bh'].median()) if 'winners_bh' in match_totals.columns else 12.0,\n",
    "                    'unforced_errors': float(match_totals['unforced'].median()) if 'unforced' in match_totals.columns else 28.0,\n",
    "                    'unforced_fh': float(match_totals['unforced_fh'].median()) if 'unforced_fh' in match_totals.columns else 16.0,\n",
    "                    'unforced_bh': float(match_totals['unforced_bh'].median()) if 'unforced_bh' in match_totals.columns else 12.0,\n",
    "                })\n",
    "\n",
    "        # Add fallback defaults for missing features\n",
    "        fallback = get_fallback_defaults(gender)\n",
    "        for key, value in fallback.items():\n",
    "            if key not in gender_defaults:\n",
    "                gender_defaults[key] = float(value)\n",
    "\n",
    "        defaults[gender] = gender_defaults\n",
    "        print(f\"Calculated defaults for {gender}: {len(gender_defaults)} features\")\n",
    "\n",
    "    return defaults\n",
    "\n",
    "def extract_comprehensive_jeff_features(player_canonical, gender, jeff_data, weighted_defaults=None):\n",
    "    \"\"\"Extract features from all Jeff datasets with Player_canonical checks\"\"\"\n",
    "    gender_key = 'men' if gender == 'M' else 'women'\n",
    "\n",
    "    if gender_key not in jeff_data:\n",
    "        return get_fallback_defaults(gender_key)\n",
    "\n",
    "    if weighted_defaults and gender_key in weighted_defaults:\n",
    "        features = weighted_defaults[gender_key].copy()\n",
    "    else:\n",
    "        features = get_fallback_defaults(gender_key)\n",
    "\n",
    "    # Overview stats\n",
    "    if 'overview' in jeff_data[gender_key]:\n",
    "        overview_df = jeff_data[gender_key]['overview']\n",
    "        if 'Player_canonical' in overview_df.columns:\n",
    "            player_overview = overview_df[\n",
    "                (overview_df['Player_canonical'] == player_canonical) &\n",
    "                (overview_df['set'] == 'Total')\n",
    "            ]\n",
    "\n",
    "            if len(player_overview) > 0:\n",
    "                latest = player_overview.iloc[-1]\n",
    "                serve_pts = latest.get('serve_pts', 80)\n",
    "                if serve_pts > 0:\n",
    "                    features.update({\n",
    "                        'serve_pts': float(serve_pts),\n",
    "                        'aces': float(latest.get('aces', 0)),\n",
    "                        'double_faults': float(latest.get('dfs', 0)),\n",
    "                        'first_serve_pct': float(latest.get('first_in', 0)) / float(serve_pts) if serve_pts > 0 else 0.62,\n",
    "                        'first_serve_won': float(latest.get('first_won', 0)),\n",
    "                        'second_serve_won': float(latest.get('second_won', 0)),\n",
    "                        'break_points_saved': float(latest.get('bp_saved', 0)),\n",
    "                        'return_pts_won': float(latest.get('return_pts_won', 0)),\n",
    "                        'winners_total': float(latest.get('winners', 0)),\n",
    "                        'winners_fh': float(latest.get('winners_fh', 0)),\n",
    "                        'winners_bh': float(latest.get('winners_bh', 0)),\n",
    "                        'unforced_errors': float(latest.get('unforced', 0)),\n",
    "                        'unforced_fh': float(latest.get('unforced_fh', 0)),\n",
    "                        'unforced_bh': float(latest.get('unforced_bh', 0))\n",
    "                    })\n",
    "\n",
    "    return features\n",
    "\n",
    "def clean_data_for_parquet(df):\n",
    "    \"\"\"Clean data to ensure parquet compatibility\"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Convert problematic data types\n",
    "    for col in df_clean.columns:\n",
    "        # Handle mixed types and objects\n",
    "        if df_clean[col].dtype == 'object':\n",
    "            # Try to convert to numeric first\n",
    "            numeric_version = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "            if not numeric_version.isna().all():\n",
    "                df_clean[col] = numeric_version\n",
    "            else:\n",
    "                # Convert to string and handle nulls\n",
    "                df_clean[col] = df_clean[col].astype(str)\n",
    "                df_clean[col] = df_clean[col].replace('nan', pd.NA)\n",
    "                df_clean[col] = df_clean[col].replace('<NA>', pd.NA)\n",
    "\n",
    "        # Handle date columns\n",
    "        if 'date' in col.lower() and df_clean[col].dtype == 'object':\n",
    "            try:\n",
    "                df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Replace infinities\n",
    "        if pd.api.types.is_numeric_dtype(df_clean[col]):\n",
    "            df_clean[col] = df_clean[col].replace([np.inf, -np.inf], pd.NA)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "# CACHE SETUP\n",
    "CACHE_DIR = os.path.expanduser(\"~/Desktop/data/cache\")\n",
    "HD_PATH   = os.path.join(CACHE_DIR, \"historical_data.parquet\")\n",
    "JEFF_PATH = os.path.join(CACHE_DIR, \"jeff_data.pkl\")\n",
    "DEF_PATH  = os.path.join(CACHE_DIR, \"weighted_defaults.pkl\")\n",
    "\n",
    "def safe_save_to_cache(historical_data, jeff_data, weighted_defaults):\n",
    "    \"\"\"Safely save data with proper error handling\"\"\"\n",
    "    print(\"\\n=== SAVING TO CACHE ===\")\n",
    "\n",
    "    # Check cache directory\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # 1. Clean and save historical data\n",
    "        print(\"  Cleaning historical data for parquet...\")\n",
    "        historical_clean = clean_data_for_parquet(historical_data)\n",
    "        print(f\"  Saving historical data: {historical_clean.shape}\")\n",
    "        historical_clean.to_parquet(HD_PATH, index=False, engine='pyarrow')\n",
    "        print(\"  âœ“ Historical data saved\")\n",
    "\n",
    "        # 2. Save Jeff data\n",
    "        print(\"  Saving Jeff data...\")\n",
    "        with open(JEFF_PATH, \"wb\") as f:\n",
    "            pickle.dump(jeff_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"  âœ“ Jeff data saved\")\n",
    "\n",
    "        # 3. Save weighted defaults\n",
    "        print(\"  Saving weighted defaults...\")\n",
    "        with open(DEF_PATH, \"wb\") as f:\n",
    "            pickle.dump(weighted_defaults, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"  âœ“ Weighted defaults saved\")\n",
    "\n",
    "        print(\"âœ“ All data cached successfully\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR saving cache: {e}\")\n",
    "        print(f\"Error details: {type(e).__name__}: {str(e)}\")\n",
    "\n",
    "        # Try alternative save methods\n",
    "        try:\n",
    "            print(\"  Trying alternative CSV save...\")\n",
    "            historical_data.to_csv(HD_PATH.replace('.parquet', '.csv'), index=False)\n",
    "            print(\"  âœ“ Saved as CSV instead\")\n",
    "            return True\n",
    "        except Exception as e2:\n",
    "            print(f\"  CSV save also failed: {e2}\")\n",
    "            return False\n",
    "\n",
    "def generate_comprehensive_historical_all_years_fixed(*, fast: bool = True, n_sample: int = 500):\n",
    "    \"\"\"Fixed version with proper error handling\"\"\"\n",
    "    print(\"=== STARTING DATA GENERATION ===\")\n",
    "\n",
    "    # Step 1: Load Jeff's data\n",
    "    print(\"Step 1: Loading Jeff's comprehensive data...\")\n",
    "    try:\n",
    "        jeff_data = load_jeff_comprehensive_data()\n",
    "        if not jeff_data or ('men' not in jeff_data and 'women' not in jeff_data):\n",
    "            print(\"ERROR: Jeff data loading failed\")\n",
    "            return pd.DataFrame(), {}, {}\n",
    "\n",
    "        print(f\"âœ“ Jeff data loaded successfully\")\n",
    "        print(f\"  - Men's datasets: {len(jeff_data.get('men', {}))}\")\n",
    "        print(f\"  - Women's datasets: {len(jeff_data.get('women', {}))}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading Jeff data: {e}\")\n",
    "        return pd.DataFrame(), {}, {}\n",
    "\n",
    "    # Step 2: Calculate weighted defaults\n",
    "    print(\"Step 2: Calculating weighted defaults...\")\n",
    "    try:\n",
    "        weighted_defaults = calculate_comprehensive_weighted_defaults(jeff_data)\n",
    "        if not weighted_defaults:\n",
    "            print(\"ERROR: Weighted defaults calculation failed\")\n",
    "            return pd.DataFrame(), jeff_data, {}\n",
    "\n",
    "        print(f\"âœ“ Weighted defaults calculated\")\n",
    "        print(f\"  - Men's features: {len(weighted_defaults.get('men', {}))}\")\n",
    "        print(f\"  - Women's features: {len(weighted_defaults.get('women', {}))}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR calculating weighted defaults: {e}\")\n",
    "        return pd.DataFrame(), jeff_data, {}\n",
    "\n",
    "    # Step 3: Load tennis match data\n",
    "    print(\"Step 3: Loading tennis match data...\")\n",
    "    try:\n",
    "        tennis_data = load_all_tennis_data()\n",
    "        if tennis_data.empty:\n",
    "            print(\"ERROR: No tennis data loaded\")\n",
    "            return pd.DataFrame(), jeff_data, weighted_defaults\n",
    "\n",
    "        print(f\"âœ“ Tennis data loaded: {len(tennis_data)} matches\")\n",
    "\n",
    "        # Fast mode for testing\n",
    "        if fast:\n",
    "            total_rows = len(tennis_data)\n",
    "            take = min(n_sample, total_rows)\n",
    "            tennis_data = tennis_data.sample(take, random_state=1).reset_index(drop=True)\n",
    "            print(f\"[FAST MODE] Using sample of {take}/{total_rows} rows\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading tennis data: {e}\")\n",
    "        return pd.DataFrame(), jeff_data, weighted_defaults\n",
    "\n",
    "    # Step 4: Process tennis data\n",
    "    print(\"Step 4: Processing tennis data...\")\n",
    "    try:\n",
    "        # Normalize player names\n",
    "        tennis_data['winner_canonical'] = tennis_data['Winner'].apply(normalize_name)\n",
    "        tennis_data['loser_canonical'] = tennis_data['Loser'].apply(normalize_name)\n",
    "        tennis_data['tournament_canonical'] = tennis_data['Tournament'].apply(normalize_tournament_name)\n",
    "\n",
    "        # Fix dates\n",
    "        tennis_data['Date'] = pd.to_datetime(tennis_data['Date'], errors='coerce')\n",
    "        tennis_data['date'] = tennis_data['Date'].dt.date\n",
    "\n",
    "        # Add odds data - SIMPLIFIED to avoid errors\n",
    "        tennis_data['tennis_data_odds1'] = pd.to_numeric(tennis_data.get('PSW', 0), errors='coerce')\n",
    "        tennis_data['tennis_data_odds2'] = pd.to_numeric(tennis_data.get('PSL', 0), errors='coerce')\n",
    "\n",
    "        # Add ranking difference\n",
    "        if 'WRank' in tennis_data.columns and 'LRank' in tennis_data.columns:\n",
    "            tennis_data['rank_difference'] = abs(pd.to_numeric(tennis_data['WRank'], errors='coerce') -\n",
    "                                                 pd.to_numeric(tennis_data['LRank'], errors='coerce'))\n",
    "\n",
    "        print(f\"âœ“ Tennis data processed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing tennis data: {e}\")\n",
    "        return pd.DataFrame(), jeff_data, weighted_defaults\n",
    "\n",
    "    # Step 5: Add Jeff feature columns\n",
    "    print(\"Step 5: Adding Jeff feature columns...\")\n",
    "    try:\n",
    "        all_jeff_features = [\n",
    "            'serve_pts', 'aces', 'double_faults', 'first_serve_pct', 'first_serve_won',\n",
    "            'second_serve_won', 'break_points_saved', 'return_pts_won',\n",
    "            'winners_total', 'winners_fh', 'winners_bh', 'unforced_errors', 'unforced_fh', 'unforced_bh',\n",
    "            'serve_wide_pct', 'serve_t_pct', 'serve_body_pct',\n",
    "            'return_deep_pct', 'return_shallow_pct', 'return_very_deep_pct',\n",
    "            'key_points_serve_won_pct', 'key_points_aces_pct', 'key_points_first_in_pct',\n",
    "            'key_points_return_won_pct', 'key_points_return_winners',\n",
    "            'net_points_won_pct', 'net_winners_pct', 'passed_at_net_pct',\n",
    "            'rally_server_winners_pct', 'rally_server_unforced_pct',\n",
    "            'rally_returner_winners_pct', 'rally_returner_unforced_pct',\n",
    "            'shot_crosscourt_pct', 'shot_down_line_pct', 'shot_inside_out_pct',\n",
    "            'serve_volley_frequency', 'serve_volley_success_pct',\n",
    "            'return_error_net_pct', 'return_error_wide_pct',\n",
    "            'aggression_index', 'consistency_index', 'pressure_performance', 'net_game_strength'\n",
    "        ]\n",
    "\n",
    "        # Initialize feature columns with proper data types\n",
    "        for feature in all_jeff_features:\n",
    "            tennis_data[f'winner_{feature}'] = pd.Series(dtype='float64')\n",
    "            tennis_data[f'loser_{feature}'] = pd.Series(dtype='float64')\n",
    "\n",
    "        print(f\"âœ“ Added {len(all_jeff_features) * 2} feature columns\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR adding feature columns: {e}\")\n",
    "        return pd.DataFrame(), jeff_data, weighted_defaults\n",
    "\n",
    "    # Step 6: Extract Jeff features\n",
    "    print(\"Step 6: Extracting Jeff features...\")\n",
    "    try:\n",
    "        total_matches = len(tennis_data)\n",
    "        matches_with_jeff_features = 0\n",
    "\n",
    "        # Test feature extraction first\n",
    "        if 'men' in jeff_data and 'overview' in jeff_data['men'] and len(jeff_data['men']['overview']) > 0:\n",
    "            test_player = jeff_data['men']['overview']['Player_canonical'].iloc[0]\n",
    "            test_features = extract_comprehensive_jeff_features(test_player, 'M', jeff_data, weighted_defaults)\n",
    "            print(f\"âœ“ Feature extraction test passed for {test_player}\")\n",
    "            print(f\"  Sample features: serve_pts={test_features.get('serve_pts', 'N/A')}\")\n",
    "\n",
    "        for idx, row in tennis_data.iterrows():\n",
    "            if idx % 100 == 0:  # More frequent updates for small datasets\n",
    "                print(f\"  Processing match {idx}/{total_matches}\")\n",
    "\n",
    "            try:\n",
    "                gender = row['gender']\n",
    "\n",
    "                # Only extract Jeff features for matches before cutoff\n",
    "                if row['date'] <= date(2025, 6, 10):\n",
    "                    winner_features = extract_comprehensive_jeff_features(\n",
    "                        row['winner_canonical'], gender, jeff_data, weighted_defaults\n",
    "                    )\n",
    "                    loser_features = extract_comprehensive_jeff_features(\n",
    "                        row['loser_canonical'], gender, jeff_data, weighted_defaults\n",
    "                    )\n",
    "\n",
    "                    # Assign features with proper error handling\n",
    "                    for feature_name, feature_value in winner_features.items():\n",
    "                        col_name = f'winner_{feature_name}'\n",
    "                        if col_name in tennis_data.columns:\n",
    "                            tennis_data.at[idx, col_name] = feature_value\n",
    "\n",
    "                    for feature_name, feature_value in loser_features.items():\n",
    "                        col_name = f'loser_{feature_name}'\n",
    "                        if col_name in tennis_data.columns:\n",
    "                            tennis_data.at[idx, col_name] = feature_value\n",
    "\n",
    "                    if winner_features and loser_features:\n",
    "                        matches_with_jeff_features += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                if idx < 5:  # Only print first few errors\n",
    "                    print(f\"  Warning: Error processing match {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"âœ“ Jeff features extracted for {matches_with_jeff_features}/{total_matches} matches\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR extracting Jeff features: {e}\")\n",
    "        return pd.DataFrame(), jeff_data, weighted_defaults\n",
    "\n",
    "    print(f\"=== DATA GENERATION COMPLETE ===\")\n",
    "    print(f\"Final data shape: {tennis_data.shape}\")\n",
    "    print(f\"Columns: {len(tennis_data.columns)}\")\n",
    "\n",
    "    return tennis_data, jeff_data, weighted_defaults\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# API-Tennis helpers\n",
    "# ------------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "CACHE_API = Path.home() / \".api_tennis_cache\"\n",
    "CACHE_API.mkdir(exist_ok=True)\n",
    "\n",
    "os.environ[\"API_TENNIS_KEY\"] = \"adfc70491c47895e5fffdc6428bbf36a561989d4bffcfa9ecfba8d91e947b4fb\"\n",
    "API_KEY = os.getenv(\"API_TENNIS_KEY\")\n",
    "BASE = \"https://api.api-tennis.com/tennis/\"\n",
    "\n",
    "def api(method: str, **params):\n",
    "    r = requests.get(\n",
    "        \"https://api.api-tennis.com/tennis/\",\n",
    "        params={\"method\": method, \"APIkey\": API_KEY, **params},\n",
    "        timeout=30\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "    if str(j.get(\"error\", 0)) != \"0\":\n",
    "        raise RuntimeError(j)\n",
    "    return j.get(\"result\", [])\n",
    "\n",
    "def tournaments_lookup():\n",
    "    fn = CACHE_API / \"tournaments.pkl\"\n",
    "    if fn.exists():\n",
    "        return pickle.loads(fn.read_bytes())\n",
    "    res = api(\"get_tournaments\")\n",
    "    fn.write_bytes(pickle.dumps(res, 4))\n",
    "    return res\n",
    "\n",
    "def standings_lookup(day: date, league: str = \"ATP\"):\n",
    "    \"\"\"\n",
    "    Fetch weekly standings from APIâ€‘Tennis.\n",
    "    API expects `event_type` = \"Men\" | \"Women\", **not** `league`.\n",
    "    Response is cached one file per ISOâ€‘week.\n",
    "    \"\"\"\n",
    "    tag = f\"{league}_{day.isocalendar()[0]}_{day.isocalendar()[1]:02d}.pkl\"\n",
    "    fn = CACHE_API / tag\n",
    "    if fn.exists():\n",
    "        return pickle.loads(fn.read_bytes())\n",
    "\n",
    "    event_type = \"Men\" if league.upper() == \"ATP\" else \"Women\"\n",
    "    rows = api(\"get_standings\", event_type=event_type)\n",
    "    fn.write_bytes(pickle.dumps(rows, 4))\n",
    "    return rows\n",
    "\n",
    "\n",
    "# Clear cache first\n",
    "print(\"=== CLEARING CACHE ===\")\n",
    "if os.path.exists(CACHE_DIR):\n",
    "    shutil.rmtree(CACHE_DIR)\n",
    "    print(f\"Removed cache directory: {CACHE_DIR}\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "print(\"Created fresh cache directory\")\n",
    "\n",
    "print(\"Starting data generation...\")\n",
    "historical_data, jeff_data, weighted_defaults = generate_comprehensive_historical_all_years_fixed(\n",
    "    fast=True, n_sample=500\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Step 7 â€“ API-Tennis ingestion   (run AFTER Step 6)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"Step 7: Appending API-Tennis data â€¦\")\n",
    "\n",
    "from pathlib import Path\n",
    "import json, pickle, time\n",
    "\n",
    "# ---------- constants --------------------------------------------------------\n",
    "START_CUTOFF = date(2025, 6, 11)          # first day Jeff is missing\n",
    "ODDS_SWITCH  = date(2025, 6, 23)          # API-Tennis odds priority\n",
    "CACHE_API    = Path.home() / \".api_tennis_cache\"\n",
    "CACHE_API.mkdir(exist_ok=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def fixtures_finished(day: date) -> list[dict]:\n",
    "    return [\n",
    "        ev for ev in api(\"get_fixtures\",\n",
    "                         date_start=day.isoformat(),\n",
    "                         date_stop=day.isoformat(),\n",
    "                         timezone=\"UTC\")\n",
    "        if ev.get(\"event_status\") == \"Finished\"\n",
    "    ]\n",
    "\n",
    "def statistics_block(match_key: int) -> dict:\n",
    "    try:\n",
    "        s = api(\"get_statistics\", match_id=match_key)[0]\n",
    "    except Exception:\n",
    "        return {}\n",
    "    return {\n",
    "        \"aces\"            : int(s.get(\"aces\", 0)),\n",
    "        \"double_faults\"   : int(s.get(\"double_faults\", 0)),\n",
    "        \"first_serve_pct\" : float(s.get(\"first_serve_percentage\", 0))/100,\n",
    "        \"first_serve_won\" : int(s.get(\"first_serve_points_won\", 0)),\n",
    "        \"second_serve_won\": int(s.get(\"second_serve_points_won\", 0)),\n",
    "        \"return_pts_won\"  : int(s.get(\"return_points_won\", 0)),\n",
    "    }\n",
    "\n",
    "def odds_block(match_key: int, day: date) -> tuple[float|None,float|None]:\n",
    "    if day < ODDS_SWITCH:\n",
    "        return (None, None)\n",
    "    try:\n",
    "        mkts = api(\"get_odds\", match_id=match_key)\n",
    "    except Exception:\n",
    "        return (None, None)\n",
    "    mkt = next((m for m in mkts if m[\"odd_name\"] == \"Home/Away\"), None)\n",
    "    if not mkt:\n",
    "        return (None, None)\n",
    "    home, away = None, None\n",
    "    for sel in mkt[\"value\"]:\n",
    "        if sel[\"type\"] == \"Home\":\n",
    "            home = float(sel[\"odd\"])\n",
    "        elif sel[\"type\"] == \"Away\":\n",
    "            away = float(sel[\"odd\"])\n",
    "    return (home, away)\n",
    "\n",
    "def standings_lookup(day: date, league=\"ATP\") -> list[dict]:\n",
    "    tag = f\"{league}_{day.isocalendar()[0]}_{day.isocalendar()[1]:02d}.pkl\"\n",
    "    fn  = CACHE_API / tag\n",
    "    if fn.exists():\n",
    "        return pickle.loads(fn.read_bytes())\n",
    "    rows = api(\"get_standings\", league=league)\n",
    "    fn.write_bytes(pickle.dumps(rows, 4))\n",
    "    return rows\n",
    "\n",
    "def ranking_map(day: date, league: str) -> dict[int,int]:\n",
    "    return {int(r[\"player_key\"]): int(r[\"place\"])\n",
    "            for r in standings_lookup(day, league)}\n",
    "\n",
    "# ---------- H2H cache --------------------------------------------------------\n",
    "def h2h_features(p1_key: int, p2_key: int) -> dict:\n",
    "    tag = CACHE_API / f\"h2h_{p1_key}_{p2_key}.pkl\"\n",
    "    if tag.exists():\n",
    "        return pickle.loads(tag.read_bytes())\n",
    "\n",
    "    try:\n",
    "        blk = api(\"get_H2H\",\n",
    "                  first_player_key=p1_key,\n",
    "                  second_player_key=p2_key)[0]\n",
    "    except Exception:\n",
    "        tag.write_bytes(pickle.dumps({}, 4))\n",
    "        return {}\n",
    "\n",
    "    rows = blk[\"H2H\"] or []\n",
    "    if not rows:\n",
    "        tag.write_bytes(pickle.dumps({}, 4))\n",
    "        return {}\n",
    "\n",
    "    wins1 = sum(ev[\"event_winner\"].startswith(\"First\") for ev in rows)\n",
    "    wins2 = len(rows) - wins1\n",
    "    last_dt = max(pd.to_datetime(ev[\"event_date\"]) for ev in rows)\n",
    "    gap    = (pd.Timestamp.utcnow() - last_dt).days\n",
    "\n",
    "    surf_cnt = {}\n",
    "    for ev in rows:\n",
    "        surf = ev.get(\"court_surface\") or \"Hard\"\n",
    "        key  = (\"p1\" if ev[\"event_winner\"].startswith(\"First\") else \"p2\") + \"_\" + surf\n",
    "        surf_cnt[key] = surf_cnt.get(key, 0) + 1\n",
    "\n",
    "    out = {\n",
    "        \"h2h_played\"   : len(rows),\n",
    "        \"h2h_p1_wins\"  : wins1,\n",
    "        \"h2h_p2_wins\"  : wins2,\n",
    "        \"h2h_gap_days\" : gap,\n",
    "        **surf_cnt\n",
    "    }\n",
    "    tag.write_bytes(pickle.dumps(out, 4))\n",
    "    return out\n",
    "\n",
    "# ---------- streaming loop ---------------------------------------------------\n",
    "# --- ensure event_key column exists ---------------------------------\n",
    "if \"event_key\" not in historical_data.columns:\n",
    "    historical_data[\"event_key\"] = pd.NA\n",
    "existing_keys = set(\n",
    "    historical_data.loc[historical_data[\"date\"] >= START_CUTOFF, \"event_key\"]\n",
    "        .dropna().astype(int)\n",
    ")\n",
    "\n",
    "append_buf = []\n",
    "for d in pd.date_range(START_CUTOFF, date.today()):\n",
    "    day = d.date()\n",
    "    for ev in fixtures_finished(day):\n",
    "        k = int(ev[\"event_key\"])\n",
    "        if k in existing_keys:\n",
    "            continue\n",
    "\n",
    "        p1_name, p2_name = ev[\"event_first_player\"], ev[\"event_second_player\"]\n",
    "        winner = p1_name if ev[\"event_winner\"].startswith(\"First\") else p2_name\n",
    "        loser  = p2_name if winner == p1_name else p1_name\n",
    "\n",
    "        row = {\n",
    "            \"Date\"      : pd.to_datetime(ev[\"event_date\"]),\n",
    "            \"date\"      : pd.to_datetime(ev[\"event_date\"]).date(),\n",
    "            \"event_key\" : k,\n",
    "            \"Tournament\": ev[\"tournament_name\"],\n",
    "            \"round\"     : ev.get(\"tournament_round\", \"\"),\n",
    "            \"Surface\"   : ev.get(\"court_surface\") or \"Hard\",\n",
    "            \"Winner\"    : winner,\n",
    "            \"Loser\"     : loser,\n",
    "            \"score_raw\" : json.dumps(ev.get(\"scores\", \"\")),\n",
    "            \"source_rank\": 1                         # Jeff 0, API 1, tennis-data 2\n",
    "        }\n",
    "\n",
    "        # stats\n",
    "        row.update(statistics_block(k))\n",
    "\n",
    "        # odds\n",
    "        o1, o2 = odds_block(k, day)\n",
    "        row[\"tennis_data_odds1\"] = o1\n",
    "        row[\"tennis_data_odds2\"] = o2\n",
    "\n",
    "        # rankings\n",
    "        league = \"WTA\" if \"wta\" in ev[\"event_type_type\"].lower() else \"ATP\"\n",
    "        rmap   = ranking_map(day, league)\n",
    "        row[\"WRank\"] = rmap.get(int(ev[\"first_player_key\"]),  pd.NA)\n",
    "        row[\"LRank\"] = rmap.get(int(ev[\"second_player_key\"]), pd.NA)\n",
    "\n",
    "        # H2H\n",
    "        row.update(\n",
    "            h2h_features(int(ev[\"first_player_key\"]),\n",
    "                         int(ev[\"second_player_key\"]))\n",
    "        )\n",
    "\n",
    "        append_buf.append(row)\n",
    "        existing_keys.add(k)\n",
    "\n",
    "print(f\"  finished streaming: {len(append_buf)} new rows\")\n",
    "\n",
    "if append_buf:\n",
    "    api_df = pd.DataFrame(append_buf)\n",
    "    api_df[\"Date\"] = pd.to_datetime(api_df[\"Date\"])\n",
    "    api_df[\"date\"] = api_df[\"Date\"].dt.date\n",
    "\n",
    "    HIST_KEY = [\"date\", \"Tournament\", \"Winner\", \"Loser\"]\n",
    "    historical_data = (\n",
    "        pd.concat([historical_data, api_df], ignore_index=True)\n",
    "          .sort_values(\"source_rank\")\n",
    "          .drop_duplicates(subset=HIST_KEY, keep=\"first\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "print(\"API-Tennis merge complete\")\n",
    "\n",
    "# Save to cache with error handling\n",
    "if len(historical_data) > 0 and jeff_data and weighted_defaults:\n",
    "    success = safe_save_to_cache(historical_data, jeff_data, weighted_defaults)\n",
    "    if success:\n",
    "        print(\"âœ“ Data generation and caching completed successfully\")\n",
    "    else:\n",
    "        print(\"âœ— Data generation completed but caching failed\")\n",
    "else:\n",
    "    print(\"âœ— Data generation failed\")\n",
    "\n",
    "# Final status\n",
    "print(f\"\\n=== FINAL STATUS ===\")\n",
    "print(f\"Historical data: {historical_data.shape}\")\n",
    "print(f\"Jeff data available: {bool(jeff_data)}\")\n",
    "print(f\"Weighted defaults available: {bool(weighted_defaults)}\")\n",
    "\n",
    "if len(historical_data) > 0:\n",
    "    # Check Jeff features\n",
    "    jeff_cols = [col for col in historical_data.columns if 'winner_serve_pts' in col]\n",
    "    print(f\"Jeff feature columns: {jeff_cols}\")\n",
    "\n",
    "    if 'winner_serve_pts' in historical_data.columns:\n",
    "        non_null_count = historical_data['winner_serve_pts'].notna().sum()\n",
    "        print(f\"Non-null serve_pts values: {non_null_count}\")\n",
    "\n",
    "        if non_null_count > 0:\n",
    "            sample_values = historical_data['winner_serve_pts'].dropna().head(5)\n",
    "            print(f\"Sample serve_pts: {sample_values.tolist()}\")\n",
    "\n",
    "print(\"\\nData generation complete!\")"
   ],
   "id": "2f68f1700366c720",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7e769e487b08c238",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8e9be67d905bd820",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "86e82ea2d8b2cef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "17b91a9560acac3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f26d5bd055e318a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5e3171b097e8a443",
   "metadata": {},
   "source": [
    "# LAYER 1 ##\n",
    "def extract_data_samples():\n",
    "    # Jeff Sackmann data samples\n",
    "    jeff_samples = {\n",
    "        'matches': jeff_data['men']['matches'].head(3),\n",
    "        'serve_basics': jeff_data['men']['serve_basics'].head(3),\n",
    "        'overview': jeff_data['men']['overview'].head(3)\n",
    "    }\n",
    "\n",
    "    # Tennis-data samples\n",
    "    tennis_samples = historical_data[\n",
    "        ['Winner', 'Loser', 'WRank', 'LRank', 'PSW', 'PSL', 'Surface']\n",
    "    ].head(3)\n",
    "\n",
    "    return jeff_samples, tennis_samples\n",
    "\n",
    "# Hold/break computation method verification\n",
    "hold_break_computation = {\n",
    "    'current_method': 'Jeff aggregated stats from overview dataset',\n",
    "    'available_columns': ['serve_pts', 'first_in', 'first_won', 'second_won'],\n",
    "    'computation_level': 'Per-player aggregate from charting data'\n",
    "}\n",
    "\n",
    "# Bayesian\n",
    "def extract_priors_from_current_data(player_canonical, gender, surface):\n",
    "    priors = {}\n",
    "\n",
    "    # Layer 1: Elo approximation from rankings\n",
    "    player_matches = historical_data[\n",
    "        (historical_data['winner_canonical'] == player_canonical) |\n",
    "        (historical_data['loser_canonical'] == player_canonical)\n",
    "    ]\n",
    "\n",
    "    if len(player_matches) > 0:\n",
    "        # Ranking-based Elo estimation\n",
    "        recent_rank = get_recent_rank(player_canonical, player_matches)\n",
    "        elo_estimate = 2000 - (recent_rank * 5) if recent_rank else 1500\n",
    "\n",
    "        # Jeff feature extraction\n",
    "        jeff_features = extract_jeff_features(player_canonical, gender, jeff_data)\n",
    "\n",
    "        priors = {\n",
    "            'elo_estimate': elo_estimate,\n",
    "            'serve_effectiveness': jeff_features.get('serve_pts', 0.6),\n",
    "            'return_strength': jeff_features.get('return_pts_won', 0.3),\n",
    "            'surface_factor': calculate_surface_adjustment(player_matches, surface)\n",
    "        }\n",
    "\n",
    "    return priors\n",
    "\n",
    "# Time decay for recent form\n",
    "def calculate_time_decayed_performance(player_matches, reference_date):\n",
    "    player_matches['days_ago'] = (reference_date - player_matches['date']).dt.days\n",
    "\n",
    "    # Exponential decay: recent matches weighted heavier\n",
    "    weights = np.exp(-0.01 * player_matches['days_ago'])  # 1% daily decay\n",
    "\n",
    "    weighted_performance = {\n",
    "        'win_rate': np.average(player_matches['is_winner'], weights=weights),\n",
    "        'games_won_rate': np.average(player_matches['games_won_pct'], weights=weights)\n",
    "    }\n",
    "\n",
    "    return weighted_performance"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c2cbbdc2ab346155",
   "metadata": {},
   "source": [
    "## TEST ##\n",
    "import os, pickle, pandas as pd\n",
    "\n",
    "CACHE_DIR = os.path.expanduser(\"~/Desktop/data/cache\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "HD_PATH   = os.path.join(CACHE_DIR, \"historical_data.parquet\")\n",
    "JEFF_PATH = os.path.join(CACHE_DIR, \"jeff_data.pkl\")\n",
    "DEF_PATH  = os.path.join(CACHE_DIR, \"weighted_defaults.pkl\")\n",
    "\n",
    "if (os.path.exists(HD_PATH) and\n",
    "    os.path.exists(JEFF_PATH) and\n",
    "    os.path.exists(DEF_PATH)):\n",
    "    print(\"Loading cached data â€¦\")\n",
    "    historical_data = pd.read_parquet(HD_PATH)\n",
    "    with open(JEFF_PATH, \"rb\") as fh:\n",
    "        jeff_data = pickle.load(fh)\n",
    "    with open(DEF_PATH, \"rb\") as fh:\n",
    "        weighted_defaults = pickle.load(fh)\n",
    "else:\n",
    "    print(\"Cache miss â€“ regenerating (one-time slow run).\")\n",
    "    combined_data, jeff_data, weighted_defaults = generate_comprehensive_historical_all_years()\n",
    "    historical_data = combined_data\n",
    "    historical_data.to_parquet(HD_PATH, index=False)\n",
    "    with open(JEFF_PATH, \"wb\") as fh:\n",
    "        pickle.dump(jeff_data, fh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(DEF_PATH, \"wb\") as fh:\n",
    "        pickle.dump(weighted_defaults, fh, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8d16702e22852abc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fb6b2b6eab640f01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "62e256f099bbb5fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8543b131d956f79e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "919a681cbe603498",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\"SIMULATION\"",
   "id": "d75b71c28e131e31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "81ade7c0a80f1583",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "97b8f8ee84bce119",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalize_name_canonical(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    name = str(name).strip()\n",
    "    name = name.replace('.', '').replace(\"'\", '').replace('-', ' ')\n",
    "    return ' '.join(name.lower().split())\n",
    "\n",
    "def extract_jeff_features(player_canonical, gender, jeff_data):\n",
    "    \"\"\"Extract actual features from Jeff Sackmann data\"\"\"\n",
    "    gender_key = 'men' if gender == 'M' else 'women'\n",
    "\n",
    "    if gender_key not in jeff_data or player_canonical not in jeff_data[gender_key]:\n",
    "        return {\n",
    "            'serve_pts': 60,\n",
    "            'first_won': 0,\n",
    "            'second_won': 0,\n",
    "            'return_pts_won': 20\n",
    "        }\n",
    "\n",
    "    player_data = jeff_data[gender_key][player_canonical]\n",
    "\n",
    "    first_in = player_data.get('1stIn', 0)\n",
    "    first_won = player_data.get('1stWon', 0)\n",
    "    second_won = player_data.get('2ndWon', 0)\n",
    "    double_faults = player_data.get('df', 0)\n",
    "\n",
    "    total_serve_pts = first_in + double_faults + (first_won - first_in) if first_won >= first_in else first_in + second_won + double_faults\n",
    "\n",
    "    break_points_saved = player_data.get('bpSaved', 0)\n",
    "    break_points_faced = player_data.get('bpFaced', 0)\n",
    "    return_pts_won = break_points_faced - break_points_saved\n",
    "\n",
    "    return {\n",
    "        'serve_pts': max(1, total_serve_pts),\n",
    "        'first_won': first_won,\n",
    "        'second_won': second_won,\n",
    "        'return_pts_won': max(0, return_pts_won)\n",
    "    }\n",
    "\n",
    "class BayesianTennisModel:\n",
    "    def __init__(self):\n",
    "        self.simulation_count = 10000\n",
    "        self.jeff_data = jeff_data\n",
    "        self.historical_data = historical_data\n",
    "\n",
    "    def default_priors(self):\n",
    "        return {\n",
    "            'elo_mean': 1500,\n",
    "            'elo_std': 200,\n",
    "            'hold_prob': 0.65,\n",
    "            'break_prob': 0.35,\n",
    "            'surface': 'Hard',\n",
    "            'form_factor': 1.0,\n",
    "            'confidence': 0.1\n",
    "        }\n",
    "\n",
    "    def extract_refined_priors(self, player_canonical, gender, surface, reference_date):\n",
    "        player_matches = self.historical_data[\n",
    "            (self.historical_data['winner_canonical'] == player_canonical) |\n",
    "            (self.historical_data['loser_canonical'] == player_canonical)\n",
    "        ].copy()\n",
    "\n",
    "        if len(player_matches) == 0:\n",
    "            return self.default_priors()\n",
    "\n",
    "        surface_matches = player_matches[player_matches['Surface'] == surface]\n",
    "        if len(surface_matches) < 5:\n",
    "            surface_matches = player_matches\n",
    "\n",
    "        recent_matches = surface_matches.tail(20).copy()\n",
    "        recent_matches['days_ago'] = (pd.to_datetime(reference_date) - pd.to_datetime(recent_matches['Date'])).dt.days\n",
    "        weights = np.exp(-0.05 * recent_matches['days_ago'])\n",
    "\n",
    "        base_elo = self.get_player_weighted_elo(player_canonical, surface, reference_date)\n",
    "        surface_factor = self.calculate_surface_adaptation(player_canonical, surface)\n",
    "        elo_prior = base_elo * surface_factor\n",
    "\n",
    "        jeff_features = extract_jeff_features(player_canonical, gender, self.jeff_data)\n",
    "\n",
    "        serve_pts = jeff_features['serve_pts']\n",
    "        serve_won = jeff_features['first_won'] + jeff_features['second_won']\n",
    "        hold_prob = serve_won / serve_pts if serve_pts > 0 else 0.65\n",
    "\n",
    "        return_pts = jeff_features['return_pts_won']\n",
    "        total_return_pts = serve_pts\n",
    "        break_prob = (1 - return_pts / total_return_pts) if total_return_pts > 0 else 0.35\n",
    "\n",
    "        return {\n",
    "            'elo_mean': elo_prior,\n",
    "            'elo_std': 150,\n",
    "            'hold_prob': min(0.95, max(0.3, hold_prob)),\n",
    "            'break_prob': max(0.05, min(0.7, break_prob)),\n",
    "            'surface': surface,\n",
    "            'form_factor': self.calculate_form_spike(recent_matches, weights, player_canonical),\n",
    "            'confidence': max(0.05, min(1.0, len(recent_matches) / 15))\n",
    "        }\n",
    "\n",
    "    def calculate_ranking_differential_odds(self, p1_ranking, p2_ranking):\n",
    "        \"\"\"Convert ranking differential to implied probability\"\"\"\n",
    "        if p1_ranking == 0 or p2_ranking == 0:\n",
    "            return 0.5\n",
    "\n",
    "        ranking_diff = p2_ranking - p1_ranking\n",
    "\n",
    "        if ranking_diff > 50:\n",
    "            return 0.85\n",
    "        elif ranking_diff > 20:\n",
    "            return 0.75\n",
    "        elif ranking_diff > 10:\n",
    "            return 0.65\n",
    "        elif ranking_diff > 0:\n",
    "            return 0.55\n",
    "        elif ranking_diff > -10:\n",
    "            return 0.45\n",
    "        elif ranking_diff > -20:\n",
    "            return 0.35\n",
    "        elif ranking_diff > -50:\n",
    "            return 0.25\n",
    "        else:\n",
    "            return 0.15\n",
    "\n",
    "    def calculate_upset_frequency(self, ranking_diff, surface, historical_data):\n",
    "        \"\"\"Calculate upset frequency by ranking differential and surface\"\"\"\n",
    "        upset_matches = historical_data[\n",
    "            ((historical_data['WRank'] - historical_data['LRank']) > ranking_diff) &\n",
    "            (historical_data['Surface'] == surface)\n",
    "        ]\n",
    "\n",
    "        total_matches = historical_data[\n",
    "            (abs(historical_data['WRank'] - historical_data['LRank']) >= abs(ranking_diff)) &\n",
    "            (historical_data['Surface'] == surface)\n",
    "        ]\n",
    "\n",
    "        if len(total_matches) < 10 and surface != 'fallback':\n",
    "            return self.calculate_upset_frequency(ranking_diff, 'fallback', historical_data)\n",
    "\n",
    "        if surface == 'fallback':\n",
    "            upset_matches = historical_data[\n",
    "                (historical_data['WRank'] - historical_data['LRank']) > ranking_diff\n",
    "            ]\n",
    "            total_matches = historical_data[\n",
    "                abs(historical_data['WRank'] - historical_data['LRank']) >= abs(ranking_diff)\n",
    "            ]\n",
    "\n",
    "        if len(total_matches) == 0:\n",
    "            return 0.1\n",
    "\n",
    "        upset_rate = len(upset_matches) / len(total_matches)\n",
    "        return min(0.45, max(0.05, upset_rate))\n",
    "\n",
    "    def calculate_surface_performance_ratio(self, player_canonical, surface, opponent_canonical, reference_date):\n",
    "        \"\"\"Calculate player's surface-specific performance vs opponent's baseline\"\"\"\n",
    "        player_surface_matches = self.historical_data[\n",
    "            ((self.historical_data['winner_canonical'] == player_canonical) |\n",
    "             (self.historical_data['loser_canonical'] == player_canonical)) &\n",
    "            (self.historical_data['Surface'] == surface) &\n",
    "            (pd.to_datetime(self.historical_data['Date']) <= pd.to_datetime(reference_date))\n",
    "        ].tail(20)\n",
    "\n",
    "        opponent_surface_matches = self.historical_data[\n",
    "            ((self.historical_data['winner_canonical'] == opponent_canonical) |\n",
    "             (self.historical_data['loser_canonical'] == opponent_canonical)) &\n",
    "            (self.historical_data['Surface'] == surface) &\n",
    "            (pd.to_datetime(self.historical_data['Date']) <= pd.to_datetime(reference_date))\n",
    "        ].tail(20)\n",
    "\n",
    "        if len(player_surface_matches) < 3 or len(opponent_surface_matches) < 3:\n",
    "            return 1.0\n",
    "\n",
    "        player_wins = len(player_surface_matches[player_surface_matches['winner_canonical'] == player_canonical])\n",
    "        opponent_wins = len(opponent_surface_matches[opponent_surface_matches['winner_canonical'] == opponent_canonical])\n",
    "\n",
    "        player_ratio = player_wins / len(player_surface_matches)\n",
    "        opponent_ratio = opponent_wins / len(opponent_surface_matches)\n",
    "\n",
    "        return player_ratio / opponent_ratio if opponent_ratio > 0 else 1.0\n",
    "\n",
    "    def run_simulation(self, p1_priors, p2_priors, iterations):\n",
    "        return [self.simulate_match(p1_priors, p2_priors)]\n",
    "\n",
    "    def predict_match_outcome(self, player1_canonical, player2_canonical, surface, gender, date):\n",
    "        p1_priors = self.extract_refined_priors(player1_canonical, gender, surface, date)\n",
    "        p2_priors = self.extract_refined_priors(player2_canonical, gender, surface, date)\n",
    "\n",
    "        base_prob = self.run_simulation(p1_priors, p2_priors, 1000)[0]\n",
    "\n",
    "        p1_rank = self.get_player_ranking(player1_canonical, date)\n",
    "        p2_rank = self.get_player_ranking(player2_canonical, date)\n",
    "        ranking_prob = self.calculate_ranking_differential_odds(p1_rank, p2_rank)\n",
    "\n",
    "        ranking_diff = p1_rank - p2_rank\n",
    "        upset_adjustment = self.calculate_upset_frequency(ranking_diff, surface, self.historical_data)\n",
    "\n",
    "        surface_ratio = self.calculate_surface_performance_ratio(player1_canonical, surface, player2_canonical, date)\n",
    "\n",
    "        calibrated_prob = (0.6 * base_prob + 0.25 * ranking_prob + 0.15 * surface_ratio) * (1 - upset_adjustment * 0.1)\n",
    "\n",
    "        return max(0.05, min(0.95, calibrated_prob))\n",
    "\n",
    "    def get_player_ranking(self, player_canonical, date):\n",
    "        \"\"\"Get player ranking at specific date\"\"\"\n",
    "        date_obj = pd.to_datetime(date)\n",
    "\n",
    "        player_matches = self.historical_data[\n",
    "            ((self.historical_data['winner_canonical'] == player_canonical) |\n",
    "             (self.historical_data['loser_canonical'] == player_canonical)) &\n",
    "            (pd.to_datetime(self.historical_data['Date']) <= date_obj)\n",
    "        ].sort_values('Date', ascending=False)\n",
    "\n",
    "        if len(player_matches) == 0:\n",
    "            return 999\n",
    "\n",
    "        latest_match = player_matches.iloc[0]\n",
    "\n",
    "        if latest_match['winner_canonical'] == player_canonical:\n",
    "            return latest_match.get('WRank', 999)\n",
    "        else:\n",
    "            return latest_match.get('LRank', 999)\n",
    "\n",
    "    def calculate_match_probability(self, player1_canonical, player2_canonical, gender, surface, reference_date, best_of=3):\n",
    "        player1_priors = self.extract_refined_priors(player1_canonical, gender, surface, reference_date)\n",
    "        player2_priors = self.extract_refined_priors(player2_canonical, gender, surface, reference_date)\n",
    "\n",
    "        probability = self.simulate_match(player1_priors, player2_priors, best_of)\n",
    "        confidence = min(player1_priors['confidence'], player2_priors['confidence'])\n",
    "\n",
    "        return {\n",
    "            'player1_win_probability': probability,\n",
    "            'player2_win_probability': 1 - probability,\n",
    "            'confidence': confidence,\n",
    "            'player1_priors': player1_priors,\n",
    "            'player2_priors': player2_priors\n",
    "        }\n",
    "\n",
    "    def calculate_form_spike(self, recent_matches, weights, player_canonical):\n",
    "        if len(recent_matches) == 0:\n",
    "            return 1.0\n",
    "\n",
    "        wins = (recent_matches['winner_canonical'] == player_canonical).astype(int)\n",
    "        weighted_win_rate = np.average(wins, weights=weights)\n",
    "\n",
    "        avg_opponent_rank = recent_matches['LRank'].fillna(recent_matches['WRank']).mean()\n",
    "        player_rank = recent_matches['WRank'].fillna(recent_matches['LRank']).iloc[-1]\n",
    "\n",
    "        if pd.notna(avg_opponent_rank) and pd.notna(player_rank):\n",
    "            rank_diff = player_rank - avg_opponent_rank\n",
    "            expected_win_rate = 1 / (1 + 10**(rank_diff/400))\n",
    "            form_spike = min(2.0, weighted_win_rate / max(0.1, expected_win_rate))\n",
    "        else:\n",
    "            form_spike = 1.0\n",
    "\n",
    "        return form_spike\n",
    "\n",
    "    def simulate_match(self, player1_priors, player2_priors, best_of=3):\n",
    "        wins = 0\n",
    "        for _ in range(self.simulation_count):\n",
    "            sets_won = [0, 0]\n",
    "            while max(sets_won) < (best_of + 1) // 2:\n",
    "                set_winner = self.simulate_set(player1_priors, player2_priors)\n",
    "                sets_won[set_winner] += 1\n",
    "            if sets_won[0] > sets_won[1]:\n",
    "                wins += 1\n",
    "        return wins / self.simulation_count\n",
    "\n",
    "    def simulate_set(self, p1_priors, p2_priors):\n",
    "        games = [0, 0]\n",
    "        server = 0\n",
    "        while True:\n",
    "            hold_prob = p1_priors['hold_prob'] if server == 0 else p2_priors['hold_prob']\n",
    "            game_winner = server if np.random.random() < hold_prob else 1 - server\n",
    "            games[game_winner] += 1\n",
    "            server = 1 - server\n",
    "            if games[0] >= 6 and games[0] - games[1] >= 2:\n",
    "                return 0\n",
    "            elif games[1] >= 6 and games[1] - games[0] >= 2:\n",
    "                return 1\n",
    "            elif games[0] == 6 and games[1] == 6:\n",
    "                return self.simulate_tiebreak(p1_priors, p2_priors)\n",
    "\n",
    "    def simulate_tiebreak(self, p1_priors, p2_priors):\n",
    "        points = [0, 0]\n",
    "        server = 0\n",
    "        serve_count = 0\n",
    "        while True:\n",
    "            hold_prob = p1_priors['hold_prob'] if server == 0 else p2_priors['hold_prob']\n",
    "            point_winner = server if np.random.random() < hold_prob else 1 - server\n",
    "            points[point_winner] += 1\n",
    "            serve_count += 1\n",
    "            if serve_count == 1 or serve_count % 2 == 0:\n",
    "                server = 1 - server\n",
    "            if points[0] >= 7 and points[0] - points[1] >= 2:\n",
    "                return 0\n",
    "            elif points[1] >= 7 and points[1] - points[0] >= 2:\n",
    "                return 1\n",
    "\n",
    "    def get_player_weighted_elo(self, player_canonical, surface, reference_date):\n",
    "        recent_match = self.historical_data[\n",
    "            ((self.historical_data['winner_canonical'] == player_canonical) |\n",
    "             (self.historical_data['loser_canonical'] == player_canonical)) &\n",
    "            (self.historical_data['Surface'] == surface)\n",
    "        ].tail(1)\n",
    "\n",
    "        if len(recent_match) > 0 and 'BlendScore' in recent_match.columns:\n",
    "            blend_score = recent_match['BlendScore'].iloc[0]\n",
    "            return 1500 + blend_score * 50\n",
    "\n",
    "        any_surface_match = self.historical_data[\n",
    "            (self.historical_data['winner_canonical'] == player_canonical) |\n",
    "            (self.historical_data['loser_canonical'] == player_canonical)\n",
    "        ].tail(1)\n",
    "\n",
    "        if len(any_surface_match) > 0 and 'BlendScore' in any_surface_match.columns:\n",
    "            return 1500 + any_surface_match['BlendScore'].iloc[0] * 200\n",
    "\n",
    "        return 1500\n",
    "\n",
    "    def calculate_surface_adaptation(self, player_canonical, target_surface):\n",
    "        player_matches = self.historical_data[\n",
    "            (self.historical_data['winner_canonical'] == player_canonical) |\n",
    "            (self.historical_data['loser_canonical'] == player_canonical)\n",
    "        ].copy()\n",
    "\n",
    "        if len(player_matches) < 10:\n",
    "            return 1.0\n",
    "\n",
    "        surface_matches = player_matches[player_matches['Surface'] == target_surface]\n",
    "        if len(surface_matches) < 3:\n",
    "            return 1.0\n",
    "\n",
    "        surface_wins = (surface_matches['winner_canonical'] == player_canonical).sum()\n",
    "        surface_win_rate = surface_wins / len(surface_matches)\n",
    "\n",
    "        total_wins = (player_matches['winner_canonical'] == player_canonical).sum()\n",
    "        baseline_win_rate = total_wins / len(player_matches)\n",
    "\n",
    "        if baseline_win_rate == 0:\n",
    "            return 1.0\n",
    "\n",
    "        adaptation_ratio = surface_win_rate / baseline_win_rate\n",
    "        return max(0.7, min(1.5, adaptation_ratio))\n",
    "\n",
    "    def evaluate_predictions(self, test_data):\n",
    "        \"\"\"Evaluate model accuracy on test dataset\"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for _, match in test_data.iterrows():\n",
    "            prob = self.predict_match_outcome(\n",
    "                match['winner_canonical'],\n",
    "                match['loser_canonical'],\n",
    "                match['Surface'],\n",
    "                match['gender'],\n",
    "                match['Date']\n",
    "            )\n",
    "\n",
    "            predicted_winner = match['winner_canonical'] if prob > 0.5 else match['loser_canonical']\n",
    "            actual_winner = match['winner_canonical']\n",
    "\n",
    "            if predicted_winner == actual_winner:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "        return correct / total if total > 0 else 0\n",
    "\n",
    "def convert_to_canonical(name):\n",
    "    return normalize_name_canonical(name)\n",
    "\n",
    "model = BayesianTennisModel()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5a44a93653fa2ec3",
   "metadata": {},
   "source": [
    "## LAYER 2 ##\n",
    "def apply_contextual_adjustments(self, priors, player_canonical, opponent_canonical, match_context):\n",
    "    \"\"\"Layer 2: Contextual Bayesian adjustments for fatigue, injury, motivation\"\"\"\n",
    "\n",
    "    adjusted_priors = priors.copy()\n",
    "\n",
    "    # Fatigue Index\n",
    "    fatigue_penalty = self.calculate_fatigue_index(player_canonical, match_context['reference_date'])\n",
    "    adjusted_priors['hold_prob'] *= (1 - fatigue_penalty * 0.15)  # Max 15% hold penalty\n",
    "    adjusted_priors['elo_std'] *= (1 + fatigue_penalty * 0.3)    # Increase uncertainty\n",
    "\n",
    "    # Injury Flag Adjustment\n",
    "    injury_factor = self.get_injury_factor(player_canonical, match_context['reference_date'])\n",
    "    adjusted_priors['hold_prob'] *= injury_factor\n",
    "    adjusted_priors['break_prob'] *= (2 - injury_factor)  # Inverse relationship\n",
    "\n",
    "    # Form Spike Sustainability\n",
    "    form_sustainability = self.calculate_form_sustainability(player_canonical, match_context)\n",
    "    if adjusted_priors['form_factor'] > 1.2:  # Hot streak detection\n",
    "        sustainability_discount = 1 - ((adjusted_priors['form_factor'] - 1) * (1 - form_sustainability))\n",
    "        adjusted_priors['hold_prob'] *= sustainability_discount\n",
    "        adjusted_priors['elo_mean'] *= sustainability_discount\n",
    "\n",
    "    # Opponent Quality Weighting\n",
    "    opponent_elo = self.estimate_opponent_elo(opponent_canonical, match_context)\n",
    "    elo_diff = adjusted_priors['elo_mean'] - opponent_elo\n",
    "    quality_adjustment = 1 / (1 + np.exp(-elo_diff / 200))  # Sigmoid scaling\n",
    "    adjusted_priors['break_prob'] *= quality_adjustment\n",
    "\n",
    "    return adjusted_priors\n",
    "\n",
    "def calculate_fatigue_index(self, player_canonical, reference_date):\n",
    "    \"\"\"Fatigue based on recent match load and recovery time\"\"\"\n",
    "    recent_matches = self.get_recent_matches(player_canonical, reference_date, days=14)\n",
    "\n",
    "    if len(recent_matches) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate cumulative fatigue\n",
    "    fatigue_score = 0\n",
    "    for _, match in recent_matches.iterrows():\n",
    "        days_ago = (pd.to_datetime(reference_date) - pd.to_datetime(match['Date'])).days\n",
    "        match_duration = match.get('minutes', 120)  # Default 2 hours\n",
    "\n",
    "        # Exponential decay with match duration weighting\n",
    "        fatigue_contribution = (match_duration / 60) * np.exp(-0.1 * days_ago)\n",
    "        fatigue_score += fatigue_contribution\n",
    "\n",
    "    return min(1.0, fatigue_score / 10)  # Normalize to 0-1\n",
    "\n",
    "def get_injury_factor(self, player_canonical, reference_date):\n",
    "    \"\"\"Player-specific injury fragility scoring\"\"\"\n",
    "    # Injury memory bank - replace with actual injury tracking\n",
    "    injury_prone_players = {\n",
    "        'nadal_r': 0.85,\n",
    "        'murray_a': 0.80,\n",
    "        'thiem_d': 0.75,\n",
    "        'badosa_p': 0.70\n",
    "    }\n",
    "\n",
    "    base_factor = injury_prone_players.get(player_canonical, 0.95)\n",
    "\n",
    "    # Check for recent retirement/walkover flags\n",
    "    recent_retirements = self.check_recent_retirements(player_canonical, reference_date)\n",
    "    if recent_retirements > 0:\n",
    "        base_factor *= (0.8 ** recent_retirements)\n",
    "\n",
    "    return max(0.5, base_factor)\n",
    "\n",
    "def calculate_form_sustainability(self, player_canonical, match_context):\n",
    "    \"\"\"Form spike sustainability based on opponent quality and win quality\"\"\"\n",
    "    recent_matches = self.get_recent_matches(player_canonical, match_context['reference_date'], days=21)\n",
    "\n",
    "    if len(recent_matches) < 3:\n",
    "        return 0.5\n",
    "\n",
    "    # Quality-weighted recent performance\n",
    "    quality_scores = []\n",
    "    for _, match in recent_matches.iterrows():\n",
    "        opponent_rank = match['LRank'] if match['winner_canonical'] == player_canonical else match['WRank']\n",
    "        win_quality = 1 / (1 + opponent_rank / 100) if pd.notna(opponent_rank) else 0.5\n",
    "        quality_scores.append(win_quality)\n",
    "\n",
    "    avg_opponent_quality = np.mean(quality_scores)\n",
    "    consistency = 1 - np.std(quality_scores)\n",
    "\n",
    "    return min(1.0, avg_opponent_quality * consistency)\n",
    "\n",
    "def estimate_opponent_elo(self, opponent_canonical, match_context):\n",
    "    \"\"\"Quick opponent Elo estimation for quality weighting\"\"\"\n",
    "    opponent_priors = self.extract_refined_priors(\n",
    "        opponent_canonical,\n",
    "        match_context['gender'],\n",
    "        match_context['surface'],\n",
    "        match_context['reference_date']\n",
    "    )\n",
    "    return opponent_priors['elo_mean']\n",
    "\n",
    "def get_recent_matches(self, player_canonical, reference_date, days=14):\n",
    "    try:\n",
    "        cutoff_date = pd.to_datetime(reference_date) - pd.Timedelta(days=days)\n",
    "\n",
    "        player_matches = self.historical_data[\n",
    "            ((self.historical_data['winner_canonical'] == player_canonical) |\n",
    "             (self.historical_data['loser_canonical'] == player_canonical))\n",
    "        ].copy()\n",
    "\n",
    "        if len(player_matches) == 0:\n",
    "            return player_matches\n",
    "\n",
    "        # Force string conversion then datetime to avoid mixed types\n",
    "        player_matches['Date'] = pd.to_datetime(player_matches['Date'].astype(str), errors='coerce')\n",
    "        player_matches = player_matches.dropna(subset=['Date'])\n",
    "        player_matches = player_matches[player_matches['Date'] >= cutoff_date]\n",
    "\n",
    "        return player_matches.sort_values('Date')\n",
    "    except:\n",
    "        # Return empty DataFrame on any error\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def check_recent_retirements(self, player_canonical, reference_date):\n",
    "    \"\"\"Count recent retirements/walkovers - placeholder for actual retirement tracking\"\"\"\n",
    "    # Implementation depends on your data structure for retirement flags\n",
    "    return 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1199334327215982",
   "metadata": {},
   "source": [
    "## LAYER 3 ##\n",
    "def simulate_match(self, player1_priors, player2_priors, best_of=3, tiebreak_sets=[1,2,3]):\n",
    "    \"\"\"Layer 3: Monte Carlo match simulation with Bayesian priors\"\"\"\n",
    "\n",
    "    wins = 0\n",
    "    simulations = self.simulation_count\n",
    "\n",
    "    for _ in range(simulations):\n",
    "        sets_won = [0, 0]  # [player1, player2]\n",
    "\n",
    "        while max(sets_won) < (best_of + 1) // 2:\n",
    "            set_winner = self.simulate_set(\n",
    "                player1_priors,\n",
    "                player2_priors,\n",
    "                tiebreak=len([s for s in sets_won if s > 0]) + 1 in tiebreak_sets\n",
    "            )\n",
    "            sets_won[set_winner] += 1\n",
    "\n",
    "        if sets_won[0] > sets_won[1]:\n",
    "            wins += 1\n",
    "\n",
    "    return wins / simulations\n",
    "\n",
    "def simulate_set(self, p1_priors, p2_priors, tiebreak=True):\n",
    "    \"\"\"Simulate single set with service alternation\"\"\"\n",
    "    games = [0, 0]\n",
    "    server = 0  # 0 = player1 serves first\n",
    "\n",
    "    while True:\n",
    "        # Determine game winner based on server\n",
    "        if server == 0:\n",
    "            hold_prob = p1_priors['hold_prob']\n",
    "            game_winner = 0 if np.random.random() < hold_prob else 1\n",
    "        else:\n",
    "            hold_prob = p2_priors['hold_prob']\n",
    "            game_winner = 1 if np.random.random() < hold_prob else 0\n",
    "\n",
    "        games[game_winner] += 1\n",
    "        server = 1 - server  # Alternate serve\n",
    "\n",
    "        # Check set completion\n",
    "        if games[0] >= 6 and games[0] - games[1] >= 2:\n",
    "            return 0\n",
    "        elif games[1] >= 6 and games[1] - games[0] >= 2:\n",
    "            return 1\n",
    "        elif games[0] == 6 and games[1] == 6 and tiebreak:\n",
    "            return self.simulate_tiebreak(p1_priors, p2_priors)\n",
    "\n",
    "def simulate_tiebreak(self, p1_priors, p2_priors):\n",
    "    \"\"\"Simulate tiebreak with point-by-point serve alternation\"\"\"\n",
    "    points = [0, 0]\n",
    "    server = 0\n",
    "    serve_count = 0\n",
    "\n",
    "    while True:\n",
    "        # Determine point winner\n",
    "        if server == 0:\n",
    "            hold_prob = p1_priors['hold_prob']\n",
    "            point_winner = 0 if np.random.random() < hold_prob else 1\n",
    "        else:\n",
    "            hold_prob = p2_priors['hold_prob']\n",
    "            point_winner = 1 if np.random.random() < hold_prob else 0\n",
    "\n",
    "        points[point_winner] += 1\n",
    "        serve_count += 1\n",
    "\n",
    "        # Alternate server every 2 points (except first point)\n",
    "        if serve_count == 1 or serve_count % 2 == 0:\n",
    "            server = 1 - server\n",
    "\n",
    "        # Check tiebreak completion\n",
    "        if points[0] >= 7 and points[0] - points[1] >= 2:\n",
    "            return 0\n",
    "        elif points[1] >= 7 and points[1] - points[0] >= 2:\n",
    "            return 1\n",
    "\n",
    "def simulate_match(self, player1_priors, player2_priors, best_of=3, tiebreak_sets=[1,2,3]):\n",
    "    wins = 0\n",
    "    simulations = self.simulation_count\n",
    "\n",
    "    for _ in range(simulations):\n",
    "        sets_won = [0, 0]\n",
    "\n",
    "        while max(sets_won) < (best_of + 1) // 2:\n",
    "            set_winner = self.simulate_set(\n",
    "                player1_priors,\n",
    "                player2_priors,\n",
    "                tiebreak=len([s for s in sets_won if s > 0]) + 1 in tiebreak_sets\n",
    "            )\n",
    "            sets_won[set_winner] += 1\n",
    "\n",
    "        if sets_won[0] > sets_won[1]:\n",
    "            wins += 1\n",
    "\n",
    "    return wins / simulations\n",
    "\n",
    "def simulate_set(self, p1_priors, p2_priors, tiebreak=True):\n",
    "    games = [0, 0]\n",
    "    server = 0\n",
    "\n",
    "    while True:\n",
    "        if server == 0:\n",
    "            hold_prob = p1_priors['hold_prob']\n",
    "            game_winner = 0 if np.random.random() < hold_prob else 1\n",
    "        else:\n",
    "            hold_prob = p2_priors['hold_prob']\n",
    "            game_winner = 1 if np.random.random() < hold_prob else 0\n",
    "\n",
    "        games[game_winner] += 1\n",
    "        server = 1 - server\n",
    "\n",
    "        if games[0] >= 6 and games[0] - games[1] >= 2:\n",
    "            return 0\n",
    "        elif games[1] >= 6 and games[1] - games[0] >= 2:\n",
    "            return 1\n",
    "        elif games[0] == 6 and games[1] == 6 and tiebreak:\n",
    "            return self.simulate_tiebreak(p1_priors, p2_priors)\n",
    "\n",
    "def simulate_tiebreak(self, p1_priors, p2_priors):\n",
    "    points = [0, 0]\n",
    "    server = 0\n",
    "    serve_count = 0\n",
    "\n",
    "    while True:\n",
    "        if server == 0:\n",
    "            hold_prob = p1_priors['hold_prob']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eff82911a069f6d",
   "metadata": {},
   "source": [
    "# Tomorrow's slate\n",
    "\n",
    "import requests\n",
    "from datetime import date, timedelta\n",
    "\n",
    "API_KEY = \"adfc70491c47895e5fffdc6428bbf36a561989d4bffcfa9ecfba8d91e947b4fb\"\n",
    "BASE = \"https://api.api-tennis.com/tennis/\"\n",
    "\n",
    "def get_matches_for_date(target_date):\n",
    "    params = {\n",
    "        \"method\": \"get_fixtures\",\n",
    "        \"APIkey\": API_KEY,\n",
    "        \"date_start\": target_date,\n",
    "        \"date_stop\": target_date\n",
    "    }\n",
    "    response = requests.get(BASE, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"HTTP {response.status_code}\")\n",
    "\n",
    "    # Surface mapping\n",
    "    TOURNAMENT_SURFACES = {\n",
    "        'ATP Wimbledon': 'Grass',\n",
    "        'WTA Wimbledon': 'Grass',\n",
    "        'ATP French Open': 'Clay',\n",
    "        'WTA French Open': 'Clay',\n",
    "        'ATP US Open': 'Hard',\n",
    "        'WTA US Open': 'Hard',\n",
    "        'ATP Australian Open': 'Hard',\n",
    "        'WTA Australian Open': 'Hard'\n",
    "    }\n",
    "\n",
    "    data = response.json()\n",
    "    matches = []\n",
    "\n",
    "    for event in data.get(\"result\", []):\n",
    "        matches.append({\n",
    "            'event_key': event.get('event_key'),\n",
    "            'player1_name': event['event_first_player'],\n",
    "            'player2_name': event['event_second_player'],\n",
    "            'tournament_name': event.get('tournament_name', 'Unknown'),\n",
    "            'tournament_round': event.get('tournament_round', ''),\n",
    "            'event_status': event.get('event_status', ''),\n",
    "            'event_type_type': event.get('event_type_type', ''),\n",
    "            'surface': TOURNAMENT_SURFACES.get(event.get('tournament_name', ''), 'Unknown'),\n",
    "            'time': event.get('event_time', ''),\n",
    "            'date': event.get('event_date', '')\n",
    "        })\n",
    "\n",
    "    return matches\n",
    "\n",
    "def get_high_confidence_matches(target_date, min_confidence=0.2):\n",
    "    matches = get_matches_for_date(target_date)\n",
    "\n",
    "    results = []\n",
    "    for match in matches:\n",
    "        p1_canonical = convert_to_canonical(match['player1_name'])\n",
    "        p2_canonical = convert_to_canonical(match['player2_name'])\n",
    "\n",
    "        p1_priors = model.extract_refined_priors(p1_canonical, 'men', match['surface'], target_date)\n",
    "        p2_priors = model.extract_refined_priors(p2_canonical, 'men', match['surface'], target_date)\n",
    "\n",
    "        p1_win_prob = model.simulate_match(p1_priors, p2_priors)\n",
    "        confidence = abs(p1_win_prob - 0.5)\n",
    "\n",
    "        if confidence >= min_confidence:\n",
    "            favorite = match['player1_name'] if p1_win_prob > 0.5 else match['player2_name']\n",
    "            win_prob = max(p1_win_prob, 1 - p1_win_prob)\n",
    "\n",
    "            results.append({\n",
    "                'match': f\"{match['player1_name']} vs {match['player2_name']}\",\n",
    "                'favorite': favorite,\n",
    "                'probability': win_prob,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "\n",
    "    return sorted(results, key=lambda x: x['confidence'], reverse=True)\n",
    "\n",
    "# Usage\n",
    "today = date.today().isoformat()\n",
    "tomorrow = (date.today() + timedelta(days=1)).isoformat()\n",
    "\n",
    "todays_matches = get_matches_for_date(today)\n",
    "tomorrows_matches = get_matches_for_date(tomorrow)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "399e2e6f9701affe",
   "metadata": {},
   "source": [
    "# Todays_matches or tomorrows_matches\n",
    "todays_matches"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9ab25a7b1fb3ee6",
   "metadata": {},
   "source": [
    "# Get top 5 picks\n",
    "def get_top_confidence_matches(target_date, top_n=5, min_confidence=0.05):\n",
    "    matches = get_matches_for_date(target_date)\n",
    "\n",
    "    results = []\n",
    "    for match in matches:\n",
    "        p1_canonical = convert_to_canonical(match['player1_name'])\n",
    "        p2_canonical = convert_to_canonical(match['player2_name'])\n",
    "\n",
    "        p1_priors = model.extract_refined_priors(p1_canonical, 'men', match['surface'], target_date)\n",
    "        p2_priors = model.extract_refined_priors(p2_canonical, 'men', match['surface'], target_date)\n",
    "\n",
    "        p1_win_prob = model.simulate_match(p1_priors, p2_priors)\n",
    "        confidence = abs(p1_win_prob - 0.5)\n",
    "\n",
    "        if confidence >= min_confidence:\n",
    "            favorite = match['player1_name'] if p1_win_prob > 0.5 else match['player2_name']\n",
    "            win_prob = max(p1_win_prob, 1 - p1_win_prob)\n",
    "\n",
    "            results.append({\n",
    "                'match': f\"{match['player1_name']} vs {match['player2_name']}\",\n",
    "                'favorite': favorite,\n",
    "                'probability': win_prob,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "\n",
    "    return sorted(results, key=lambda x: x['confidence'], reverse=True)[:top_n]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    target_date = date.today().isoformat()  # today's matches\n",
    "    picks = get_top_confidence_matches(target_date, top_n=5, min_confidence=0.15)\n",
    "\n",
    "    for i, pick in enumerate(picks, 1):\n",
    "        print(f\"{i}. {pick['match']}\")\n",
    "        print(f\"   Favorite: {pick['favorite']}\")\n",
    "        print(f\"   Win Prob: {pick['probability']:.2%}\")\n",
    "        print(f\"   Confidence: {pick['confidence']:.5%}\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2b9d6f0f3cff6de",
   "metadata": {},
   "source": [
    "# See picks\n",
    "from datetime import date\n",
    "\n",
    "# get todayâ€™s top-5 at 5% confidence\n",
    "picks = get_top_confidence_matches(date.today().isoformat(), top_n=5, min_confidence=0.05)\n",
    "\n",
    "# print them\n",
    "for i, pick in enumerate(picks, 1):\n",
    "    print(f\"{i}. {pick['match']}\")\n",
    "    print(f\"   Favorite: {pick['favorite']}\")\n",
    "    print(f\"   Win Prob: {pick['probability']:.2%}\")\n",
    "    print(f\"   Confidence: {pick['confidence']:.1%}\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7280936c04122eb0",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(picks)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "636f482291cd31b9",
   "metadata": {},
   "source": [
    "# Split data chronologically\n",
    "split_date = '2023-01-01'\n",
    "train_data = historical_data[pd.to_datetime(historical_data['Date']) < split_date]\n",
    "test_data = historical_data[pd.to_datetime(historical_data['Date']) >= split_date]\n",
    "\n",
    "# Initialize model with training data\n",
    "model.historical_data = train_data\n",
    "\n",
    "# Run evaluation\n",
    "accuracy = model.evaluate_predictions(test_data.head(100))\n",
    "print(f\"Enhanced model accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Compare with baseline\n",
    "model_baseline = BayesianTennisModel()\n",
    "model_baseline.historical_data = train_data\n",
    "baseline_accuracy = model_baseline.evaluate_predictions(test_data.head(100))\n",
    "print(f\"Baseline accuracy: {baseline_accuracy:.3f}\")\n",
    "print(f\"Improvement: {accuracy - baseline_accuracy:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40b3cdc2e4eba13e",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "329d94439459a9e6",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee25387c027552f9",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24e9660711402dd",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import html\n",
    "\n",
    "class TennisAbstractScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    # Stats Overview\n",
    "    def scrape_stats_overview(self, url):\n",
    "        resp = requests.get(url, headers=self.headers)\n",
    "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "        scripts = [tag.string for tag in soup.find_all(\"script\") if tag.string]\n",
    "        all_js = \"\\n\".join(scripts)\n",
    "        blocks = dict(re.findall(r\"var\\s+(\\w+)\\s*=\\s*'([\\s\\S]*?)';\", all_js, flags=re.S))\n",
    "        labels = {span.get_text(strip=True): span[\"id\"] for span in soup.select(\"span.rounds\")}\n",
    "        sections = {label: html.unescape(blocks[token]) for label, token in labels.items() if token in blocks}\n",
    "\n",
    "        match_info = self._parse_match_url(url)\n",
    "        stats_html = sections.get(\"Stats Overview\", \"\")\n",
    "        stats_data = self._extract_stats_overview_table(stats_html)\n",
    "\n",
    "        return self._convert_to_jeff_format(stats_data, match_info)\n",
    "\n",
    "    # Serve Basics\n",
    "    def scrape_serve_basics(self, url):\n",
    "        resp = requests.get(url, headers=self.headers)\n",
    "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "        scripts = [tag.string for tag in soup.find_all(\"script\") if tag.string]\n",
    "        all_js = \"\\n\".join(scripts)\n",
    "        blocks = dict(re.findall(r\"var\\s+(\\w+)\\s*=\\s*'([\\s\\S]*?)';\", all_js, flags=re.S))\n",
    "        labels = {span.get_text(strip=True): span[\"id\"] for span in soup.select(\"span.rounds\")}\n",
    "        sections = {label: html.unescape(blocks[token]) for label, token in labels.items() if token in blocks}\n",
    "\n",
    "        match_info = self._parse_match_url(url)\n",
    "        serve_html = sections.get(\"Serve Basics\", \"\")\n",
    "        serve_data = self._parse_serve_basics(serve_html)\n",
    "\n",
    "        return self._convert_serve_basics_to_jeff(serve_data, match_info)\n",
    "\n",
    "    # add to TennisAbstractScraper\n",
    "    MAP_SERVE_INFL = {\n",
    "        'Wide %':   'serve_wide_pct',\n",
    "        'T %':      'serve_t_pct',\n",
    "        'Body %':   'serve_body_pct'\n",
    "    }\n",
    "\n",
    "    def scrape_serve_influence(self, url):\n",
    "        resp = requests.get(url, headers=self.headers)\n",
    "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "        scripts = [t.string for t in soup.find_all(\"script\") if t.string]\n",
    "        js = \"\\n\".join(scripts)\n",
    "        blocks = dict(re.findall(r\"var\\s+(\\w+)\\s*=\\s*'([\\s\\S]*?)';\", js, re.S))\n",
    "        html_block = html.unescape(blocks.get('serve', ''))\n",
    "        if not html_block:\n",
    "            return []\n",
    "\n",
    "        tbl = BeautifulSoup(html_block, 'html.parser').table\n",
    "        heads = [c.get_text(strip=True) for c in tbl.tr.find_all(['th', 'td'])]\n",
    "        out = []\n",
    "        for row in tbl.find_all('tr')[1:]:\n",
    "            cells = [c.get_text(strip=True) for c in row.find_all('td')]\n",
    "            player = cells[0]\n",
    "            rec = {'Player_canonical': self._normalize_player_name(player)}\n",
    "            for h, v in zip(heads[1:], cells[1:]):\n",
    "                key = MAP_SERVE_INFL.get(h)\n",
    "                if key:\n",
    "                    rec[key] = float(v.rstrip('%')) / 100\n",
    "            out.append(rec)\n",
    "        return out\n",
    "\n",
    "    def _parse_serve_basics(self, html_content):\n",
    "        \"\"\"Parse Serve Basics section - serves, aces, double faults breakdown\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        if not table:\n",
    "            return {}\n",
    "\n",
    "        rows = table.find_all('tr')\n",
    "        headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n",
    "\n",
    "        data_rows = []\n",
    "        for row in rows[1:]:\n",
    "            cells = [td.get_text(strip=True) for td in row.find_all('td')]\n",
    "            if len(cells) >= len(headers):\n",
    "                data_rows.append(cells)\n",
    "\n",
    "        return self._parse_serve_basics_data(headers, data_rows)\n",
    "\n",
    "    def _parse_serve_basics_data(self, headers, data_rows):\n",
    "        \"\"\"Convert Serve Basics table to structured data\"\"\"\n",
    "        stats_data = {}\n",
    "        current_set = \"Total\"\n",
    "\n",
    "        for row in data_rows:\n",
    "            if not row[0]:\n",
    "                continue\n",
    "\n",
    "            if row[0].startswith('SET'):\n",
    "                current_set = row[0]\n",
    "                continue\n",
    "\n",
    "            player_name = row[0]\n",
    "\n",
    "            if current_set not in stats_data:\n",
    "                stats_data[current_set] = {}\n",
    "\n",
    "            # Parse serve basics columns - adjust indices based on actual table structure\n",
    "            stats_data[current_set][player_name] = {\n",
    "                'serve_pts': int(row[1]) if len(row) > 1 and row[1].isdigit() else 0,\n",
    "                'aces': int(row[2]) if len(row) > 2 and row[2].isdigit() else 0,\n",
    "                'dfs': int(row[3]) if len(row) > 3 and row[3].isdigit() else 0,\n",
    "                'first_in': int(row[4]) if len(row) > 4 and row[4].isdigit() else 0,\n",
    "                'first_won': int(row[5]) if len(row) > 5 and row[5].isdigit() else 0,\n",
    "                'second_won': int(row[6]) if len(row) > 6 and row[6].isdigit() else 0\n",
    "            }\n",
    "\n",
    "        return stats_data\n",
    "\n",
    "    def _convert_serve_basics_to_jeff(self, serve_data, match_info):\n",
    "        \"\"\"Convert serve basics data to Jeff format records\"\"\"\n",
    "        jeff_records = []\n",
    "\n",
    "        for set_name, set_data in serve_data.items():\n",
    "            for player, data in set_data.items():\n",
    "                jeff_record = {\n",
    "                    'match_id': f\"{match_info['Date']}-{player.replace(' ', '_')}\",\n",
    "                    'Date': match_info['Date'],\n",
    "                    'Tournament': match_info['tournament'],\n",
    "                    'player': player,\n",
    "                    'Player_canonical': self._normalize_player_name(player),\n",
    "                    'set': set_name,\n",
    "                    'serve_pts': data['serve_pts'],\n",
    "                    'aces': data['aces'],\n",
    "                    'dfs': data['dfs'],\n",
    "                    'first_in': data['first_in'],\n",
    "                    'first_won': data['first_won'],\n",
    "                    'second_won': data['second_won']\n",
    "                }\n",
    "                jeff_records.append(jeff_record)\n",
    "\n",
    "        return jeff_records\n",
    "\n",
    "    # Existing methods unchanged\n",
    "    def _extract_stats_overview_table(self, html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        if not table:\n",
    "            return {}\n",
    "\n",
    "        rows = table.find_all('tr')\n",
    "        headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n",
    "\n",
    "        data_rows = []\n",
    "        for row in rows[1:]:\n",
    "            cells = [td.get_text(strip=True) for td in row.find_all('td')]\n",
    "            if len(cells) >= len(headers):\n",
    "                data_rows.append(cells)\n",
    "\n",
    "        return self._parse_tennis_stats(headers, data_rows)\n",
    "\n",
    "    def _parse_tennis_stats(self, headers, data_rows):\n",
    "        stats_data = {}\n",
    "        current_set = \"Total\"\n",
    "\n",
    "        for row in data_rows:\n",
    "            if not row[0]:\n",
    "                continue\n",
    "\n",
    "            if row[0].startswith('SET'):\n",
    "                current_set = row[0]\n",
    "                continue\n",
    "\n",
    "            player_name = row[0]\n",
    "\n",
    "            if current_set not in stats_data:\n",
    "                stats_data[current_set] = {}\n",
    "\n",
    "            winners_text = row[8] if len(row) > 8 else \"0 (0/0)\"\n",
    "            winners_match = re.match(r'(\\d+)\\s*\\((\\d+)/(\\d+)\\)', winners_text)\n",
    "            winners_total = int(winners_match.group(1)) if winners_match else 0\n",
    "            winners_fh = int(winners_match.group(2)) if winners_match else 0\n",
    "            winners_bh = int(winners_match.group(3)) if winners_match else 0\n",
    "\n",
    "            ufe_text = row[9] if len(row) > 9 else \"0 (0/0)\"\n",
    "            ufe_match = re.match(r'(\\d+)\\s*\\((\\d+)/(\\d+)\\)', ufe_text)\n",
    "            ufe_total = int(ufe_match.group(1)) if ufe_match else 0\n",
    "            ufe_fh = int(ufe_match.group(2)) if ufe_match else 0\n",
    "            ufe_bh = int(ufe_match.group(3)) if ufe_match else 0\n",
    "\n",
    "            stats_data[current_set][player_name] = {\n",
    "                'aces_pct': row[1] if len(row) > 1 else '0%',\n",
    "                'df_pct': row[2] if len(row) > 2 else '0%',\n",
    "                'first_in_pct': row[3] if len(row) > 3 else '0%',\n",
    "                'first_won_pct': row[4] if len(row) > 4 else '0%',\n",
    "                'second_won_pct': row[5] if len(row) > 5 else '0%',\n",
    "                'bp_saved': row[6] if len(row) > 6 else '0/0',\n",
    "                'rpw_pct': row[7] if len(row) > 7 else '0%',\n",
    "                'winners': str(winners_total),\n",
    "                'winners_fh': str(winners_fh),\n",
    "                'winners_bh': str(winners_bh),\n",
    "                'ufe': str(ufe_total),\n",
    "                'ufe_fh': str(ufe_fh),\n",
    "                'ufe_bh': str(ufe_bh)\n",
    "            }\n",
    "\n",
    "        return stats_data\n",
    "\n",
    "    def _parse_match_url(self, url):\n",
    "        pattern = r'(\\d{8})-([MW])-(.+?)-(.+?)-(.+?)-(.+?)\\.html'\n",
    "        match = re.search(pattern, url)\n",
    "        if match:\n",
    "            date_str, gender, tournament, round_info, player1, player2 = match.groups()\n",
    "            return {\n",
    "                'Date': date_str,\n",
    "                'gender': 'M' if gender == 'M' else 'W',\n",
    "                'tournament': tournament.replace('_', ' '),\n",
    "                'round': round_info,\n",
    "                'player1': player1.replace('_', ' '),\n",
    "                'player2': player2.replace('_', ' ')\n",
    "            }\n",
    "        return {}\n",
    "\n",
    "    def _convert_to_jeff_format(self, stats_data, match_info):\n",
    "        jeff_records = []\n",
    "        for set_name, set_data in stats_data.items():\n",
    "            for player, data in set_data.items():\n",
    "                serve_pts = 67 if set_name == 'Total' else (40 if set_name == 'SET 1' else 27)\n",
    "\n",
    "                aces = int(float(data['aces_pct'].rstrip('%')) / 100 * serve_pts)\n",
    "                dfs = int(float(data['df_pct'].rstrip('%')) / 100 * serve_pts)\n",
    "                first_in = int(float(data['first_in_pct'].rstrip('%')) / 100 * serve_pts)\n",
    "                first_won = int(float(data['first_won_pct'].rstrip('%')) / 100 * first_in) if first_in > 0 else 0\n",
    "                second_won = int(float(data['second_won_pct'].rstrip('%')) / 100 * (serve_pts - first_in)) if (serve_pts - first_in) > 0 else 0\n",
    "\n",
    "                bp_parts = data['bp_saved'].split('/')\n",
    "                bp_saved = int(bp_parts[0])\n",
    "                bp_faced = int(bp_parts[1]) if len(bp_parts) > 1 else 0\n",
    "\n",
    "                return_pts_won = int(float(data['rpw_pct'].rstrip('%')) / 100 * serve_pts)\n",
    "\n",
    "                jeff_record = {\n",
    "                    'match_id': f\"{match_info['Date']}-{player.replace(' ', '_')}\",\n",
    "                    'Date': match_info['Date'],\n",
    "                    'Tournament': match_info['tournament'],\n",
    "                    'player': player,\n",
    "                    'Player_canonical': self._normalize_player_name(player),\n",
    "                    'set': set_name,\n",
    "                    'serve_pts': serve_pts,\n",
    "                    'aces': aces,\n",
    "                    'dfs': dfs,\n",
    "                    'first_in': first_in,\n",
    "                    'first_won': first_won,\n",
    "                    'second_won': second_won,\n",
    "                    'bp_saved': bp_saved,\n",
    "                    'bp_faced': bp_faced,\n",
    "                    'return_pts_won': return_pts_won,\n",
    "                    'winners': int(data['winners']),\n",
    "                    'winners_fh': int(data['winners_fh']),\n",
    "                    'winners_bh': int(data['winners_bh']),\n",
    "                    'unforced': int(data['ufe']),\n",
    "                    'unforced_fh': int(data['ufe_fh']),\n",
    "                    'unforced_bh': int(data['ufe_bh'])\n",
    "                }\n",
    "                jeff_records.append(jeff_record)\n",
    "        return jeff_records\n",
    "\n",
    "    def _normalize_player_name(self, name):\n",
    "        parts = name.lower().replace('.', '').split()\n",
    "        if len(parts) >= 2:\n",
    "            return f\"{parts[-1]}_{parts[0][0]}\"\n",
    "        return name.lower().replace(' ', '_')\n",
    "\n",
    "    def debug_available_sections(self, url):\n",
    "        resp = requests.get(url, headers=self.headers)\n",
    "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "        scripts = [tag.string for tag in soup.find_all(\"script\") if tag.string]\n",
    "        all_js = \"\\n\".join(scripts)\n",
    "        blocks = dict(re.findall(r\"var\\s+(\\w+)\\s*=\\s*'([\\s\\S]*?)';\", all_js, flags=re.S))\n",
    "        labels = {span.get_text(strip=True): span[\"id\"] for span in soup.select(\"span.rounds\")}\n",
    "\n",
    "        print(\"Available sections:\")\n",
    "        for label in labels.keys():\n",
    "            print(f\"- '{label}'\")\n",
    "        return labels\n",
    "\n",
    "def test_extraction_completeness(self, url):\n",
    "    \"\"\"Test all available sections and validate data structure\"\"\"\n",
    "    sections = self.debug_available_sections(url)\n",
    "\n",
    "    results = {}\n",
    "    for section_name in sections.keys():\n",
    "        try:\n",
    "            # Test each section extraction\n",
    "            extracted_data = self._test_section_extraction(url, section_name)\n",
    "            results[section_name] = len(extracted_data) > 0\n",
    "        except Exception as e:\n",
    "            results[section_name] = f\"Error: {e}\"\n",
    "\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d8d625144ae05312",
   "metadata": {},
   "source": [
    "# Test both methods\n",
    "scraper = TennisAbstractScraper()\n",
    "url = \"https://www.tennisabstract.com/charting/20250628-W-Eastbourne-F-Maya_Joint-Alexandra_Eala.html\"\n",
    "\n",
    "# Test Stats Overview\n",
    "print(\"=== STATS OVERVIEW ===\")\n",
    "overview_data = scraper.scrape_stats_overview(url)\n",
    "for record in overview_data:\n",
    "    print(record)\n",
    "\n",
    "print(\"\\n=== SERVE BASICS ===\")\n",
    "serve_data = scraper.scrape_serve_basics(url)\n",
    "for record in serve_data:\n",
    "    print(record)\n",
    "\n",
    "print(\"\\n=== SERVE INFLUENCE ===\")\n",
    "serve_infl_data = scraper.scrape_serve_influence(url)\n",
    "for record in serve_infl_data:\n",
    "    print(record)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1b457071fbdeb2d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
