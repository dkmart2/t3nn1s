{
 "cells": [
  {
   "cell_type": "code",
   "id": "7e769e487b08c238",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime, date, timedelta\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from unidecode import unidecode\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# API Configuration\n",
    "os.environ[\"API_TENNIS_KEY\"] = \"adfc70491c47895e5fffdc6428bbf36a561989d4bffcfa9ecfba8d91e947b4fb\"\n",
    "API_KEY = os.getenv(\"API_TENNIS_KEY\")\n",
    "BASE = \"https://api.api-tennis.com/tennis/\"\n",
    "\n",
    "def call(method: str, **params):\n",
    "    q = {\"method\": method, \"APIkey\": API_KEY, **params}\n",
    "    r = requests.get(BASE, params=q, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "    if str(j.get(\"error\", \"0\")) != \"0\":\n",
    "        raise RuntimeError(j)\n",
    "    return j[\"result\"]\n",
    "\n",
    "def fetch_day(day):\n",
    "    if isinstance(day, str):\n",
    "        day = datetime.strptime(day, '%Y-%m-%d').date()\n",
    "    return call(\"get_fixtures\", date_start=day.isoformat(), date_stop=day.isoformat())\n",
    "\n",
    "def normalize_name_canonical(name: str) -> str:\n",
    "    clean_name = unidecode(str(name)).lower().replace(\".\", \"\").strip()\n",
    "    clean_name = re.sub(r'\\s+', ' ', clean_name)\n",
    "    if '/' in clean_name:\n",
    "        return clean_name.replace('/', '_')\n",
    "    parts = clean_name.split()\n",
    "    if len(parts) >= 2:\n",
    "        if len(parts[-1]) == 1:\n",
    "            return f\"{parts[-2]}_{parts[-1]}\"\n",
    "        else:\n",
    "            return f\"{parts[-1]}_{parts[0][0]}\"\n",
    "    return clean_name\n",
    "\n",
    "def determine_surface(tournament_name: str) -> str:\n",
    "    tournament_lower = tournament_name.lower() if tournament_name else \"\"\n",
    "\n",
    "    surface_mapping = {\n",
    "        'roland garros': 'Clay', 'french open': 'Clay', 'monte carlo': 'Clay',\n",
    "        'rome': 'Clay', 'madrid': 'Clay', 'barcelona': 'Clay',\n",
    "        'wimbledon': 'Grass', 'queens': 'Grass', 'halle': 'Grass',\n",
    "        'australian open': 'Hard', 'us open': 'Hard', 'indian wells': 'Hard',\n",
    "        'miami': 'Hard', 'canada': 'Hard', 'cincinnati': 'Hard'\n",
    "    }\n",
    "\n",
    "    for keyword, surface in surface_mapping.items():\n",
    "        if keyword in tournament_lower:\n",
    "            return surface\n",
    "\n",
    "    if 'clay' in tournament_lower:\n",
    "        return 'Clay'\n",
    "    elif 'grass' in tournament_lower:\n",
    "        return 'Grass'\n",
    "    else:\n",
    "        return 'Hard'\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"Normalize tennis player names for matching\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "\n",
    "    name = str(name).replace('.', '').lower()\n",
    "    parts = name.split()\n",
    "\n",
    "    if len(parts) < 2:\n",
    "        return name.replace(' ', '_')\n",
    "\n",
    "    # Handle \"Lastname F\" format\n",
    "    if len(parts[-1]) == 1:  # Last part is single letter (first initial)\n",
    "        last_name = parts[-2]\n",
    "        first_initial = parts[-1]\n",
    "    else:  # Handle \"First Lastname\" format\n",
    "        last_name = parts[-1]\n",
    "        first_initial = parts[0][0] if parts[0] else ''\n",
    "\n",
    "    return f\"{last_name}_{first_initial}\"\n",
    "\n",
    "def normalize_jeff_name(name):\n",
    "    \"\"\"Normalize Jeff's player names for matching\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "\n",
    "    name = str(name).lower()\n",
    "    parts = name.split()\n",
    "\n",
    "    if len(parts) < 2:\n",
    "        return name.replace(' ', '_')\n",
    "\n",
    "    # Jeff data is \"First Last\" format\n",
    "    last_name = parts[-1]\n",
    "    first_initial = parts[0][0] if parts[0] else ''\n",
    "\n",
    "    return f\"{last_name}_{first_initial}\"\n",
    "\n",
    "def normalize_tournament_name(name):\n",
    "    \"\"\"Normalize tournament names\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "\n",
    "    name = str(name).lower()\n",
    "    # Remove common variations\n",
    "    name = name.replace('masters cup', 'masters')\n",
    "    name = name.replace('atp finals', 'masters')\n",
    "    name = name.replace('wta finals', 'masters')\n",
    "\n",
    "    return name.strip()\n",
    "\n",
    "def determine_gender_from_tournament(tournament_name: str) -> str:\n",
    "    \"\"\"Determine gender from tournament name\"\"\"\n",
    "    tournament_lower = tournament_name.lower() if tournament_name else \"\"\n",
    "\n",
    "    wta_keywords = ['wta', 'women', 'ladies', 'womens']\n",
    "    atp_keywords = ['atp', 'men', 'mens']\n",
    "\n",
    "    for keyword in wta_keywords:\n",
    "        if keyword in tournament_lower:\n",
    "            return 'W'\n",
    "\n",
    "    for keyword in atp_keywords:\n",
    "        if keyword in tournament_lower:\n",
    "            return 'M'\n",
    "\n",
    "    return 'M'  # Default to men's if unclear\n",
    "\n",
    "def load_excel_data(file_path):\n",
    "    \"\"\"Load data from Excel file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        if 'Date' not in df.columns:\n",
    "            print(f\"Warning: No Date column in {file_path}\")\n",
    "            return pd.DataFrame()\n",
    "        print(f\"Loaded {len(df)} matches from {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_tournament_tier_weight(tournament_name: str) -> float:\n",
    "    \"\"\"Classify tournament tier and return weight\"\"\"\n",
    "    if pd.isna(tournament_name):\n",
    "        return 0.5\n",
    "\n",
    "    tournament_lower = tournament_name.lower()\n",
    "\n",
    "    # Grand Slams\n",
    "    if any(slam in tournament_lower for slam in ['roland garros', 'wimbledon', 'australian open', 'us open']):\n",
    "        return 1.0\n",
    "\n",
    "    # Masters/WTA 1000\n",
    "    masters_events = ['indian wells', 'miami', 'monte carlo', 'madrid', 'rome', 'canada', 'cincinnati', 'shanghai', 'paris masters']\n",
    "    if any(masters in tournament_lower for masters in masters_events):\n",
    "        return 0.9\n",
    "\n",
    "    # ATP 500/WTA 500\n",
    "    atp500_events = ['stuttgart', 'barcelona', 'hamburg', 'halle', 'queens', 'washington', 'dubai', 'rotterdam']\n",
    "    if any(event in tournament_lower for event in atp500_events):\n",
    "        return 0.7\n",
    "\n",
    "    # ITF/Challenger/Juniors\n",
    "    if any(lower_tier in tournament_lower for lower_tier in ['itf', 'challenger', 'juniors']):\n",
    "        return 0.2\n",
    "\n",
    "    # ATP 250/WTA 250 (everything else)\n",
    "    return 0.5\n",
    "\n",
    "def calculate_recency_weight(match_date, reference_date='2025-07-01'):\n",
    "    \"\"\"Calculate exponential decay weight based on match recency\"\"\"\n",
    "    try:\n",
    "        if isinstance(match_date, str):\n",
    "            match_dt = datetime.strptime(match_date, '%Y%m%d')\n",
    "        else:\n",
    "            match_dt = match_date\n",
    "\n",
    "        ref_dt = datetime.strptime(reference_date, '%Y-%m-%d')\n",
    "        days_ago = (ref_dt - match_dt).days\n",
    "\n",
    "        # Exponential decay: ~60% weight after 2 years (730 days)\n",
    "        return np.exp(-0.0005 * days_ago)\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def load_jeff_comprehensive_data():\n",
    "    \"\"\"Load all of Jeff's comprehensive tennis data\"\"\"\n",
    "    base_path = os.path.expanduser(\"~/Desktop/data/Jeff 6.14.25\")\n",
    "\n",
    "    data = {\n",
    "        'men': {},\n",
    "        'women': {}\n",
    "    }\n",
    "\n",
    "    # File mappings with actual names\n",
    "    files = {\n",
    "        'matches': 'charting-{}-matches.csv',\n",
    "        'points_2020s': 'charting-{}-points-2020s.csv',\n",
    "        'overview': 'charting-{}-stats-Overview.csv',\n",
    "        'serve_basics': 'charting-{}-stats-ServeBasics.csv',\n",
    "        'return_outcomes': 'charting-{}-stats-ReturnOutcomes.csv',\n",
    "        'return_depth': 'charting-{}-stats-ReturnDepth.csv',\n",
    "        'key_points_serve': 'charting-{}-stats-KeyPointsServe.csv',\n",
    "        'key_points_return': 'charting-{}-stats-KeyPointsReturn.csv',\n",
    "        'net_points': 'charting-{}-stats-NetPoints.csv',\n",
    "        'rally': 'charting-{}-stats-Rally.csv',\n",
    "        'serve_direction': 'charting-{}-stats-ServeDirection.csv',\n",
    "        'serve_influence': 'charting-{}-stats-ServeInfluence.csv',\n",
    "        'shot_direction': 'charting-{}-stats-ShotDirection.csv',\n",
    "        'shot_dir_outcomes': 'charting-{}-stats-ShotDirOutcomes.csv',\n",
    "        'shot_types': 'charting-{}-stats-ShotTypes.csv',\n",
    "        'snv': 'charting-{}-stats-SnV.csv',\n",
    "        'sv_break_split': 'charting-{}-stats-SvBreakSplit.csv',\n",
    "        'sv_break_total': 'charting-{}-stats-SvBreakTotal.csv'\n",
    "    }\n",
    "\n",
    "    # Load men's data\n",
    "    men_path = os.path.join(base_path, 'men')\n",
    "    if os.path.exists(men_path):\n",
    "        for key, filename_template in files.items():\n",
    "            filename = filename_template.format('m')\n",
    "            file_path = os.path.join(men_path, filename)\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path, low_memory=False)\n",
    "                if 'player' in df.columns:\n",
    "                    df['Player_canonical'] = df['player'].apply(normalize_jeff_name)\n",
    "                data['men'][key] = df\n",
    "                print(f\"Loaded men/{filename}: {len(df)} records\")\n",
    "\n",
    "    # Load women's data\n",
    "    women_path = os.path.join(base_path, 'women')\n",
    "    if os.path.exists(women_path):\n",
    "        for key, filename_template in files.items():\n",
    "            filename = filename_template.format('w')\n",
    "            file_path = os.path.join(women_path, filename)\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path, low_memory=False)\n",
    "                if 'player' in df.columns:\n",
    "                    df['Player_canonical'] = df['player'].apply(normalize_jeff_name)\n",
    "                data['women'][key] = df\n",
    "                print(f\"Loaded women/{filename}: {len(df)} records\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def calculate_comprehensive_weighted_defaults(jeff_data):\n",
    "    \"\"\"Calculate weighted defaults from all Jeff datasets\"\"\"\n",
    "    print(\"Calculating comprehensive weighted defaults from Jeff's data...\")\n",
    "\n",
    "    defaults = {'men': {}, 'women': {}}\n",
    "\n",
    "    for gender in ['men', 'women']:\n",
    "        if gender not in jeff_data:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing {gender}'s comprehensive data...\")\n",
    "\n",
    "        # Load matches for tournament context and dates\n",
    "        matches_df = jeff_data[gender].get('matches')\n",
    "        if matches_df is None:\n",
    "            print(f\"No matches data for {gender}\")\n",
    "            continue\n",
    "\n",
    "        gender_defaults = {}\n",
    "\n",
    "        # Overview stats - basic serving/returning\n",
    "        if 'overview' in jeff_data[gender]:\n",
    "            overview_df = jeff_data[gender]['overview']\n",
    "            match_totals = overview_df[overview_df['set'] == 'Total'].copy()\n",
    "            match_totals = match_totals.merge(\n",
    "                matches_df[['match_id', 'Date', 'Tournament']],\n",
    "                on='match_id',\n",
    "                how='left'\n",
    "            )\n",
    "\n",
    "            weights = []\n",
    "            for _, row in match_totals.iterrows():\n",
    "                tournament_weight = get_tournament_tier_weight(row.get('Tournament', ''))\n",
    "                recency_weight = calculate_recency_weight(row.get('Date', '20200101'))\n",
    "                weights.append(tournament_weight * recency_weight)\n",
    "\n",
    "            def weighted_quantile(values, weights, quantile=0.5):\n",
    "                if len(values) == 0:\n",
    "                    return 0\n",
    "                sorted_indices = np.argsort(values)\n",
    "                sorted_weights = np.array(weights)[sorted_indices]\n",
    "                cumsum_weights = np.cumsum(sorted_weights)\n",
    "                total_weight = cumsum_weights[-1]\n",
    "                target_weight = quantile * total_weight\n",
    "                index = np.searchsorted(cumsum_weights, target_weight)\n",
    "                if index >= len(values):\n",
    "                    index = len(values) - 1\n",
    "                return values[sorted_indices[index]]\n",
    "\n",
    "            serve_pts = match_totals['serve_pts'].fillna(0)\n",
    "            aces = match_totals['aces'].fillna(0)\n",
    "            dfs = match_totals['dfs'].fillna(0)\n",
    "            first_in = match_totals['first_in'].fillna(0)\n",
    "            first_won = match_totals['first_won'].fillna(0)\n",
    "            second_won = match_totals['second_won'].fillna(0)\n",
    "            bp_saved = match_totals['bp_saved'].fillna(0)\n",
    "            return_pts_won = match_totals['return_pts_won'].fillna(0)\n",
    "            winners = match_totals['winners'].fillna(0)\n",
    "            winners_fh = match_totals['winners_fh'].fillna(0)\n",
    "            winners_bh = match_totals['winners_bh'].fillna(0)\n",
    "            unforced = match_totals['unforced'].fillna(0)\n",
    "            unforced_fh = match_totals['unforced_fh'].fillna(0)\n",
    "            unforced_bh = match_totals['unforced_bh'].fillna(0)\n",
    "\n",
    "            gender_defaults.update({\n",
    "                'serve_pts': weighted_quantile(serve_pts, weights),\n",
    "                'aces': weighted_quantile(aces, weights),\n",
    "                'double_faults': weighted_quantile(dfs, weights),\n",
    "                'first_serve_pct': weighted_quantile(first_in / np.maximum(serve_pts, 1), weights),\n",
    "                'first_serve_won': weighted_quantile(first_won, weights),\n",
    "                'second_serve_won': weighted_quantile(second_won, weights),\n",
    "                'break_points_saved': weighted_quantile(bp_saved, weights),\n",
    "                'return_pts_won': weighted_quantile(return_pts_won, weights),\n",
    "                'winners_total': weighted_quantile(winners, weights),\n",
    "                'winners_fh': weighted_quantile(winners_fh, weights),\n",
    "                'winners_bh': weighted_quantile(winners_bh, weights),\n",
    "                'unforced_errors': weighted_quantile(unforced, weights),\n",
    "                'unforced_fh': weighted_quantile(unforced_fh, weights),\n",
    "                'unforced_bh': weighted_quantile(unforced_bh, weights)\n",
    "            })\n",
    "\n",
    "            # Calculate weighted composite indices from available data\n",
    "            serve_pts_vals = match_totals['serve_pts'].fillna(80)\n",
    "            winners_vals = match_totals['winners'].fillna(28)\n",
    "            unforced_vals = match_totals['unforced'].fillna(28)\n",
    "\n",
    "            aggression_values = (winners_vals / np.maximum(serve_pts_vals, 1) * 2 + 0.02 * 10 + 0.25 * 2) / 3\n",
    "            consistency_values = 1 - (unforced_vals / np.maximum(serve_pts_vals, 1) + 0.1 + 0.05) / 3\n",
    "\n",
    "            # Calculate pressure performance from key points data\n",
    "            pressure_values = []\n",
    "            if ('key_points_serve' in jeff_data[gender] and 'key_points_return' in jeff_data[gender]):\n",
    "                key_serve_df = jeff_data[gender]['key_points_serve']\n",
    "                key_return_df = jeff_data[gender]['key_points_return']\n",
    "\n",
    "                key_serve_totals = key_serve_df[key_serve_df['row'] == 'Total'].copy()\n",
    "                key_return_totals = key_return_df[key_return_df['row'] == 'Total'].copy()\n",
    "\n",
    "                if len(key_serve_totals) > 0 and len(key_return_totals) > 0:\n",
    "                    serve_won_pct = key_serve_totals['won'].fillna(0) / np.maximum(key_serve_totals['serve_pts'].fillna(1), 1)\n",
    "                    return_won_pct = key_return_totals['won'].fillna(0) / np.maximum(key_return_totals['return_pts'].fillna(1), 1)\n",
    "                    pressure_values = (serve_won_pct + return_won_pct) / 2\n",
    "\n",
    "            # Calculate net game strength\n",
    "            net_strength_values = []\n",
    "            if 'net_points' in jeff_data[gender]:\n",
    "                net_df = jeff_data[gender]['net_points']\n",
    "                net_totals = net_df[net_df['row'] == 'Total'].copy()\n",
    "                if len(net_totals) > 0:\n",
    "                    net_strength_values = net_totals['won'].fillna(0) / np.maximum(net_totals['total'].fillna(1), 1)\n",
    "\n",
    "            gender_defaults.update({\n",
    "                'aggression_index': max(0, min(1, weighted_quantile(aggression_values, weights))),\n",
    "                'consistency_index': max(0, min(1, weighted_quantile(consistency_values, weights))),\n",
    "                'pressure_performance': max(0, min(1, pressure_values.median())) if len(pressure_values) > 0 else 0.55,\n",
    "                'net_game_strength': max(0, min(1, net_strength_values.median())) if len(net_strength_values) > 0 else 0.6\n",
    "            })\n",
    "\n",
    "        # Serve direction defaults\n",
    "        if 'serve_direction' in jeff_data[gender]:\n",
    "            serve_dir_df = jeff_data[gender]['serve_direction']\n",
    "            serve_totals = serve_dir_df[serve_dir_df['row'] == 'Total'].copy()\n",
    "\n",
    "            if len(serve_totals) > 0:\n",
    "                total_wide = serve_totals['deuce_wide'].fillna(0) + serve_totals['ad_wide'].fillna(0)\n",
    "                total_t = serve_totals['deuce_t'].fillna(0) + serve_totals['ad_t'].fillna(0)\n",
    "                total_body = serve_totals['deuce_middle'].fillna(0) + serve_totals['ad_middle'].fillna(0)\n",
    "                total_serves = total_wide + total_t + total_body\n",
    "\n",
    "                gender_defaults.update({\n",
    "                    'serve_wide_pct': (total_wide / np.maximum(total_serves, 1)).median(),\n",
    "                    'serve_t_pct': (total_t / np.maximum(total_serves, 1)).median(),\n",
    "                    'serve_body_pct': (total_body / np.maximum(total_serves, 1)).median()\n",
    "                })\n",
    "\n",
    "        # Return depth defaults\n",
    "        if 'return_depth' in jeff_data[gender]:\n",
    "            return_depth_df = jeff_data[gender]['return_depth']\n",
    "            return_totals = return_depth_df[return_depth_df['row'] == 'Total'].copy()\n",
    "\n",
    "            if len(return_totals) > 0:\n",
    "                returnable = return_totals['returnable'].fillna(1)\n",
    "                deep = return_totals['deep'].fillna(0)\n",
    "                shallow = return_totals['shallow'].fillna(0)\n",
    "                very_deep = return_totals['very_deep'].fillna(0)\n",
    "\n",
    "                gender_defaults.update({\n",
    "                    'return_deep_pct': (deep / np.maximum(returnable, 1)).median(),\n",
    "                    'return_shallow_pct': (shallow / np.maximum(returnable, 1)).median(),\n",
    "                    'return_very_deep_pct': (very_deep / np.maximum(returnable, 1)).median()\n",
    "                })\n",
    "\n",
    "        # Key points defaults\n",
    "        if 'key_points_serve' in jeff_data[gender]:\n",
    "            key_serve_df = jeff_data[gender]['key_points_serve']\n",
    "            key_serve_totals = key_serve_df[key_serve_df['row'] == 'Total'].copy()\n",
    "\n",
    "            if len(key_serve_totals) > 0:\n",
    "                serve_pts = key_serve_totals['serve_pts'].fillna(1)\n",
    "                won = key_serve_totals['won'].fillna(0)\n",
    "                aces = key_serve_totals['aces'].fillna(0)\n",
    "                first_in = key_serve_totals['first_in'].fillna(0)\n",
    "\n",
    "                gender_defaults.update({\n",
    "                    'key_points_serve_won_pct': (won / np.maximum(serve_pts, 1)).median(),\n",
    "                    'key_points_aces_pct': (aces / np.maximum(serve_pts, 1)).median(),\n",
    "                    'key_points_first_in_pct': (first_in / np.maximum(serve_pts, 1)).median()\n",
    "                })\n",
    "\n",
    "        if 'key_points_return' in jeff_data[gender]:\n",
    "            key_return_df = jeff_data[gender]['key_points_return']\n",
    "            key_return_totals = key_return_df[key_return_df['row'] == 'Total'].copy()\n",
    "\n",
    "            if len(key_return_totals) > 0:\n",
    "                return_pts = key_return_totals['return_pts'].fillna(1)\n",
    "                won = key_return_totals['won'].fillna(0)\n",
    "                winners = key_return_totals['winners'].fillna(0)\n",
    "\n",
    "                gender_defaults.update({\n",
    "                    'key_points_return_won_pct': (won / np.maximum(return_pts, 1)).median(),\n",
    "                    'key_points_return_winners': (winners / np.maximum(return_pts, 1)).median()\n",
    "                })\n",
    "\n",
    "        # Net points defaults\n",
    "        if 'net_points' in jeff_data[gender]:\n",
    "            net_df = jeff_data[gender]['net_points']\n",
    "            net_totals = net_df[net_df['row'] == 'Total'].copy()\n",
    "\n",
    "            if len(net_totals) > 0:\n",
    "                total_net = net_totals['total'].fillna(1)\n",
    "                won = net_totals['won'].fillna(0)\n",
    "                winners = net_totals['winners'].fillna(0)\n",
    "                passed = net_totals['passed'].fillna(0)\n",
    "\n",
    "                gender_defaults.update({\n",
    "                    'net_points_won_pct': (won / np.maximum(total_net, 1)).median(),\n",
    "                    'net_winners_pct': (winners / np.maximum(total_net, 1)).median(),\n",
    "                    'passed_at_net_pct': (passed / np.maximum(total_net, 1)).median()\n",
    "                })\n",
    "\n",
    "        # Add remaining defaults from fallback\n",
    "        fallback = get_fallback_defaults('men' if gender == 'men' else 'women')\n",
    "        for key, value in fallback.items():\n",
    "            if key not in gender_defaults:\n",
    "                gender_defaults[key] = value\n",
    "\n",
    "        defaults[gender] = gender_defaults\n",
    "        print(f\"Calculated comprehensive weighted defaults for {gender}: {len(gender_defaults)} features\")\n",
    "\n",
    "    return defaults\n",
    "\n",
    "def extract_comprehensive_jeff_features(player_canonical, gender, jeff_data, weighted_defaults=None):\n",
    "    gender_key = 'men' if gender == 'M' else 'women'\n",
    "\n",
    "    if gender_key not in jeff_data:\n",
    "        return get_fallback_defaults(gender_key)\n",
    "\n",
    "    if weighted_defaults and gender_key in weighted_defaults:\n",
    "        features = weighted_defaults[gender_key].copy()\n",
    "    else:\n",
    "        features = get_fallback_defaults(gender_key)\n",
    "\n",
    "    # Overview stats\n",
    "    if 'overview' in jeff_data[gender_key]:\n",
    "        overview_df = jeff_data[gender_key]['overview']\n",
    "        if 'Player_canonical' in overview_df.columns:\n",
    "            player_overview = overview_df[\n",
    "                (overview_df['Player_canonical'] == player_canonical) &\n",
    "                (overview_df['set'] == 'Total')\n",
    "            ]\n",
    "\n",
    "            if len(player_overview) > 0:\n",
    "                latest = player_overview.iloc[-1]\n",
    "                serve_pts = latest.get('serve_pts', 80)\n",
    "                if serve_pts > 0:\n",
    "                    features.update({\n",
    "                        'serve_pts': serve_pts,\n",
    "                        'aces': latest.get('aces', 0),\n",
    "                        'double_faults': latest.get('dfs', 0),\n",
    "                        'first_serve_pct': latest.get('first_in', 0) / serve_pts,\n",
    "                        'first_serve_won': latest.get('first_won', 0),\n",
    "                        'second_serve_won': latest.get('second_won', 0),\n",
    "                        'break_points_saved': latest.get('bp_saved', 0),\n",
    "                        'return_pts_won': latest.get('return_pts_won', 0),\n",
    "                        'winners_total': latest.get('winners', 0),\n",
    "                        'winners_fh': latest.get('winners_fh', 0),\n",
    "                        'winners_bh': latest.get('winners_bh', 0),\n",
    "                        'unforced_errors': latest.get('unforced', 0),\n",
    "                        'unforced_fh': latest.get('unforced_fh', 0),\n",
    "                        'unforced_bh': latest.get('unforced_bh', 0)\n",
    "                    })\n",
    "\n",
    "    return features\n",
    "\n",
    "def calculate_playing_style_indices(features):\n",
    "    \"\"\"Calculate playing style composite indices\"\"\"\n",
    "\n",
    "    # Aggression Index\n",
    "    aggression = (\n",
    "        features.get('winners_total', 0) / max(1, features.get('serve_pts', 60)) * 2 +\n",
    "        features.get('serve_volley_frequency', 0) * 10 +\n",
    "        features.get('shot_down_line_pct', 0) * 2\n",
    "    ) / 3\n",
    "\n",
    "    # Consistency Index\n",
    "    consistency = 1 - (\n",
    "        features.get('unforced_errors', 0) / max(1, features.get('serve_pts', 60)) +\n",
    "        features.get('return_error_net_pct', 0) +\n",
    "        features.get('return_error_wide_pct', 0)\n",
    "    ) / 3\n",
    "\n",
    "    # Pressure Performance\n",
    "    pressure_perf = (\n",
    "        features.get('key_points_serve_won_pct', 0) +\n",
    "        features.get('key_points_return_won_pct', 0)\n",
    "    ) / 2\n",
    "\n",
    "    # Net Game Strength\n",
    "    net_game_strength = features.get('net_points_won_pct', 0)\n",
    "\n",
    "    return {\n",
    "        'aggression_index': max(0, min(1, aggression)),\n",
    "        'consistency_index': max(0, min(1, consistency)),\n",
    "        'pressure_performance': max(0, min(1, pressure_perf)),\n",
    "        'net_game_strength': max(0, min(1, net_game_strength))\n",
    "    }\n",
    "\n",
    "\n",
    "def get_fallback_defaults(gender_key):\n",
    "    \"\"\"Fallback defaults when no Jeff data available\"\"\"\n",
    "    base_defaults = {\n",
    "        'serve_pts': 80,\n",
    "        'aces': 6,\n",
    "        'double_faults': 3,\n",
    "        'first_serve_pct': 0.62,\n",
    "        'first_serve_won': 35,\n",
    "        'second_serve_won': 16,\n",
    "        'break_points_saved': 4,\n",
    "        'return_pts_won': 30,\n",
    "        'winners_total': 28,\n",
    "        'winners_fh': 16,\n",
    "        'winners_bh': 12,\n",
    "        'unforced_errors': 28,\n",
    "        'unforced_fh': 16,\n",
    "        'unforced_bh': 12,\n",
    "        'serve_wide_pct': 0.3,\n",
    "        'serve_t_pct': 0.4,\n",
    "        'serve_body_pct': 0.3,\n",
    "        'return_deep_pct': 0.4,\n",
    "        'return_shallow_pct': 0.3,\n",
    "        'return_very_deep_pct': 0.2,\n",
    "        'key_points_serve_won_pct': 0.6,\n",
    "        'key_points_aces_pct': 0.05,\n",
    "        'key_points_first_in_pct': 0.55,\n",
    "        'key_points_return_won_pct': 0.35,\n",
    "        'key_points_return_winners': 0.02,\n",
    "        'net_points_won_pct': 0.65,\n",
    "        'net_winners_pct': 0.3,\n",
    "        'passed_at_net_pct': 0.3,\n",
    "        'rally_server_winners_pct': 0.15,\n",
    "        'rally_server_unforced_pct': 0.2,\n",
    "        'rally_returner_winners_pct': 0.1,\n",
    "        'rally_returner_unforced_pct': 0.25,\n",
    "        'shot_crosscourt_pct': 0.5,\n",
    "        'shot_down_line_pct': 0.25,\n",
    "        'shot_inside_out_pct': 0.15,\n",
    "        'serve_volley_frequency': 0.02,\n",
    "        'serve_volley_success_pct': 0.6,\n",
    "        'return_error_net_pct': 0.1,\n",
    "        'return_error_wide_pct': 0.05,\n",
    "        'aggression_index': 0.5,\n",
    "        'consistency_index': 0.5,\n",
    "        'pressure_performance': 0.5,\n",
    "        'net_game_strength': 0.5\n",
    "    }\n",
    "\n",
    "    # Adjust for gender differences\n",
    "    if gender_key == 'women':\n",
    "        base_defaults.update({\n",
    "            'serve_pts': 75,\n",
    "            'aces': 4,\n",
    "            'first_serve_pct': 0.60,\n",
    "            'first_serve_won': 32,\n",
    "            'second_serve_won': 15,\n",
    "            'serve_volley_frequency': 0.01,\n",
    "            'net_points_won_pct': 0.60\n",
    "        })\n",
    "\n",
    "    return base_defaults\n",
    "\n",
    "def load_all_tennis_data():\n",
    "    \"\"\"Load tennis data from all years\"\"\"\n",
    "    base_path = os.path.expanduser(\"~/Desktop/data\")\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    # Men's data\n",
    "    men_path = os.path.join(base_path, \"tennisdata_men\")\n",
    "    if os.path.exists(men_path):\n",
    "        for year in range(2020, 2026):\n",
    "            file_path = os.path.join(men_path, f\"{year}_m.xlsx\")\n",
    "            if os.path.exists(file_path):\n",
    "                df = load_excel_data(file_path)\n",
    "                if not df.empty and 'Date' in df.columns:\n",
    "                    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "                    df['gender'] = 'M'\n",
    "                    df['year'] = df['Date'].dt.year\n",
    "                    all_data.append(df)\n",
    "\n",
    "    # Women's data\n",
    "    women_path = os.path.join(base_path, \"tennisdata_women\")\n",
    "    if os.path.exists(women_path):\n",
    "        for year in range(2020, 2026):\n",
    "            file_path = os.path.join(women_path, f\"{year}_w.xlsx\")\n",
    "            if os.path.exists(file_path):\n",
    "                df = load_excel_data(file_path)\n",
    "                if not df.empty and 'Date' in df.columns:\n",
    "                    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "                    df['gender'] = 'W'\n",
    "                    df['year'] = df['Date'].dt.year\n",
    "                    all_data.append(df)\n",
    "\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def calculate_data_quality_v2(row):\n",
    "    score = 0\n",
    "    max_possible = 0\n",
    "\n",
    "    # Core match info (always counts) - 30%\n",
    "    if pd.notna(row.get('Winner')) and pd.notna(row.get('Loser')):\n",
    "        score += 0.3\n",
    "    max_possible += 0.3\n",
    "\n",
    "    # Ranking data (when available) - 30%\n",
    "    if pd.notna(row.get('WRank')) and pd.notna(row.get('LRank')):\n",
    "        score += 0.3\n",
    "        max_possible += 0.3\n",
    "    elif row.get('date', date(2000,1,1)) >= date(2025, 6, 11):\n",
    "        max_possible += 0.3\n",
    "\n",
    "    # Odds data (when available) - 20%\n",
    "    if pd.notna(row.get('tennis_data_odds1')):\n",
    "        score += 0.2\n",
    "        max_possible += 0.2\n",
    "\n",
    "    # Jeff features (when available) - 20%\n",
    "    jeff_available = any(pd.notna(row.get(f'winner_{feat}')) for feat in ['aces', 'serve_pts'])\n",
    "    if jeff_available:\n",
    "        score += 0.2\n",
    "        max_possible += 0.2\n",
    "    elif row.get('date', date(2000,1,1)) <= date(2025, 6, 10):\n",
    "        max_possible += 0.2\n",
    "\n",
    "    return score / max_possible if max_possible > 0 else 0\n",
    "\n",
    "def extract_comprehensive_jeff_features(player_canonical, gender, jeff_data, weighted_defaults=None):\n",
    "    \"\"\"Extract features from all Jeff datasets with Player_canonical checks\"\"\"\n",
    "    gender_key = 'men' if gender == 'M' else 'women'\n",
    "\n",
    "    if gender_key not in jeff_data:\n",
    "        return get_fallback_defaults(gender_key)\n",
    "\n",
    "    if weighted_defaults and gender_key in weighted_defaults:\n",
    "        features = weighted_defaults[gender_key].copy()\n",
    "    else:\n",
    "        features = get_fallback_defaults(gender_key)\n",
    "\n",
    "    # Overview stats\n",
    "    if 'overview' in jeff_data[gender_key]:\n",
    "        overview_df = jeff_data[gender_key]['overview']\n",
    "        if 'Player_canonical' in overview_df.columns:\n",
    "            player_overview = overview_df[\n",
    "                (overview_df['Player_canonical'] == player_canonical) &\n",
    "                (overview_df['set'] == 'Total')\n",
    "            ]\n",
    "\n",
    "            if len(player_overview) > 0:\n",
    "                latest = player_overview.iloc[-1]\n",
    "                serve_pts = latest.get('serve_pts', 80)\n",
    "                if serve_pts > 0:\n",
    "                    features.update({\n",
    "                        'serve_pts': serve_pts,\n",
    "                        'aces': latest.get('aces', 0),\n",
    "                        'double_faults': latest.get('dfs', 0),\n",
    "                        'first_serve_pct': latest.get('first_in', 0) / serve_pts,\n",
    "                        'first_serve_won': latest.get('first_won', 0),\n",
    "                        'second_serve_won': latest.get('second_won', 0),\n",
    "                        'break_points_saved': latest.get('bp_saved', 0),\n",
    "                        'return_pts_won': latest.get('return_pts_won', 0),\n",
    "                        'winners_total': latest.get('winners', 0),\n",
    "                        'winners_fh': latest.get('winners_fh', 0),\n",
    "                        'winners_bh': latest.get('winners_bh', 0),\n",
    "                        'unforced_errors': latest.get('unforced', 0),\n",
    "                        'unforced_fh': latest.get('unforced_fh', 0),\n",
    "                        'unforced_bh': latest.get('unforced_bh', 0)\n",
    "                    })\n",
    "\n",
    "    # Serve direction patterns\n",
    "    if 'serve_direction' in jeff_data[gender_key]:\n",
    "        serve_dir_df = jeff_data[gender_key]['serve_direction']\n",
    "        if 'Player_canonical' in serve_dir_df.columns:\n",
    "            player_serve_dir = serve_dir_df[\n",
    "                (serve_dir_df['Player_canonical'] == player_canonical) &\n",
    "                (serve_dir_df['row'] == 'Total')\n",
    "            ]\n",
    "\n",
    "            if len(player_serve_dir) > 0:\n",
    "                latest = player_serve_dir.iloc[-1]\n",
    "                total_serves = (latest.get('deuce_wide', 0) + latest.get('deuce_t', 0) +\n",
    "                              latest.get('ad_wide', 0) + latest.get('ad_t', 0))\n",
    "\n",
    "                if total_serves > 0:\n",
    "                    features.update({\n",
    "                        'serve_wide_pct': (latest.get('deuce_wide', 0) + latest.get('ad_wide', 0)) / total_serves,\n",
    "                        'serve_t_pct': (latest.get('deuce_t', 0) + latest.get('ad_t', 0)) / total_serves,\n",
    "                        'serve_body_pct': (latest.get('deuce_middle', 0) + latest.get('ad_middle', 0)) / total_serves\n",
    "                    })\n",
    "\n",
    "    # Return depth\n",
    "    if 'return_depth' in jeff_data[gender_key]:\n",
    "        return_depth_df = jeff_data[gender_key]['return_depth']\n",
    "        if 'Player_canonical' in return_depth_df.columns:\n",
    "            player_return_depth = return_depth_df[\n",
    "                (return_depth_df['Player_canonical'] == player_canonical) &\n",
    "                (return_depth_df['row'] == 'Total')\n",
    "            ]\n",
    "\n",
    "            if len(player_return_depth) > 0:\n",
    "                latest = player_return_depth.iloc[-1]\n",
    "                returnable = latest.get('returnable', 1)\n",
    "\n",
    "                if returnable > 0:\n",
    "                    features.update({\n",
    "                        'return_deep_pct': latest.get('deep', 0) / returnable,\n",
    "                        'return_shallow_pct': latest.get('shallow', 0) / returnable,\n",
    "                        'return_very_deep_pct': latest.get('very_deep', 0) / returnable\n",
    "                    })\n",
    "\n",
    "    # Key points serve\n",
    "    if 'key_points_serve' in jeff_data[gender_key]:\n",
    "        key_serve_df = jeff_data[gender_key]['key_points_serve']\n",
    "        if 'Player_canonical' in key_serve_df.columns:\n",
    "            player_key_serve = key_serve_df[\n",
    "                (key_serve_df['Player_canonical'] == player_canonical) &\n",
    "                (key_serve_df['row'] == 'Total')\n",
    "            ]\n",
    "\n",
    "            if len(player_key_serve) > 0:\n",
    "                latest = player_key_serve.iloc[-1]\n",
    "                key_serve_pts = latest.get('serve_pts', 1)\n",
    "\n",
    "                if key_serve_pts > 0:\n",
    "                    features.update({\n",
    "                        'key_points_serve_won_pct': latest.get('won', 0) / key_serve_pts,\n",
    "                        'key_points_aces_pct': latest.get('aces', 0) / key_serve_pts,\n",
    "                        'key_points_first_in_pct': latest.get('first_in', 0) / key_serve_pts\n",
    "                    })\n",
    "\n",
    "    # Key points return\n",
    "    if 'key_points_return' in jeff_data[gender_key]:\n",
    "        key_return_df = jeff_data[gender_key]['key_points_return']\n",
    "        if 'Player_canonical' in key_return_df.columns:\n",
    "            player_key_return = key_return_df[\n",
    "                (key_return_df['Player_canonical'] == player_canonical) &\n",
    "                (key_return_df['row'] == 'Total')\n",
    "            ]\n",
    "\n",
    "            if len(player_key_return) > 0:\n",
    "                latest = player_key_return.iloc[-1]\n",
    "                return_pts = latest.get('return_pts', 1)\n",
    "\n",
    "                if return_pts > 0:\n",
    "                    features.update({\n",
    "                        'key_points_return_won_pct': latest.get('won', 0) / return_pts,\n",
    "                        'key_points_return_winners': latest.get('winners', 0) / return_pts\n",
    "                    })\n",
    "\n",
    "    # Net points\n",
    "    if 'net_points' in jeff_data[gender_key]:\n",
    "        net_df = jeff_data[gender_key]['net_points']\n",
    "        if 'Player_canonical' in net_df.columns:\n",
    "            player_net = net_df[\n",
    "                (net_df['Player_canonical'] == player_canonical) &\n",
    "                (net_df['row'] == 'Total')\n",
    "            ]\n",
    "\n",
    "            if len(player_net) > 0:\n",
    "                latest = player_net.iloc[-1]\n",
    "                net_pts = latest.get('total', 1)\n",
    "\n",
    "                if net_pts > 0:\n",
    "                    features.update({\n",
    "                        'net_points_won_pct': latest.get('won', 0) / net_pts,\n",
    "                        'net_winners_pct': latest.get('winners', 0) / net_pts,\n",
    "                        'passed_at_net_pct': latest.get('passed', 0) / net_pts\n",
    "                    })\n",
    "\n",
    "    # Rally patterns\n",
    "    if 'rally' in jeff_data[gender_key]:\n",
    "        rally_df = jeff_data[gender_key]['rally']\n",
    "        if 'Player_canonical' in rally_df.columns:\n",
    "            player_rally = rally_df[\n",
    "                (rally_df['Player_canonical'] == player_canonical) &\n",
    "                (rally_df['row'] == 'Total')\n",
    "            ]\n",
    "\n",
    "            if len(player_rally) > 0:\n",
    "                latest = player_rally.iloc[-1]\n",
    "                rally_pts = latest.get('total', 1)\n",
    "\n",
    "                if rally_pts > 0:\n",
    "                    features.update({\n",
    "                        'rally_server_winners_pct': latest.get('server_winners', 0) / rally_pts,\n",
    "                        'rally_server_unforced_pct': latest.get('server_unforced', 0) / rally_pts,\n",
    "                        'rally_returner_winners_pct': latest.get('returner_winners', 0) / rally_pts,\n",
    "                        'rally_returner_unforced_pct': latest.get('returner_unforced', 0) / rally_pts\n",
    "                    })\n",
    "\n",
    "    # Shot direction\n",
    "    if 'shot_direction' in jeff_data[gender_key]:\n",
    "        shot_dir_df = jeff_data[gender_key]['shot_direction']\n",
    "        if 'Player_canonical' in shot_dir_df.columns:\n",
    "            player_shot_dir = shot_dir_df[\n",
    "                (shot_dir_df['Player_canonical'] == player_canonical) &\n",
    "                (shot_dir_df['row'] == 'Total')\n",
    "            ]\n",
    "\n",
    "            if len(player_shot_dir) > 0:\n",
    "                latest = player_shot_dir.iloc[-1]\n",
    "                total_shots = latest.get('total', 1)\n",
    "\n",
    "                if total_shots > 0:\n",
    "                    features.update({\n",
    "                        'shot_crosscourt_pct': latest.get('crosscourt', 0) / total_shots,\n",
    "                        'shot_down_line_pct': latest.get('down_line', 0) / total_shots,\n",
    "                        'shot_inside_out_pct': latest.get('inside_out', 0) / total_shots\n",
    "                    })\n",
    "\n",
    "    # Serve and volley\n",
    "    if 'snv' in jeff_data[gender_key]:\n",
    "        snv_df = jeff_data[gender_key]['snv']\n",
    "        if 'Player_canonical' in snv_df.columns:\n",
    "            player_snv = snv_df[\n",
    "                (snv_df['Player_canonical'] == player_canonical) &\n",
    "                (snv_df['row'] == 'Total')\n",
    "            ]\n",
    "\n",
    "            if len(player_snv) > 0:\n",
    "                latest = player_snv.iloc[-1]\n",
    "                serve_pts = latest.get('serve_pts', 1)\n",
    "                snv_pts = latest.get('snv', 0)\n",
    "\n",
    "                if serve_pts > 0:\n",
    "                    features.update({\n",
    "                        'serve_volley_frequency': snv_pts / serve_pts,\n",
    "                        'serve_volley_success_pct': latest.get('snv_won', 0) / max(1, snv_pts)\n",
    "                    })\n",
    "\n",
    "    # Return outcomes\n",
    "    if 'return_outcomes' in jeff_data[gender_key]:\n",
    "        return_outcomes_df = jeff_data[gender_key]['return_outcomes']\n",
    "        if 'Player_canonical' in return_outcomes_df.columns:\n",
    "            player_return_outcomes = return_outcomes_df[\n",
    "                (return_outcomes_df['Player_canonical'] == player_canonical) &\n",
    "                (return_outcomes_df['row'] == 'Total')\n",
    "            ]\n",
    "\n",
    "            if len(player_return_outcomes) > 0:\n",
    "                latest = player_return_outcomes.iloc[-1]\n",
    "                return_attempts = latest.get('return_attempts', 1)\n",
    "\n",
    "                if return_attempts > 0:\n",
    "                    features.update({\n",
    "                        'return_error_net_pct': latest.get('net', 0) / return_attempts,\n",
    "                        'return_error_wide_pct': latest.get('wide', 0) / return_attempts\n",
    "                    })\n",
    "\n",
    "    # Calculate composite indices\n",
    "    features.update(calculate_playing_style_indices(features))\n",
    "\n",
    "    return features\n",
    "\n",
    "# Clear corrupted cache and regenerate data\n",
    "import os, pickle, pandas as pd\n",
    "from datetime import date\n",
    "import shutil\n",
    "\n",
    "CACHE_DIR = os.path.expanduser(\"~/Desktop/data/cache\")\n",
    "HD_PATH   = os.path.join(CACHE_DIR, \"historical_data.parquet\")\n",
    "JEFF_PATH = os.path.join(CACHE_DIR, \"jeff_data.pkl\")\n",
    "DEF_PATH  = os.path.join(CACHE_DIR, \"weighted_defaults.pkl\")\n",
    "\n",
    "print(\"=== CLEARING CORRUPTED CACHE ===\")\n",
    "# Remove corrupted cache files\n",
    "for file_path in [HD_PATH, JEFF_PATH, DEF_PATH]:\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Removed: {file_path}\")\n",
    "\n",
    "# Remove entire cache directory and recreate\n",
    "if os.path.exists(CACHE_DIR):\n",
    "    shutil.rmtree(CACHE_DIR)\n",
    "    print(f\"Removed cache directory: {CACHE_DIR}\")\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "print(\"Created fresh cache directory\")\n",
    "\n",
    "def generate_comprehensive_historical_all_years_fixed(*, fast: bool = False, n_sample: int = 500):\n",
    "    \"\"\"Fixed version of data generation function\"\"\"\n",
    "    print(\"=== STARTING DATA GENERATION ===\")\n",
    "\n",
    "    # Step 1: Load Jeff's data\n",
    "    print(\"Step 1: Loading Jeff's comprehensive data...\")\n",
    "    try:\n",
    "        jeff_data = load_jeff_comprehensive_data()\n",
    "        if not jeff_data or ('men' not in jeff_data and 'women' not in jeff_data):\n",
    "            print(\"ERROR: Jeff data loading failed\")\n",
    "            return pd.DataFrame(), {}, {}\n",
    "\n",
    "        print(f\"✓ Jeff data loaded successfully\")\n",
    "        print(f\"  - Men's datasets: {len(jeff_data.get('men', {}))}\")\n",
    "        print(f\"  - Women's datasets: {len(jeff_data.get('women', {}))}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading Jeff data: {e}\")\n",
    "        return pd.DataFrame(), {}, {}\n",
    "\n",
    "    # Step 2: Calculate weighted defaults\n",
    "    print(\"Step 2: Calculating weighted defaults...\")\n",
    "    try:\n",
    "        weighted_defaults = calculate_comprehensive_weighted_defaults(jeff_data)\n",
    "        if not weighted_defaults:\n",
    "            print(\"ERROR: Weighted defaults calculation failed\")\n",
    "            return pd.DataFrame(), jeff_data, {}\n",
    "\n",
    "        print(f\"✓ Weighted defaults calculated\")\n",
    "        print(f\"  - Men's features: {len(weighted_defaults.get('men', {}))}\")\n",
    "        print(f\"  - Women's features: {len(weighted_defaults.get('women', {}))}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR calculating weighted defaults: {e}\")\n",
    "        return pd.DataFrame(), jeff_data, {}\n",
    "\n",
    "    # Step 3: Load tennis match data\n",
    "    print(\"Step 3: Loading tennis match data...\")\n",
    "    try:\n",
    "        tennis_data = load_all_tennis_data()\n",
    "        if tennis_data.empty:\n",
    "            print(\"ERROR: No tennis data loaded\")\n",
    "            return pd.DataFrame(), jeff_data, weighted_defaults\n",
    "\n",
    "        print(f\"✓ Tennis data loaded: {len(tennis_data)} matches\")\n",
    "        # --------------------------------------------------------------\n",
    "        # Optional fast‑mode: work on a random subset for quick testing\n",
    "        if fast:\n",
    "            total_rows = len(tennis_data)\n",
    "            take = min(n_sample, total_rows)\n",
    "            tennis_data = tennis_data.sample(take, random_state=1).reset_index(drop=True)\n",
    "            print(f\"[fast‑mode] using sample of {take}/{total_rows} rows\")\n",
    "        # --------------------------------------------------------------\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading tennis data: {e}\")\n",
    "        return pd.DataFrame(), jeff_data, weighted_defaults\n",
    "\n",
    "    # Step 4: Process tennis data\n",
    "    print(\"Step 4: Processing tennis data...\")\n",
    "    try:\n",
    "        # Normalize player names\n",
    "        tennis_data['winner_canonical'] = tennis_data['Winner'].apply(normalize_name)\n",
    "        tennis_data['loser_canonical'] = tennis_data['Loser'].apply(normalize_name)\n",
    "        tennis_data['tournament_canonical'] = tennis_data['Tournament'].apply(normalize_tournament_name)\n",
    "\n",
    "        # Fix dates\n",
    "        tennis_data['Date'] = pd.to_datetime(tennis_data['Date'], errors='coerce')\n",
    "        tennis_data['date'] = tennis_data['Date'].dt.date\n",
    "\n",
    "        # Add odds data\n",
    "        tennis_data['tennis_data_odds1'] = pd.to_numeric(tennis_data.get('PSW', tennis_data.get('AvgW', 0)), errors='coerce')\n",
    "        tennis_data['tennis_data_odds2'] = pd.to_numeric(tennis_data.get('PSL', tennis_data.get('AvgL', 0)), errors='coerce')\n",
    "\n",
    "        # --- Odds cleaning and derived columns ---\n",
    "        # Average and min/max odds (if available)\n",
    "        tennis_data['odds_win_avg'] = pd.to_numeric(tennis_data.get('AvgW', tennis_data.get('PSW', 0)), errors='coerce')\n",
    "        tennis_data['odds_lose_avg'] = pd.to_numeric(tennis_data.get('AvgL', tennis_data.get('PSL', 0)), errors='coerce')\n",
    "        # Preserve opening prices\n",
    "        tennis_data['odds_win_open']  = tennis_data.get('PSW')\n",
    "        tennis_data['odds_lose_open'] = tennis_data.get('PSL')\n",
    "        # Calculate line move (bps) using opening odds\n",
    "        tennis_data['line_move_bps'] = (tennis_data['odds_win_avg'] - tennis_data['odds_win_open']) / tennis_data['odds_win_open']\n",
    "\n",
    "        # Add ranking difference\n",
    "        if 'WRank' in tennis_data.columns and 'LRank' in tennis_data.columns:\n",
    "            tennis_data['rank_difference'] = abs(tennis_data['WRank'] - tennis_data['LRank'])\n",
    "\n",
    "        # Drop raw odds columns (keep cleaned/derived)\n",
    "        tennis_data.drop(\n",
    "            columns=[c for c in ['PSW','PSL','AvgW','AvgL','MaxW','MaxL','MinW','MinL'] if c in tennis_data.columns],\n",
    "            inplace=True,\n",
    "            errors='ignore'\n",
    "        )\n",
    "\n",
    "        print(f\"✓ Tennis data processed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing tennis data: {e}\")\n",
    "        return pd.DataFrame(), jeff_data, weighted_defaults\n",
    "\n",
    "    # Step 5: Add Jeff feature columns\n",
    "    print(\"Step 5: Adding Jeff feature columns...\")\n",
    "    try:\n",
    "        all_jeff_features = [\n",
    "            'serve_pts', 'aces', 'double_faults', 'first_serve_pct', 'first_serve_won',\n",
    "            'second_serve_won', 'break_points_saved', 'return_pts_won',\n",
    "            'winners_total', 'winners_fh', 'winners_bh', 'unforced_errors', 'unforced_fh', 'unforced_bh',\n",
    "            'serve_wide_pct', 'serve_t_pct', 'serve_body_pct',\n",
    "            'return_deep_pct', 'return_shallow_pct', 'return_very_deep_pct',\n",
    "            'key_points_serve_won_pct', 'key_points_aces_pct', 'key_points_first_in_pct',\n",
    "            'key_points_return_won_pct', 'key_points_return_winners',\n",
    "            'net_points_won_pct', 'net_winners_pct', 'passed_at_net_pct',\n",
    "            'rally_server_winners_pct', 'rally_server_unforced_pct',\n",
    "            'rally_returner_winners_pct', 'rally_returner_unforced_pct',\n",
    "            'shot_crosscourt_pct', 'shot_down_line_pct', 'shot_inside_out_pct',\n",
    "            'serve_volley_frequency', 'serve_volley_success_pct',\n",
    "            'return_error_net_pct', 'return_error_wide_pct',\n",
    "            'aggression_index', 'consistency_index', 'pressure_performance', 'net_game_strength'\n",
    "        ]\n",
    "\n",
    "        # ---- fast column initialisation (no fragmentation) -----------------\n",
    "        winner_cols = {f\"winner_{f}\": pd.NA for f in all_jeff_features}\n",
    "        loser_cols  = {f\"loser_{f}\":  pd.NA for f in all_jeff_features}\n",
    "\n",
    "        feature_df  = pd.DataFrame(winner_cols | loser_cols, index=tennis_data.index)\n",
    "        tennis_data = pd.concat([tennis_data, feature_df], axis=1, copy=False)\n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        print(f\"✓ Added {len(all_jeff_features) * 2} feature columns\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR adding feature columns: {e}\")\n",
    "        return pd.DataFrame(), jeff_data, weighted_defaults\n",
    "\n",
    "    # Step 6: Extract Jeff features\n",
    "    print(\"Step 6: Extracting Jeff features...\")\n",
    "    try:\n",
    "        total_matches = len(tennis_data)\n",
    "        matches_with_jeff_features = 0\n",
    "\n",
    "        # Test feature extraction first\n",
    "        if 'men' in jeff_data and 'overview' in jeff_data['men']:\n",
    "            test_player = jeff_data['men']['overview']['Player_canonical'].iloc[0]\n",
    "            test_features = extract_comprehensive_jeff_features(test_player, 'M', jeff_data, weighted_defaults)\n",
    "            print(f\"✓ Feature extraction test passed for {test_player}\")\n",
    "            print(f\"  Sample features: serve_pts={test_features.get('serve_pts', 'N/A')}\")\n",
    "\n",
    "        for idx, row in tennis_data.iterrows():\n",
    "            if idx % 1000 == 0:\n",
    "                print(f\"  Processing match {idx}/{total_matches}\")\n",
    "\n",
    "            try:\n",
    "                gender = row['gender']\n",
    "\n",
    "                # Only extract Jeff features for matches before cutoff\n",
    "                if row['date'] <= date(2025, 6, 10):\n",
    "                    winner_features = extract_comprehensive_jeff_features(\n",
    "                        row['winner_canonical'], gender, jeff_data, weighted_defaults\n",
    "                    )\n",
    "                    loser_features = extract_comprehensive_jeff_features(\n",
    "                        row['loser_canonical'], gender, jeff_data, weighted_defaults\n",
    "                    )\n",
    "\n",
    "                    # Assign features\n",
    "                    for feature_name, feature_value in winner_features.items():\n",
    "                        col_name = f'winner_{feature_name}'\n",
    "                        if col_name in tennis_data.columns:\n",
    "                            tennis_data.at[idx, col_name] = feature_value\n",
    "\n",
    "                    for feature_name, feature_value in loser_features.items():\n",
    "                        col_name = f'loser_{feature_name}'\n",
    "                        if col_name in tennis_data.columns:\n",
    "                            tennis_data.at[idx, col_name] = feature_value\n",
    "\n",
    "                    if winner_features and loser_features:\n",
    "                        matches_with_jeff_features += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                if idx < 10:  # Only print first few errors\n",
    "                    print(f\"  Warning: Error processing match {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"✓ Jeff features extracted for {matches_with_jeff_features}/{total_matches} matches\")\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # Step 7: Append API‑Tennis fixtures/completed matches since Jeff cutoff\n",
    "        # ------------------------------------------------------------------\n",
    "        print(\"Step 7: Appending API‑Tennis data …\")\n",
    "        from datetime import timedelta\n",
    "\n",
    "        def _api_rows_for_day(day):\n",
    "            rows = []\n",
    "            try:\n",
    "                fixtures = fetch_day(day)\n",
    "            except Exception as err:\n",
    "                print(f\"  ⚠️  API error for {day}: {err}\")\n",
    "                return rows\n",
    "\n",
    "            for fx in fixtures:\n",
    "                # ---- finished‑match filter --------------------------------\n",
    "                status_raw = fx.get(\"status\") or fx.get(\"event_status\") or \"\"\n",
    "                status     = str(status_raw).lower()\n",
    "                if not any(s in status for s in (\"completed\", \"finished\", \"ended\", \"ft\", \"fulltime\")):\n",
    "                    continue  # skip in‑progress / scheduled fixtures\n",
    "                # ---- winner / loser names ---------------------------------\n",
    "                winner = fx.get(\"winner_name\") or fx.get(\"event_winner_name\")\n",
    "                loser  = fx.get(\"opponent_name\")\n",
    "                if not winner:\n",
    "                    # derive from event_winner index + first/second player fields\n",
    "                    who = str(fx.get(\"event_winner\", \"\")).strip()\n",
    "                    p1  = fx.get(\"event_first_player\")\n",
    "                    p2  = fx.get(\"event_second_player\")\n",
    "                    if who == \"1\":\n",
    "                        winner, loser = p1, p2\n",
    "                    elif who == \"2\":\n",
    "                        winner, loser = p2, p1\n",
    "                if not winner or not loser:\n",
    "                    continue  # cannot resolve players\n",
    "\n",
    "                # ---- other fields -----------------------------------------\n",
    "                match_date = datetime.strptime(fx.get(\"start_date\") or fx.get(\"event_date\"), \"%Y-%m-%d\").date()\n",
    "                surface    = determine_surface(fx.get(\"tournament_name\", \"\"))\n",
    "\n",
    "                rows.append({\n",
    "                    \"Date\": match_date,\n",
    "                    \"date\": match_date,\n",
    "                    \"gender\": (\"M\" if fx.get(\"gender\", fx.get(\"event_gender\",\"m\")).lower().startswith(\"m\") else \"W\"),\n",
    "                    \"Winner\": winner,\n",
    "                    \"Loser\": loser,\n",
    "                    \"Surface\": surface,\n",
    "                    \"winner_canonical\": normalize_name(winner),\n",
    "                    \"loser_canonical\":  normalize_name(loser),\n",
    "                    \"tournament_canonical\": normalize_tournament_name(fx.get(\"tournament_name\", \"\")),\n",
    "                    \"tennis_data_odds1\": pd.to_numeric(fx.get(\"odds_winner\") or fx.get(\"event_odds_player1\"), errors=\"coerce\"),\n",
    "                    \"tennis_data_odds2\": pd.to_numeric(fx.get(\"odds_loser\")  or fx.get(\"event_odds_player2\"), errors=\"coerce\"),\n",
    "                    \"WRank\": pd.NA,\n",
    "                    \"LRank\": pd.NA,\n",
    "                })\n",
    "            return rows\n",
    "\n",
    "        today   = date.today()\n",
    "        cutoff  = date(2025, 6, 10)                 # Jeff data stops at 10 Jun 2025\n",
    "        days    = [cutoff + timedelta(days=i) for i in range((today - cutoff).days + 1)]\n",
    "        print(f\"  Fetching API‑Tennis for {len(days)} days ({cutoff} → {today})\")\n",
    "\n",
    "        api_rows = []\n",
    "        for d in days:\n",
    "            api_rows.extend(_api_rows_for_day(d))\n",
    "\n",
    "        if api_rows:\n",
    "            api_df = pd.DataFrame(api_rows)\n",
    "\n",
    "            # Remove rows already present in tennis_data (exact winner/loser/date)\n",
    "            merged = tennis_data.merge(\n",
    "                api_df[['winner_canonical', 'loser_canonical', 'date']],\n",
    "                on=['winner_canonical', 'loser_canonical', 'date'],\n",
    "                how='inner'\n",
    "            )\n",
    "            mask_dup = api_df.set_index(['winner_canonical', 'loser_canonical', 'date']).index.isin(\n",
    "                merged.set_index(['winner_canonical', 'loser_canonical', 'date']).index\n",
    "            )\n",
    "            api_df = api_df[~mask_dup]\n",
    "\n",
    "            tennis_data = pd.concat([tennis_data, api_df], ignore_index=True)\n",
    "            print(f\"  ✓ Appended {len(api_df)} new API‑Tennis rows\")\n",
    "        else:\n",
    "            print(\"  ✓ No API‑Tennis rows to append\")\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR extracting Jeff features: {e}\")\n",
    "        return pd.DataFrame(), jeff_data, weighted_defaults\n",
    "\n",
    "    print(f\"=== DATA GENERATION COMPLETE ===\")\n",
    "    print(f\"Final data shape: {tennis_data.shape}\")\n",
    "    print(f\"Columns: {len(tennis_data.columns)}\")\n",
    "\n",
    "    return tennis_data, jeff_data, weighted_defaults\n",
    "\n",
    "# Generate the data\n",
    "# Use fast mode during development; remove fast=True for full rebuild\n",
    "print(\"Starting fresh data generation...\")\n",
    "historical_data, jeff_data, weighted_defaults = generate_comprehensive_historical_all_years_fixed(fast=True, n_sample=500)\n",
    "\n",
    "# Save to cache if successful\n",
    "if len(historical_data) > 0 and jeff_data and weighted_defaults:\n",
    "    print(\"\\n=== SAVING TO CACHE ===\")\n",
    "    try:\n",
    "        historical_data.to_parquet(HD_PATH, index=False)\n",
    "        with open(JEFF_PATH, \"wb\") as f:\n",
    "            pickle.dump(jeff_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(DEF_PATH, \"wb\") as f:\n",
    "            pickle.dump(weighted_defaults, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"✓ Data cached successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR saving cache: {e}\")\n",
    "else:\n",
    "    print(\"\\n=== CACHE NOT SAVED ===\")\n",
    "    print(\"Data generation failed - cache not created\")\n",
    "\n",
    "# Final status\n",
    "print(f\"\\n=== FINAL STATUS ===\")\n",
    "print(f\"Historical data: {historical_data.shape}\")\n",
    "print(f\"Jeff data: {bool(jeff_data)}\")\n",
    "print(f\"Weighted defaults: {bool(weighted_defaults)}\")\n",
    "\n",
    "if len(historical_data) > 0:\n",
    "    # Check Jeff features\n",
    "    jeff_cols = [col for col in historical_data.columns if col.startswith('winner_serve_pts')]\n",
    "    print(f\"Jeff feature columns: {jeff_cols}\")\n",
    "\n",
    "    if 'winner_serve_pts' in historical_data.columns:\n",
    "        non_null_count = historical_data['winner_serve_pts'].notna().sum()\n",
    "        print(f\"Non-null serve_pts values: {non_null_count}\")\n",
    "\n",
    "        # Show sample\n",
    "        sample_values = historical_data['winner_serve_pts'].dropna().head(5)\n",
    "        print(f\"Sample serve_pts: {sample_values.tolist()}\")\n",
    "\n",
    "print(\"Data generation complete!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T01:27:15.645027Z",
     "start_time": "2025-07-04T01:27:12.266135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------------------------------------------------\n",
    "# API-Tennis standalone utilities – copy into a scratch cell\n",
    "# -------------------------------------------------------------------\n",
    "import os, json, time, pickle, requests, pandas as pd\n",
    "from datetime import date, timedelta, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# API Configuration\n",
    "os.environ[\"API_TENNIS_KEY\"] = \"adfc70491c47895e5fffdc6428bbf36a561989d4bffcfa9ecfba8d91e947b4fb\"\n",
    "API_KEY = os.getenv(\"API_TENNIS_KEY\")\n",
    "BASE = \"https://api.api-tennis.com/tennis/\"\n",
    "from datetime import date\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# one-line wrapper that never crashes on missing \"result\"\n",
    "def api(method: str, **params):\n",
    "    r = requests.get(BASE, params={\"method\": method, \"APIkey\": API_KEY, **params}, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "    if str(j.get(\"error\", 0)) != \"0\":\n",
    "        raise RuntimeError(j)\n",
    "    return j.get(\"result\", [])\n",
    "\n",
    "\n",
    "# ---------- 1. finished fixtures for any single day  -------------- #\n",
    "def fixtures_finished(day: date) -> pd.DataFrame:\n",
    "    res = api(\"get_fixtures\", date_start=day, date_stop=day, timezone=\"America/New_York\")\n",
    "    rows = []\n",
    "    for ev in res:\n",
    "        if ev.get(\"event_status\") != \"Finished\":\n",
    "            continue\n",
    "        p1, p2 = ev[\"event_first_player\"], ev[\"event_second_player\"]\n",
    "        winner_flag = ev[\"event_winner\"]  # \"First Player\" / \"Second Player\"\n",
    "        winner, loser = (p1, p2) if winner_flag.startswith(\"First\") else (p2, p1)\n",
    "        rows.append({\n",
    "            \"date\": ev[\"event_date\"],\n",
    "            \"event_key\": ev[\"event_key\"],\n",
    "            \"tournament\": ev[\"tournament_name\"],\n",
    "            \"round\": ev.get(\"tournament_round\", \"\"),\n",
    "            \"surface\": ev.get(\"court_surface\", \"\"),  # string may be empty\n",
    "            \"winner\": winner,\n",
    "            \"loser\": loser,\n",
    "            \"score\": ev.get(\"scores\", \"\"),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# example\n",
    "print(\"=== finished fixtures 2025-06-25 ===\")\n",
    "print(fixtures_finished(date(2025, 6, 25)).head())\n",
    "\n",
    "# ---------- 2. static tournament → surface lookup ----------------- #\n",
    "CACHE = Path.home() / \".api_tennis_cache\"\n",
    "CACHE.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def tournament_surface_map():\n",
    "    fn = CACHE / \"tournaments.pkl\"\n",
    "    if fn.exists():\n",
    "        return pickle.loads(fn.read_bytes())\n",
    "\n",
    "    tbl = api(\"get_tournaments\")\n",
    "    m = {}\n",
    "    for t in tbl:\n",
    "        nm = t[\"tournament_name\"].lower()\n",
    "        if \"wimbledon\" in nm or \"halle\" in nm or \"queens\" in nm:\n",
    "            surf = \"Grass\"\n",
    "        elif any(x in nm for x in (\"french open\", \"roland\", \"monte carlo\", \"madrid\", \"rome\", \"clay\")):\n",
    "            surf = \"Clay\"\n",
    "        else:\n",
    "            surf = \"Hard\"\n",
    "        m[t[\"tournament_name\"]] = surf\n",
    "    fn.write_bytes(pickle.dumps(m, protocol=4))\n",
    "    return m\n",
    "\n",
    "\n",
    "surf_map = tournament_surface_map()\n",
    "print(\"\\nSurface for Rio Open ->\", surf_map.get(\"ATP Rio Open\"))\n",
    "\n",
    "\n",
    "# ---------- 3. quick H2H fetch (returns DataFrame) ---------------- #\n",
    "def h2h(player_key1: str, player_key2: str) -> pd.DataFrame:\n",
    "    tag = CACHE / f\"h2h_{player_key1}_{player_key2}.pkl\"\n",
    "    if tag.exists():\n",
    "        return pickle.loads(tag.read_bytes())\n",
    "\n",
    "    # API sometimes throttles – one retry\n",
    "    for attempt in range(2):\n",
    "        try:\n",
    "            hist = api(\"get_H2H\", first_player_key=player_key1, second_player_key=player_key2)\n",
    "            df = pd.DataFrame(hist)\n",
    "            tag.write_bytes(pickle.dumps(df, protocol=4))\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            if attempt == 0:\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "# example H2H between generic keys \"584\" and \"52\"\n",
    "# print(h2h(\"584\",\"52\").head())"
   ],
   "id": "91e6d6cd937ef5e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== finished fixtures 2025-06-25 ===\n",
      "         date  event_key            tournament  \\\n",
      "0  2025-06-25   12046056   Lima Challenger Men   \n",
      "1  2025-06-25   12046060   Lima Challenger Men   \n",
      "2  2025-06-25   12046061   Lima Challenger Men   \n",
      "3  2025-06-25   12046064  Milan Challenger Men   \n",
      "4  2025-06-25   12046065  Milan Challenger Men   \n",
      "\n",
      "                               round surface              winner  \\\n",
      "0   Lima Challenger Men - 1/8-finals               Britto/ Carou   \n",
      "1   Lima Challenger Men - 1/8-finals          Bangoura/ Stepanov   \n",
      "2   Lima Challenger Men - 1/8-finals                   Li/ Tobon   \n",
      "3  Milan Challenger Men - 1/8-finals                Goldhoff/ Ho   \n",
      "4  Milan Challenger Men - 1/8-finals           Berrettini/ Fonio   \n",
      "\n",
      "                      loser                                              score  \n",
      "0    De La Fuente/ Olivieri  [{'score_first': '6', 'score_second': '3', 'sc...  \n",
      "1           Monge/ Nakamine  [{'score_first': '6', 'score_second': '0', 'sc...  \n",
      "2         Bertran/ Tudorica  [{'score_first': '6', 'score_second': '4', 'sc...  \n",
      "3  Chandrasekar/ Ramanathan  [{'score_first': '6.3', 'score_second': '7.7',...  \n",
      "4         Bortolotti/ Ricca  [{'score_first': '5', 'score_second': '7', 'sc...  \n",
      "\n",
      "Surface for Rio Open -> None\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Quick diagnostic: row coverage by data-feed period\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "# tag each row once\n",
    "period_col = \"source_period\"\n",
    "if period_col not in historical_data.columns:\n",
    "    historical_data[period_col] = np.select(\n",
    "        [\n",
    "            historical_data['date'] <= date(2025, 6, 10),\n",
    "            historical_data['date'].between(date(2025, 6, 11), date(2025, 6, 22)),\n",
    "            historical_data['date'] >= date(2025, 6, 23)\n",
    "        ],\n",
    "        ['P1_all', 'P2_td_api', 'P3 _api'],\n",
    "        default='UNKNOWN'\n",
    "    )\n",
    "\n",
    "# simple counts\n",
    "print(\"=== rows per period ===\")\n",
    "print(historical_data.value_counts(period_col))\n",
    "\n",
    "# optional cross-tab: feed vs period\n",
    "hist_with_feed = historical_data.assign(\n",
    "    feed=np.where(historical_data['date'] >  date(2025, 6, 22), 'API',\n",
    "         np.where(historical_data['date'] <= date(2025, 6, 10), 'Jeff', 'tennis-data'))\n",
    ")\n",
    "\n",
    "print(\"\\n=== feed × period matrix ===\")\n",
    "print(pd.crosstab(hist_with_feed[period_col], hist_with_feed['feed']))\n",
    "# after generation (full or sample)\n",
    "print(\"Total rows in historical_data:\", len(historical_data))\n",
    "print(\"Rows tagged API in historical_data:\",\n",
    "      ((historical_data['date'] >= date(2025, 6, 23))).sum())"
   ],
   "id": "956c97df75bd5d90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Test Data Loading - Fixed Version\n",
    "import os, pickle, pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "CACHE_DIR = os.path.expanduser(\"~/Desktop/data/cache\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "HD_PATH   = os.path.join(CACHE_DIR, \"historical_data.parquet\")\n",
    "JEFF_PATH = os.path.join(CACHE_DIR, \"jeff_data.pkl\")\n",
    "DEF_PATH  = os.path.join(CACHE_DIR, \"weighted_defaults.pkl\")\n",
    "\n",
    "print(\"Checking cache files...\")\n",
    "print(f\"Historical data exists: {os.path.exists(HD_PATH)}\")\n",
    "print(f\"Jeff data exists: {os.path.exists(JEFF_PATH)}\")\n",
    "print(f\"Defaults exist: {os.path.exists(DEF_PATH)}\")\n",
    "\n",
    "if (os.path.exists(HD_PATH) and\n",
    "    os.path.exists(JEFF_PATH) and\n",
    "    os.path.exists(DEF_PATH)):\n",
    "    print(\"Loading cached data...\")\n",
    "    historical_data = pd.read_parquet(HD_PATH)\n",
    "    with open(JEFF_PATH, \"rb\") as f:\n",
    "        jeff_data = pickle.load(f)\n",
    "    with open(DEF_PATH, \"rb\") as f:\n",
    "        weighted_defaults = pickle.load(f)\n",
    "    print(\"✓ Cache loaded successfully\")\n",
    "else:\n",
    "    print(\"Cache miss – regenerating data...\")\n",
    "    historical_data, jeff_data, weighted_defaults = generate_comprehensive_historical_all_years()\n",
    "\n",
    "    # Save to cache\n",
    "    print(\"Saving to cache...\")\n",
    "    historical_data.to_parquet(HD_PATH, index=False)\n",
    "    with open(JEFF_PATH, \"wb\") as f:\n",
    "        pickle.dump(jeff_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(DEF_PATH, \"wb\") as f:\n",
    "        pickle.dump(weighted_defaults, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"✓ Data cached successfully\")\n",
    "\n",
    "# Check results\n",
    "print(f\"\\n=== DATA SUMMARY ===\")\n",
    "print(f\"Historical data shape: {historical_data.shape}\")\n",
    "print(f\"Jeff data keys: {list(jeff_data.keys()) if jeff_data else 'None'}\")\n",
    "print(f\"Weighted defaults keys: {list(weighted_defaults.keys()) if weighted_defaults else 'None'}\")\n",
    "\n",
    "# Check for Jeff features\n",
    "if len(historical_data) > 0:\n",
    "    jeff_feature_cols = [col for col in historical_data.columns if 'winner_serve_pts' in col or 'loser_serve_pts' in col]\n",
    "    print(f\"Jeff feature columns found: {jeff_feature_cols}\")\n",
    "\n",
    "    if 'winner_serve_pts' in historical_data.columns:\n",
    "        nulls = historical_data['winner_serve_pts'].isna().sum()\n",
    "        non_nulls = historical_data['winner_serve_pts'].notna().sum()\n",
    "        print(f\"winner_serve_pts: {non_nulls} values, {nulls} nulls\")\n",
    "\n",
    "        # Show sample values\n",
    "        sample_values = historical_data['winner_serve_pts'].dropna().head(5)\n",
    "        print(f\"Sample serve_pts values: {sample_values.tolist()}\")\n",
    "    else:\n",
    "        print(\"winner_serve_pts column not found\")\n",
    "\n",
    "    # Show some column names\n",
    "    print(f\"\\nFirst 10 columns: {historical_data.columns[:10].tolist()}\")\n",
    "    print(f\"Last 10 columns: {historical_data.columns[-10:].tolist()}\")\n",
    "else:\n",
    "    print(\"No data loaded - check the data loading function\")\n",
    "\n",
    "# Show a sample row if data exists\n",
    "if len(historical_data) > 0:\n",
    "    print(f\"\\nSample match data:\")\n",
    "    sample_row = historical_data.iloc[0]\n",
    "    print(f\"Winner: {sample_row.get('Winner', 'N/A')}\")\n",
    "    print(f\"Loser: {sample_row.get('Loser', 'N/A')}\")\n",
    "    print(f\"Date: {sample_row.get('Date', 'N/A')}\")\n",
    "    print(f\"Surface: {sample_row.get('Surface', 'N/A')}\")\n",
    "    print(f\"Winner serve pts: {sample_row.get('winner_serve_pts', 'N/A')}\")\n",
    "    print(f\"Loser serve pts: {sample_row.get('loser_serve_pts', 'N/A')}\")\n",
    "\n",
    "print(f\"\\n=== FINAL STATUS ===\")\n",
    "print(f\"Data loaded: {len(historical_data) > 0}\")\n",
    "print(f\"Jeff features available: {'winner_serve_pts' in historical_data.columns}\")\n",
    "print(f\"Ready for modeling: {len(historical_data) > 0 and 'winner_serve_pts' in historical_data.columns}\")"
   ],
   "id": "fb4aa2e5e8c59095",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5e3171b097e8a443",
   "metadata": {},
   "source": [
    "# LAYER 1 ##\n",
    "def extract_data_samples():\n",
    "    # Jeff Sackmann data samples\n",
    "    jeff_samples = {\n",
    "        'matches': jeff_data['men']['matches'].head(3),\n",
    "        'serve_basics': jeff_data['men']['serve_basics'].head(3),\n",
    "        'overview': jeff_data['men']['overview'].head(3)\n",
    "    }\n",
    "\n",
    "    # Tennis-data samples\n",
    "    tennis_samples = historical_data[\n",
    "        ['Winner', 'Loser', 'WRank', 'LRank', 'PSW', 'PSL', 'Surface']\n",
    "    ].head(3)\n",
    "\n",
    "    return jeff_samples, tennis_samples\n",
    "\n",
    "# Hold/break computation method verification\n",
    "hold_break_computation = {\n",
    "    'current_method': 'Jeff aggregated stats from overview dataset',\n",
    "    'available_columns': ['serve_pts', 'first_in', 'first_won', 'second_won'],\n",
    "    'computation_level': 'Per-player aggregate from charting data'\n",
    "}\n",
    "\n",
    "# Bayesian\n",
    "def extract_priors_from_current_data(player_canonical, gender, surface):\n",
    "    priors = {}\n",
    "\n",
    "    # Layer 1: Elo approximation from rankings\n",
    "    player_matches = historical_data[\n",
    "        (historical_data['winner_canonical'] == player_canonical) |\n",
    "        (historical_data['loser_canonical'] == player_canonical)\n",
    "    ]\n",
    "\n",
    "    if len(player_matches) > 0:\n",
    "        # Ranking-based Elo estimation\n",
    "        recent_rank = get_recent_rank(player_canonical, player_matches)\n",
    "        elo_estimate = 2000 - (recent_rank * 5) if recent_rank else 1500\n",
    "\n",
    "        # Jeff feature extraction\n",
    "        jeff_features = extract_jeff_features(player_canonical, gender, jeff_data)\n",
    "\n",
    "        priors = {\n",
    "            'elo_estimate': elo_estimate,\n",
    "            'serve_effectiveness': jeff_features.get('serve_pts', 0.6),\n",
    "            'return_strength': jeff_features.get('return_pts_won', 0.3),\n",
    "            'surface_factor': calculate_surface_adjustment(player_matches, surface)\n",
    "        }\n",
    "\n",
    "    return priors\n",
    "\n",
    "# Time decay for recent form\n",
    "def calculate_time_decayed_performance(player_matches, reference_date):\n",
    "    player_matches['days_ago'] = (reference_date - player_matches['date']).dt.days\n",
    "\n",
    "    # Exponential decay: recent matches weighted heavier\n",
    "    weights = np.exp(-0.01 * player_matches['days_ago'])  # 1% daily decay\n",
    "\n",
    "    weighted_performance = {\n",
    "        'win_rate': np.average(player_matches['is_winner'], weights=weights),\n",
    "        'games_won_rate': np.average(player_matches['games_won_pct'], weights=weights)\n",
    "    }\n",
    "\n",
    "    return weighted_performance"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c2cbbdc2ab346155",
   "metadata": {},
   "source": [
    "## TEST ##\n",
    "import os, pickle, pandas as pd\n",
    "\n",
    "CACHE_DIR = os.path.expanduser(\"~/Desktop/data/cache\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "HD_PATH   = os.path.join(CACHE_DIR, \"historical_data.parquet\")\n",
    "JEFF_PATH = os.path.join(CACHE_DIR, \"jeff_data.pkl\")\n",
    "DEF_PATH  = os.path.join(CACHE_DIR, \"weighted_defaults.pkl\")\n",
    "\n",
    "if (os.path.exists(HD_PATH) and\n",
    "    os.path.exists(JEFF_PATH) and\n",
    "    os.path.exists(DEF_PATH)):\n",
    "    print(\"Loading cached data …\")\n",
    "    historical_data = pd.read_parquet(HD_PATH)\n",
    "    with open(JEFF_PATH, \"rb\") as fh:\n",
    "        jeff_data = pickle.load(fh)\n",
    "    with open(DEF_PATH, \"rb\") as fh:\n",
    "        weighted_defaults = pickle.load(fh)\n",
    "else:\n",
    "    print(\"Cache miss – regenerating (one-time slow run).\")\n",
    "    combined_data, jeff_data, weighted_defaults = generate_comprehensive_historical_all_years()\n",
    "    historical_data = combined_data\n",
    "    historical_data.to_parquet(HD_PATH, index=False)\n",
    "    with open(JEFF_PATH, \"wb\") as fh:\n",
    "        pickle.dump(jeff_data, fh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(DEF_PATH, \"wb\") as fh:\n",
    "        pickle.dump(weighted_defaults, fh, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8d16702e22852abc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fb6b2b6eab640f01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "62e256f099bbb5fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8543b131d956f79e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "919a681cbe603498"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\"SIMULATION\"",
   "id": "d75b71c28e131e31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "81ade7c0a80f1583"
  },
  {
   "cell_type": "code",
   "id": "97b8f8ee84bce119",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalize_name_canonical(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    name = str(name).strip()\n",
    "    name = name.replace('.', '').replace(\"'\", '').replace('-', ' ')\n",
    "    return ' '.join(name.lower().split())\n",
    "\n",
    "def extract_jeff_features(player_canonical, gender, jeff_data):\n",
    "    \"\"\"Extract actual features from Jeff Sackmann data\"\"\"\n",
    "    gender_key = 'men' if gender == 'M' else 'women'\n",
    "\n",
    "    if gender_key not in jeff_data or player_canonical not in jeff_data[gender_key]:\n",
    "        return {\n",
    "            'serve_pts': 60,\n",
    "            'first_won': 0,\n",
    "            'second_won': 0,\n",
    "            'return_pts_won': 20\n",
    "        }\n",
    "\n",
    "    player_data = jeff_data[gender_key][player_canonical]\n",
    "\n",
    "    first_in = player_data.get('1stIn', 0)\n",
    "    first_won = player_data.get('1stWon', 0)\n",
    "    second_won = player_data.get('2ndWon', 0)\n",
    "    double_faults = player_data.get('df', 0)\n",
    "\n",
    "    total_serve_pts = first_in + double_faults + (first_won - first_in) if first_won >= first_in else first_in + second_won + double_faults\n",
    "\n",
    "    break_points_saved = player_data.get('bpSaved', 0)\n",
    "    break_points_faced = player_data.get('bpFaced', 0)\n",
    "    return_pts_won = break_points_faced - break_points_saved\n",
    "\n",
    "    return {\n",
    "        'serve_pts': max(1, total_serve_pts),\n",
    "        'first_won': first_won,\n",
    "        'second_won': second_won,\n",
    "        'return_pts_won': max(0, return_pts_won)\n",
    "    }\n",
    "\n",
    "class BayesianTennisModel:\n",
    "    def __init__(self):\n",
    "        self.simulation_count = 10000\n",
    "        self.jeff_data = jeff_data\n",
    "        self.historical_data = historical_data\n",
    "\n",
    "    def default_priors(self):\n",
    "        return {\n",
    "            'elo_mean': 1500,\n",
    "            'elo_std': 200,\n",
    "            'hold_prob': 0.65,\n",
    "            'break_prob': 0.35,\n",
    "            'surface': 'Hard',\n",
    "            'form_factor': 1.0,\n",
    "            'confidence': 0.1\n",
    "        }\n",
    "\n",
    "    def extract_refined_priors(self, player_canonical, gender, surface, reference_date):\n",
    "        player_matches = self.historical_data[\n",
    "            (self.historical_data['winner_canonical'] == player_canonical) |\n",
    "            (self.historical_data['loser_canonical'] == player_canonical)\n",
    "        ].copy()\n",
    "\n",
    "        if len(player_matches) == 0:\n",
    "            return self.default_priors()\n",
    "\n",
    "        surface_matches = player_matches[player_matches['Surface'] == surface]\n",
    "        if len(surface_matches) < 5:\n",
    "            surface_matches = player_matches\n",
    "\n",
    "        recent_matches = surface_matches.tail(20).copy()\n",
    "        recent_matches['days_ago'] = (pd.to_datetime(reference_date) - pd.to_datetime(recent_matches['Date'])).dt.days\n",
    "        weights = np.exp(-0.05 * recent_matches['days_ago'])\n",
    "\n",
    "        base_elo = self.get_player_weighted_elo(player_canonical, surface, reference_date)\n",
    "        surface_factor = self.calculate_surface_adaptation(player_canonical, surface)\n",
    "        elo_prior = base_elo * surface_factor\n",
    "\n",
    "        jeff_features = extract_jeff_features(player_canonical, gender, self.jeff_data)\n",
    "\n",
    "        serve_pts = jeff_features['serve_pts']\n",
    "        serve_won = jeff_features['first_won'] + jeff_features['second_won']\n",
    "        hold_prob = serve_won / serve_pts if serve_pts > 0 else 0.65\n",
    "\n",
    "        return_pts = jeff_features['return_pts_won']\n",
    "        total_return_pts = serve_pts\n",
    "        break_prob = (1 - return_pts / total_return_pts) if total_return_pts > 0 else 0.35\n",
    "\n",
    "        return {\n",
    "            'elo_mean': elo_prior,\n",
    "            'elo_std': 150,\n",
    "            'hold_prob': min(0.95, max(0.3, hold_prob)),\n",
    "            'break_prob': max(0.05, min(0.7, break_prob)),\n",
    "            'surface': surface,\n",
    "            'form_factor': self.calculate_form_spike(recent_matches, weights, player_canonical),\n",
    "            'confidence': max(0.05, min(1.0, len(recent_matches) / 15))\n",
    "        }\n",
    "\n",
    "    def calculate_ranking_differential_odds(self, p1_ranking, p2_ranking):\n",
    "        \"\"\"Convert ranking differential to implied probability\"\"\"\n",
    "        if p1_ranking == 0 or p2_ranking == 0:\n",
    "            return 0.5\n",
    "\n",
    "        ranking_diff = p2_ranking - p1_ranking\n",
    "\n",
    "        if ranking_diff > 50:\n",
    "            return 0.85\n",
    "        elif ranking_diff > 20:\n",
    "            return 0.75\n",
    "        elif ranking_diff > 10:\n",
    "            return 0.65\n",
    "        elif ranking_diff > 0:\n",
    "            return 0.55\n",
    "        elif ranking_diff > -10:\n",
    "            return 0.45\n",
    "        elif ranking_diff > -20:\n",
    "            return 0.35\n",
    "        elif ranking_diff > -50:\n",
    "            return 0.25\n",
    "        else:\n",
    "            return 0.15\n",
    "\n",
    "    def calculate_upset_frequency(self, ranking_diff, surface, historical_data):\n",
    "        \"\"\"Calculate upset frequency by ranking differential and surface\"\"\"\n",
    "        upset_matches = historical_data[\n",
    "            ((historical_data['WRank'] - historical_data['LRank']) > ranking_diff) &\n",
    "            (historical_data['Surface'] == surface)\n",
    "        ]\n",
    "\n",
    "        total_matches = historical_data[\n",
    "            (abs(historical_data['WRank'] - historical_data['LRank']) >= abs(ranking_diff)) &\n",
    "            (historical_data['Surface'] == surface)\n",
    "        ]\n",
    "\n",
    "        if len(total_matches) < 10 and surface != 'fallback':\n",
    "            return self.calculate_upset_frequency(ranking_diff, 'fallback', historical_data)\n",
    "\n",
    "        if surface == 'fallback':\n",
    "            upset_matches = historical_data[\n",
    "                (historical_data['WRank'] - historical_data['LRank']) > ranking_diff\n",
    "            ]\n",
    "            total_matches = historical_data[\n",
    "                abs(historical_data['WRank'] - historical_data['LRank']) >= abs(ranking_diff)\n",
    "            ]\n",
    "\n",
    "        if len(total_matches) == 0:\n",
    "            return 0.1\n",
    "\n",
    "        upset_rate = len(upset_matches) / len(total_matches)\n",
    "        return min(0.45, max(0.05, upset_rate))\n",
    "\n",
    "    def calculate_surface_performance_ratio(self, player_canonical, surface, opponent_canonical, reference_date):\n",
    "        \"\"\"Calculate player's surface-specific performance vs opponent's baseline\"\"\"\n",
    "        player_surface_matches = self.historical_data[\n",
    "            ((self.historical_data['winner_canonical'] == player_canonical) |\n",
    "             (self.historical_data['loser_canonical'] == player_canonical)) &\n",
    "            (self.historical_data['Surface'] == surface) &\n",
    "            (pd.to_datetime(self.historical_data['Date']) <= pd.to_datetime(reference_date))\n",
    "        ].tail(20)\n",
    "\n",
    "        opponent_surface_matches = self.historical_data[\n",
    "            ((self.historical_data['winner_canonical'] == opponent_canonical) |\n",
    "             (self.historical_data['loser_canonical'] == opponent_canonical)) &\n",
    "            (self.historical_data['Surface'] == surface) &\n",
    "            (pd.to_datetime(self.historical_data['Date']) <= pd.to_datetime(reference_date))\n",
    "        ].tail(20)\n",
    "\n",
    "        if len(player_surface_matches) < 3 or len(opponent_surface_matches) < 3:\n",
    "            return 1.0\n",
    "\n",
    "        player_wins = len(player_surface_matches[player_surface_matches['winner_canonical'] == player_canonical])\n",
    "        opponent_wins = len(opponent_surface_matches[opponent_surface_matches['winner_canonical'] == opponent_canonical])\n",
    "\n",
    "        player_ratio = player_wins / len(player_surface_matches)\n",
    "        opponent_ratio = opponent_wins / len(opponent_surface_matches)\n",
    "\n",
    "        return player_ratio / opponent_ratio if opponent_ratio > 0 else 1.0\n",
    "\n",
    "    def run_simulation(self, p1_priors, p2_priors, iterations):\n",
    "        return [self.simulate_match(p1_priors, p2_priors)]\n",
    "\n",
    "    def predict_match_outcome(self, player1_canonical, player2_canonical, surface, gender, date):\n",
    "        p1_priors = self.extract_refined_priors(player1_canonical, gender, surface, date)\n",
    "        p2_priors = self.extract_refined_priors(player2_canonical, gender, surface, date)\n",
    "\n",
    "        base_prob = self.run_simulation(p1_priors, p2_priors, 1000)[0]\n",
    "\n",
    "        p1_rank = self.get_player_ranking(player1_canonical, date)\n",
    "        p2_rank = self.get_player_ranking(player2_canonical, date)\n",
    "        ranking_prob = self.calculate_ranking_differential_odds(p1_rank, p2_rank)\n",
    "\n",
    "        ranking_diff = p1_rank - p2_rank\n",
    "        upset_adjustment = self.calculate_upset_frequency(ranking_diff, surface, self.historical_data)\n",
    "\n",
    "        surface_ratio = self.calculate_surface_performance_ratio(player1_canonical, surface, player2_canonical, date)\n",
    "\n",
    "        calibrated_prob = (0.6 * base_prob + 0.25 * ranking_prob + 0.15 * surface_ratio) * (1 - upset_adjustment * 0.1)\n",
    "\n",
    "        return max(0.05, min(0.95, calibrated_prob))\n",
    "\n",
    "    def get_player_ranking(self, player_canonical, date):\n",
    "        \"\"\"Get player ranking at specific date\"\"\"\n",
    "        date_obj = pd.to_datetime(date)\n",
    "\n",
    "        player_matches = self.historical_data[\n",
    "            ((self.historical_data['winner_canonical'] == player_canonical) |\n",
    "             (self.historical_data['loser_canonical'] == player_canonical)) &\n",
    "            (pd.to_datetime(self.historical_data['Date']) <= date_obj)\n",
    "        ].sort_values('Date', ascending=False)\n",
    "\n",
    "        if len(player_matches) == 0:\n",
    "            return 999\n",
    "\n",
    "        latest_match = player_matches.iloc[0]\n",
    "\n",
    "        if latest_match['winner_canonical'] == player_canonical:\n",
    "            return latest_match.get('WRank', 999)\n",
    "        else:\n",
    "            return latest_match.get('LRank', 999)\n",
    "\n",
    "    def calculate_match_probability(self, player1_canonical, player2_canonical, gender, surface, reference_date, best_of=3):\n",
    "        player1_priors = self.extract_refined_priors(player1_canonical, gender, surface, reference_date)\n",
    "        player2_priors = self.extract_refined_priors(player2_canonical, gender, surface, reference_date)\n",
    "\n",
    "        probability = self.simulate_match(player1_priors, player2_priors, best_of)\n",
    "        confidence = min(player1_priors['confidence'], player2_priors['confidence'])\n",
    "\n",
    "        return {\n",
    "            'player1_win_probability': probability,\n",
    "            'player2_win_probability': 1 - probability,\n",
    "            'confidence': confidence,\n",
    "            'player1_priors': player1_priors,\n",
    "            'player2_priors': player2_priors\n",
    "        }\n",
    "\n",
    "    def calculate_form_spike(self, recent_matches, weights, player_canonical):\n",
    "        if len(recent_matches) == 0:\n",
    "            return 1.0\n",
    "\n",
    "        wins = (recent_matches['winner_canonical'] == player_canonical).astype(int)\n",
    "        weighted_win_rate = np.average(wins, weights=weights)\n",
    "\n",
    "        avg_opponent_rank = recent_matches['LRank'].fillna(recent_matches['WRank']).mean()\n",
    "        player_rank = recent_matches['WRank'].fillna(recent_matches['LRank']).iloc[-1]\n",
    "\n",
    "        if pd.notna(avg_opponent_rank) and pd.notna(player_rank):\n",
    "            rank_diff = player_rank - avg_opponent_rank\n",
    "            expected_win_rate = 1 / (1 + 10**(rank_diff/400))\n",
    "            form_spike = min(2.0, weighted_win_rate / max(0.1, expected_win_rate))\n",
    "        else:\n",
    "            form_spike = 1.0\n",
    "\n",
    "        return form_spike\n",
    "\n",
    "    def simulate_match(self, player1_priors, player2_priors, best_of=3):\n",
    "        wins = 0\n",
    "        for _ in range(self.simulation_count):\n",
    "            sets_won = [0, 0]\n",
    "            while max(sets_won) < (best_of + 1) // 2:\n",
    "                set_winner = self.simulate_set(player1_priors, player2_priors)\n",
    "                sets_won[set_winner] += 1\n",
    "            if sets_won[0] > sets_won[1]:\n",
    "                wins += 1\n",
    "        return wins / self.simulation_count\n",
    "\n",
    "    def simulate_set(self, p1_priors, p2_priors):\n",
    "        games = [0, 0]\n",
    "        server = 0\n",
    "        while True:\n",
    "            hold_prob = p1_priors['hold_prob'] if server == 0 else p2_priors['hold_prob']\n",
    "            game_winner = server if np.random.random() < hold_prob else 1 - server\n",
    "            games[game_winner] += 1\n",
    "            server = 1 - server\n",
    "            if games[0] >= 6 and games[0] - games[1] >= 2:\n",
    "                return 0\n",
    "            elif games[1] >= 6 and games[1] - games[0] >= 2:\n",
    "                return 1\n",
    "            elif games[0] == 6 and games[1] == 6:\n",
    "                return self.simulate_tiebreak(p1_priors, p2_priors)\n",
    "\n",
    "    def simulate_tiebreak(self, p1_priors, p2_priors):\n",
    "        points = [0, 0]\n",
    "        server = 0\n",
    "        serve_count = 0\n",
    "        while True:\n",
    "            hold_prob = p1_priors['hold_prob'] if server == 0 else p2_priors['hold_prob']\n",
    "            point_winner = server if np.random.random() < hold_prob else 1 - server\n",
    "            points[point_winner] += 1\n",
    "            serve_count += 1\n",
    "            if serve_count == 1 or serve_count % 2 == 0:\n",
    "                server = 1 - server\n",
    "            if points[0] >= 7 and points[0] - points[1] >= 2:\n",
    "                return 0\n",
    "            elif points[1] >= 7 and points[1] - points[0] >= 2:\n",
    "                return 1\n",
    "\n",
    "    def get_player_weighted_elo(self, player_canonical, surface, reference_date):\n",
    "        recent_match = self.historical_data[\n",
    "            ((self.historical_data['winner_canonical'] == player_canonical) |\n",
    "             (self.historical_data['loser_canonical'] == player_canonical)) &\n",
    "            (self.historical_data['Surface'] == surface)\n",
    "        ].tail(1)\n",
    "\n",
    "        if len(recent_match) > 0 and 'BlendScore' in recent_match.columns:\n",
    "            blend_score = recent_match['BlendScore'].iloc[0]\n",
    "            return 1500 + blend_score * 50\n",
    "\n",
    "        any_surface_match = self.historical_data[\n",
    "            (self.historical_data['winner_canonical'] == player_canonical) |\n",
    "            (self.historical_data['loser_canonical'] == player_canonical)\n",
    "        ].tail(1)\n",
    "\n",
    "        if len(any_surface_match) > 0 and 'BlendScore' in any_surface_match.columns:\n",
    "            return 1500 + any_surface_match['BlendScore'].iloc[0] * 200\n",
    "\n",
    "        return 1500\n",
    "\n",
    "    def calculate_surface_adaptation(self, player_canonical, target_surface):\n",
    "        player_matches = self.historical_data[\n",
    "            (self.historical_data['winner_canonical'] == player_canonical) |\n",
    "            (self.historical_data['loser_canonical'] == player_canonical)\n",
    "        ].copy()\n",
    "\n",
    "        if len(player_matches) < 10:\n",
    "            return 1.0\n",
    "\n",
    "        surface_matches = player_matches[player_matches['Surface'] == target_surface]\n",
    "        if len(surface_matches) < 3:\n",
    "            return 1.0\n",
    "\n",
    "        surface_wins = (surface_matches['winner_canonical'] == player_canonical).sum()\n",
    "        surface_win_rate = surface_wins / len(surface_matches)\n",
    "\n",
    "        total_wins = (player_matches['winner_canonical'] == player_canonical).sum()\n",
    "        baseline_win_rate = total_wins / len(player_matches)\n",
    "\n",
    "        if baseline_win_rate == 0:\n",
    "            return 1.0\n",
    "\n",
    "        adaptation_ratio = surface_win_rate / baseline_win_rate\n",
    "        return max(0.7, min(1.5, adaptation_ratio))\n",
    "\n",
    "    def evaluate_predictions(self, test_data):\n",
    "        \"\"\"Evaluate model accuracy on test dataset\"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for _, match in test_data.iterrows():\n",
    "            prob = self.predict_match_outcome(\n",
    "                match['winner_canonical'],\n",
    "                match['loser_canonical'],\n",
    "                match['Surface'],\n",
    "                match['gender'],\n",
    "                match['Date']\n",
    "            )\n",
    "\n",
    "            predicted_winner = match['winner_canonical'] if prob > 0.5 else match['loser_canonical']\n",
    "            actual_winner = match['winner_canonical']\n",
    "\n",
    "            if predicted_winner == actual_winner:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "        return correct / total if total > 0 else 0\n",
    "\n",
    "def convert_to_canonical(name):\n",
    "    return normalize_name_canonical(name)\n",
    "\n",
    "model = BayesianTennisModel()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5a44a93653fa2ec3",
   "metadata": {},
   "source": [
    "## LAYER 2 ##\n",
    "def apply_contextual_adjustments(self, priors, player_canonical, opponent_canonical, match_context):\n",
    "    \"\"\"Layer 2: Contextual Bayesian adjustments for fatigue, injury, motivation\"\"\"\n",
    "\n",
    "    adjusted_priors = priors.copy()\n",
    "\n",
    "    # Fatigue Index\n",
    "    fatigue_penalty = self.calculate_fatigue_index(player_canonical, match_context['reference_date'])\n",
    "    adjusted_priors['hold_prob'] *= (1 - fatigue_penalty * 0.15)  # Max 15% hold penalty\n",
    "    adjusted_priors['elo_std'] *= (1 + fatigue_penalty * 0.3)    # Increase uncertainty\n",
    "\n",
    "    # Injury Flag Adjustment\n",
    "    injury_factor = self.get_injury_factor(player_canonical, match_context['reference_date'])\n",
    "    adjusted_priors['hold_prob'] *= injury_factor\n",
    "    adjusted_priors['break_prob'] *= (2 - injury_factor)  # Inverse relationship\n",
    "\n",
    "    # Form Spike Sustainability\n",
    "    form_sustainability = self.calculate_form_sustainability(player_canonical, match_context)\n",
    "    if adjusted_priors['form_factor'] > 1.2:  # Hot streak detection\n",
    "        sustainability_discount = 1 - ((adjusted_priors['form_factor'] - 1) * (1 - form_sustainability))\n",
    "        adjusted_priors['hold_prob'] *= sustainability_discount\n",
    "        adjusted_priors['elo_mean'] *= sustainability_discount\n",
    "\n",
    "    # Opponent Quality Weighting\n",
    "    opponent_elo = self.estimate_opponent_elo(opponent_canonical, match_context)\n",
    "    elo_diff = adjusted_priors['elo_mean'] - opponent_elo\n",
    "    quality_adjustment = 1 / (1 + np.exp(-elo_diff / 200))  # Sigmoid scaling\n",
    "    adjusted_priors['break_prob'] *= quality_adjustment\n",
    "\n",
    "    return adjusted_priors\n",
    "\n",
    "def calculate_fatigue_index(self, player_canonical, reference_date):\n",
    "    \"\"\"Fatigue based on recent match load and recovery time\"\"\"\n",
    "    recent_matches = self.get_recent_matches(player_canonical, reference_date, days=14)\n",
    "\n",
    "    if len(recent_matches) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate cumulative fatigue\n",
    "    fatigue_score = 0\n",
    "    for _, match in recent_matches.iterrows():\n",
    "        days_ago = (pd.to_datetime(reference_date) - pd.to_datetime(match['Date'])).days\n",
    "        match_duration = match.get('minutes', 120)  # Default 2 hours\n",
    "\n",
    "        # Exponential decay with match duration weighting\n",
    "        fatigue_contribution = (match_duration / 60) * np.exp(-0.1 * days_ago)\n",
    "        fatigue_score += fatigue_contribution\n",
    "\n",
    "    return min(1.0, fatigue_score / 10)  # Normalize to 0-1\n",
    "\n",
    "def get_injury_factor(self, player_canonical, reference_date):\n",
    "    \"\"\"Player-specific injury fragility scoring\"\"\"\n",
    "    # Injury memory bank - replace with actual injury tracking\n",
    "    injury_prone_players = {\n",
    "        'nadal_r': 0.85,\n",
    "        'murray_a': 0.80,\n",
    "        'thiem_d': 0.75,\n",
    "        'badosa_p': 0.70\n",
    "    }\n",
    "\n",
    "    base_factor = injury_prone_players.get(player_canonical, 0.95)\n",
    "\n",
    "    # Check for recent retirement/walkover flags\n",
    "    recent_retirements = self.check_recent_retirements(player_canonical, reference_date)\n",
    "    if recent_retirements > 0:\n",
    "        base_factor *= (0.8 ** recent_retirements)\n",
    "\n",
    "    return max(0.5, base_factor)\n",
    "\n",
    "def calculate_form_sustainability(self, player_canonical, match_context):\n",
    "    \"\"\"Form spike sustainability based on opponent quality and win quality\"\"\"\n",
    "    recent_matches = self.get_recent_matches(player_canonical, match_context['reference_date'], days=21)\n",
    "\n",
    "    if len(recent_matches) < 3:\n",
    "        return 0.5\n",
    "\n",
    "    # Quality-weighted recent performance\n",
    "    quality_scores = []\n",
    "    for _, match in recent_matches.iterrows():\n",
    "        opponent_rank = match['LRank'] if match['winner_canonical'] == player_canonical else match['WRank']\n",
    "        win_quality = 1 / (1 + opponent_rank / 100) if pd.notna(opponent_rank) else 0.5\n",
    "        quality_scores.append(win_quality)\n",
    "\n",
    "    avg_opponent_quality = np.mean(quality_scores)\n",
    "    consistency = 1 - np.std(quality_scores)\n",
    "\n",
    "    return min(1.0, avg_opponent_quality * consistency)\n",
    "\n",
    "def estimate_opponent_elo(self, opponent_canonical, match_context):\n",
    "    \"\"\"Quick opponent Elo estimation for quality weighting\"\"\"\n",
    "    opponent_priors = self.extract_refined_priors(\n",
    "        opponent_canonical,\n",
    "        match_context['gender'],\n",
    "        match_context['surface'],\n",
    "        match_context['reference_date']\n",
    "    )\n",
    "    return opponent_priors['elo_mean']\n",
    "\n",
    "def get_recent_matches(self, player_canonical, reference_date, days=14):\n",
    "    try:\n",
    "        cutoff_date = pd.to_datetime(reference_date) - pd.Timedelta(days=days)\n",
    "\n",
    "        player_matches = self.historical_data[\n",
    "            ((self.historical_data['winner_canonical'] == player_canonical) |\n",
    "             (self.historical_data['loser_canonical'] == player_canonical))\n",
    "        ].copy()\n",
    "\n",
    "        if len(player_matches) == 0:\n",
    "            return player_matches\n",
    "\n",
    "        # Force string conversion then datetime to avoid mixed types\n",
    "        player_matches['Date'] = pd.to_datetime(player_matches['Date'].astype(str), errors='coerce')\n",
    "        player_matches = player_matches.dropna(subset=['Date'])\n",
    "        player_matches = player_matches[player_matches['Date'] >= cutoff_date]\n",
    "\n",
    "        return player_matches.sort_values('Date')\n",
    "    except:\n",
    "        # Return empty DataFrame on any error\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def check_recent_retirements(self, player_canonical, reference_date):\n",
    "    \"\"\"Count recent retirements/walkovers - placeholder for actual retirement tracking\"\"\"\n",
    "    # Implementation depends on your data structure for retirement flags\n",
    "    return 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1199334327215982",
   "metadata": {},
   "source": [
    "## LAYER 3 ##\n",
    "def simulate_match(self, player1_priors, player2_priors, best_of=3, tiebreak_sets=[1,2,3]):\n",
    "    \"\"\"Layer 3: Monte Carlo match simulation with Bayesian priors\"\"\"\n",
    "\n",
    "    wins = 0\n",
    "    simulations = self.simulation_count\n",
    "\n",
    "    for _ in range(simulations):\n",
    "        sets_won = [0, 0]  # [player1, player2]\n",
    "\n",
    "        while max(sets_won) < (best_of + 1) // 2:\n",
    "            set_winner = self.simulate_set(\n",
    "                player1_priors,\n",
    "                player2_priors,\n",
    "                tiebreak=len([s for s in sets_won if s > 0]) + 1 in tiebreak_sets\n",
    "            )\n",
    "            sets_won[set_winner] += 1\n",
    "\n",
    "        if sets_won[0] > sets_won[1]:\n",
    "            wins += 1\n",
    "\n",
    "    return wins / simulations\n",
    "\n",
    "def simulate_set(self, p1_priors, p2_priors, tiebreak=True):\n",
    "    \"\"\"Simulate single set with service alternation\"\"\"\n",
    "    games = [0, 0]\n",
    "    server = 0  # 0 = player1 serves first\n",
    "\n",
    "    while True:\n",
    "        # Determine game winner based on server\n",
    "        if server == 0:\n",
    "            hold_prob = p1_priors['hold_prob']\n",
    "            game_winner = 0 if np.random.random() < hold_prob else 1\n",
    "        else:\n",
    "            hold_prob = p2_priors['hold_prob']\n",
    "            game_winner = 1 if np.random.random() < hold_prob else 0\n",
    "\n",
    "        games[game_winner] += 1\n",
    "        server = 1 - server  # Alternate serve\n",
    "\n",
    "        # Check set completion\n",
    "        if games[0] >= 6 and games[0] - games[1] >= 2:\n",
    "            return 0\n",
    "        elif games[1] >= 6 and games[1] - games[0] >= 2:\n",
    "            return 1\n",
    "        elif games[0] == 6 and games[1] == 6 and tiebreak:\n",
    "            return self.simulate_tiebreak(p1_priors, p2_priors)\n",
    "\n",
    "def simulate_tiebreak(self, p1_priors, p2_priors):\n",
    "    \"\"\"Simulate tiebreak with point-by-point serve alternation\"\"\"\n",
    "    points = [0, 0]\n",
    "    server = 0\n",
    "    serve_count = 0\n",
    "\n",
    "    while True:\n",
    "        # Determine point winner\n",
    "        if server == 0:\n",
    "            hold_prob = p1_priors['hold_prob']\n",
    "            point_winner = 0 if np.random.random() < hold_prob else 1\n",
    "        else:\n",
    "            hold_prob = p2_priors['hold_prob']\n",
    "            point_winner = 1 if np.random.random() < hold_prob else 0\n",
    "\n",
    "        points[point_winner] += 1\n",
    "        serve_count += 1\n",
    "\n",
    "        # Alternate server every 2 points (except first point)\n",
    "        if serve_count == 1 or serve_count % 2 == 0:\n",
    "            server = 1 - server\n",
    "\n",
    "        # Check tiebreak completion\n",
    "        if points[0] >= 7 and points[0] - points[1] >= 2:\n",
    "            return 0\n",
    "        elif points[1] >= 7 and points[1] - points[0] >= 2:\n",
    "            return 1\n",
    "\n",
    "def simulate_match(self, player1_priors, player2_priors, best_of=3, tiebreak_sets=[1,2,3]):\n",
    "    wins = 0\n",
    "    simulations = self.simulation_count\n",
    "\n",
    "    for _ in range(simulations):\n",
    "        sets_won = [0, 0]\n",
    "\n",
    "        while max(sets_won) < (best_of + 1) // 2:\n",
    "            set_winner = self.simulate_set(\n",
    "                player1_priors,\n",
    "                player2_priors,\n",
    "                tiebreak=len([s for s in sets_won if s > 0]) + 1 in tiebreak_sets\n",
    "            )\n",
    "            sets_won[set_winner] += 1\n",
    "\n",
    "        if sets_won[0] > sets_won[1]:\n",
    "            wins += 1\n",
    "\n",
    "    return wins / simulations\n",
    "\n",
    "def simulate_set(self, p1_priors, p2_priors, tiebreak=True):\n",
    "    games = [0, 0]\n",
    "    server = 0\n",
    "\n",
    "    while True:\n",
    "        if server == 0:\n",
    "            hold_prob = p1_priors['hold_prob']\n",
    "            game_winner = 0 if np.random.random() < hold_prob else 1\n",
    "        else:\n",
    "            hold_prob = p2_priors['hold_prob']\n",
    "            game_winner = 1 if np.random.random() < hold_prob else 0\n",
    "\n",
    "        games[game_winner] += 1\n",
    "        server = 1 - server\n",
    "\n",
    "        if games[0] >= 6 and games[0] - games[1] >= 2:\n",
    "            return 0\n",
    "        elif games[1] >= 6 and games[1] - games[0] >= 2:\n",
    "            return 1\n",
    "        elif games[0] == 6 and games[1] == 6 and tiebreak:\n",
    "            return self.simulate_tiebreak(p1_priors, p2_priors)\n",
    "\n",
    "def simulate_tiebreak(self, p1_priors, p2_priors):\n",
    "    points = [0, 0]\n",
    "    server = 0\n",
    "    serve_count = 0\n",
    "\n",
    "    while True:\n",
    "        if server == 0:\n",
    "            hold_prob = p1_priors['hold_prob']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eff82911a069f6d",
   "metadata": {},
   "source": [
    "# Tomorrow's slate\n",
    "\n",
    "import requests\n",
    "from datetime import date, timedelta\n",
    "\n",
    "API_KEY = \"adfc70491c47895e5fffdc6428bbf36a561989d4bffcfa9ecfba8d91e947b4fb\"\n",
    "BASE = \"https://api.api-tennis.com/tennis/\"\n",
    "\n",
    "def get_matches_for_date(target_date):\n",
    "    params = {\n",
    "        \"method\": \"get_fixtures\",\n",
    "        \"APIkey\": API_KEY,\n",
    "        \"date_start\": target_date,\n",
    "        \"date_stop\": target_date\n",
    "    }\n",
    "    response = requests.get(BASE, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"HTTP {response.status_code}\")\n",
    "\n",
    "    # Surface mapping\n",
    "    TOURNAMENT_SURFACES = {\n",
    "        'ATP Wimbledon': 'Grass',\n",
    "        'WTA Wimbledon': 'Grass',\n",
    "        'ATP French Open': 'Clay',\n",
    "        'WTA French Open': 'Clay',\n",
    "        'ATP US Open': 'Hard',\n",
    "        'WTA US Open': 'Hard',\n",
    "        'ATP Australian Open': 'Hard',\n",
    "        'WTA Australian Open': 'Hard'\n",
    "    }\n",
    "\n",
    "    data = response.json()\n",
    "    matches = []\n",
    "\n",
    "    for event in data.get(\"result\", []):\n",
    "        matches.append({\n",
    "            'event_key': event.get('event_key'),\n",
    "            'player1_name': event['event_first_player'],\n",
    "            'player2_name': event['event_second_player'],\n",
    "            'tournament_name': event.get('tournament_name', 'Unknown'),\n",
    "            'tournament_round': event.get('tournament_round', ''),\n",
    "            'event_status': event.get('event_status', ''),\n",
    "            'event_type_type': event.get('event_type_type', ''),\n",
    "            'surface': TOURNAMENT_SURFACES.get(event.get('tournament_name', ''), 'Unknown'),\n",
    "            'time': event.get('event_time', ''),\n",
    "            'date': event.get('event_date', '')\n",
    "        })\n",
    "\n",
    "    return matches\n",
    "\n",
    "def get_high_confidence_matches(target_date, min_confidence=0.2):\n",
    "    matches = get_matches_for_date(target_date)\n",
    "\n",
    "    results = []\n",
    "    for match in matches:\n",
    "        p1_canonical = convert_to_canonical(match['player1_name'])\n",
    "        p2_canonical = convert_to_canonical(match['player2_name'])\n",
    "\n",
    "        p1_priors = model.extract_refined_priors(p1_canonical, 'men', match['surface'], target_date)\n",
    "        p2_priors = model.extract_refined_priors(p2_canonical, 'men', match['surface'], target_date)\n",
    "\n",
    "        p1_win_prob = model.simulate_match(p1_priors, p2_priors)\n",
    "        confidence = abs(p1_win_prob - 0.5)\n",
    "\n",
    "        if confidence >= min_confidence:\n",
    "            favorite = match['player1_name'] if p1_win_prob > 0.5 else match['player2_name']\n",
    "            win_prob = max(p1_win_prob, 1 - p1_win_prob)\n",
    "\n",
    "            results.append({\n",
    "                'match': f\"{match['player1_name']} vs {match['player2_name']}\",\n",
    "                'favorite': favorite,\n",
    "                'probability': win_prob,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "\n",
    "    return sorted(results, key=lambda x: x['confidence'], reverse=True)\n",
    "\n",
    "# Usage\n",
    "today = date.today().isoformat()\n",
    "tomorrow = (date.today() + timedelta(days=1)).isoformat()\n",
    "\n",
    "todays_matches = get_matches_for_date(today)\n",
    "tomorrows_matches = get_matches_for_date(tomorrow)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "399e2e6f9701affe",
   "metadata": {},
   "source": [
    "# Todays_matches or tomorrows_matches\n",
    "todays_matches"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9ab25a7b1fb3ee6",
   "metadata": {},
   "source": [
    "# Get top 5 picks\n",
    "def get_top_confidence_matches(target_date, top_n=5, min_confidence=0.05):\n",
    "    matches = get_matches_for_date(target_date)\n",
    "\n",
    "    results = []\n",
    "    for match in matches:\n",
    "        p1_canonical = convert_to_canonical(match['player1_name'])\n",
    "        p2_canonical = convert_to_canonical(match['player2_name'])\n",
    "\n",
    "        p1_priors = model.extract_refined_priors(p1_canonical, 'men', match['surface'], target_date)\n",
    "        p2_priors = model.extract_refined_priors(p2_canonical, 'men', match['surface'], target_date)\n",
    "\n",
    "        p1_win_prob = model.simulate_match(p1_priors, p2_priors)\n",
    "        confidence = abs(p1_win_prob - 0.5)\n",
    "\n",
    "        if confidence >= min_confidence:\n",
    "            favorite = match['player1_name'] if p1_win_prob > 0.5 else match['player2_name']\n",
    "            win_prob = max(p1_win_prob, 1 - p1_win_prob)\n",
    "\n",
    "            results.append({\n",
    "                'match': f\"{match['player1_name']} vs {match['player2_name']}\",\n",
    "                'favorite': favorite,\n",
    "                'probability': win_prob,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "\n",
    "    return sorted(results, key=lambda x: x['confidence'], reverse=True)[:top_n]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    target_date = date.today().isoformat()  # today's matches\n",
    "    picks = get_top_confidence_matches(target_date, top_n=5, min_confidence=0.15)\n",
    "\n",
    "    for i, pick in enumerate(picks, 1):\n",
    "        print(f\"{i}. {pick['match']}\")\n",
    "        print(f\"   Favorite: {pick['favorite']}\")\n",
    "        print(f\"   Win Prob: {pick['probability']:.2%}\")\n",
    "        print(f\"   Confidence: {pick['confidence']:.5%}\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2b9d6f0f3cff6de",
   "metadata": {},
   "source": [
    "# See picks\n",
    "from datetime import date\n",
    "\n",
    "# get today’s top-5 at 5% confidence\n",
    "picks = get_top_confidence_matches(date.today().isoformat(), top_n=5, min_confidence=0.05)\n",
    "\n",
    "# print them\n",
    "for i, pick in enumerate(picks, 1):\n",
    "    print(f\"{i}. {pick['match']}\")\n",
    "    print(f\"   Favorite: {pick['favorite']}\")\n",
    "    print(f\"   Win Prob: {pick['probability']:.2%}\")\n",
    "    print(f\"   Confidence: {pick['confidence']:.1%}\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7280936c04122eb0",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(picks)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "636f482291cd31b9",
   "metadata": {},
   "source": [
    "# Split data chronologically\n",
    "split_date = '2023-01-01'\n",
    "train_data = historical_data[pd.to_datetime(historical_data['Date']) < split_date]\n",
    "test_data = historical_data[pd.to_datetime(historical_data['Date']) >= split_date]\n",
    "\n",
    "# Initialize model with training data\n",
    "model.historical_data = train_data\n",
    "\n",
    "# Run evaluation\n",
    "accuracy = model.evaluate_predictions(test_data.head(100))\n",
    "print(f\"Enhanced model accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Compare with baseline\n",
    "model_baseline = BayesianTennisModel()\n",
    "model_baseline.historical_data = train_data\n",
    "baseline_accuracy = model_baseline.evaluate_predictions(test_data.head(100))\n",
    "print(f\"Baseline accuracy: {baseline_accuracy:.3f}\")\n",
    "print(f\"Improvement: {accuracy - baseline_accuracy:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40b3cdc2e4eba13e",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "329d94439459a9e6",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee25387c027552f9",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24e9660711402dd",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import html\n",
    "\n",
    "class TennisAbstractScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    # Stats Overview\n",
    "    def scrape_stats_overview(self, url):\n",
    "        resp = requests.get(url, headers=self.headers)\n",
    "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "        scripts = [tag.string for tag in soup.find_all(\"script\") if tag.string]\n",
    "        all_js = \"\\n\".join(scripts)\n",
    "        blocks = dict(re.findall(r\"var\\s+(\\w+)\\s*=\\s*'([\\s\\S]*?)';\", all_js, flags=re.S))\n",
    "        labels = {span.get_text(strip=True): span[\"id\"] for span in soup.select(\"span.rounds\")}\n",
    "        sections = {label: html.unescape(blocks[token]) for label, token in labels.items() if token in blocks}\n",
    "\n",
    "        match_info = self._parse_match_url(url)\n",
    "        stats_html = sections.get(\"Stats Overview\", \"\")\n",
    "        stats_data = self._extract_stats_overview_table(stats_html)\n",
    "\n",
    "        return self._convert_to_jeff_format(stats_data, match_info)\n",
    "\n",
    "    # Serve Basics\n",
    "    def scrape_serve_basics(self, url):\n",
    "        resp = requests.get(url, headers=self.headers)\n",
    "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "        scripts = [tag.string for tag in soup.find_all(\"script\") if tag.string]\n",
    "        all_js = \"\\n\".join(scripts)\n",
    "        blocks = dict(re.findall(r\"var\\s+(\\w+)\\s*=\\s*'([\\s\\S]*?)';\", all_js, flags=re.S))\n",
    "        labels = {span.get_text(strip=True): span[\"id\"] for span in soup.select(\"span.rounds\")}\n",
    "        sections = {label: html.unescape(blocks[token]) for label, token in labels.items() if token in blocks}\n",
    "\n",
    "        match_info = self._parse_match_url(url)\n",
    "        serve_html = sections.get(\"Serve Basics\", \"\")\n",
    "        serve_data = self._parse_serve_basics(serve_html)\n",
    "\n",
    "        return self._convert_serve_basics_to_jeff(serve_data, match_info)\n",
    "\n",
    "    # add to TennisAbstractScraper\n",
    "    MAP_SERVE_INFL = {\n",
    "        'Wide %':   'serve_wide_pct',\n",
    "        'T %':      'serve_t_pct',\n",
    "        'Body %':   'serve_body_pct'\n",
    "    }\n",
    "\n",
    "    def scrape_serve_influence(self, url):\n",
    "        resp = requests.get(url, headers=self.headers)\n",
    "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "        scripts = [t.string for t in soup.find_all(\"script\") if t.string]\n",
    "        js = \"\\n\".join(scripts)\n",
    "        blocks = dict(re.findall(r\"var\\s+(\\w+)\\s*=\\s*'([\\s\\S]*?)';\", js, re.S))\n",
    "        html_block = html.unescape(blocks.get('serve', ''))\n",
    "        if not html_block:\n",
    "            return []\n",
    "\n",
    "        tbl = BeautifulSoup(html_block, 'html.parser').table\n",
    "        heads = [c.get_text(strip=True) for c in tbl.tr.find_all(['th', 'td'])]\n",
    "        out = []\n",
    "        for row in tbl.find_all('tr')[1:]:\n",
    "            cells = [c.get_text(strip=True) for c in row.find_all('td')]\n",
    "            player = cells[0]\n",
    "            rec = {'Player_canonical': self._normalize_player_name(player)}\n",
    "            for h, v in zip(heads[1:], cells[1:]):\n",
    "                key = MAP_SERVE_INFL.get(h)\n",
    "                if key:\n",
    "                    rec[key] = float(v.rstrip('%')) / 100\n",
    "            out.append(rec)\n",
    "        return out\n",
    "\n",
    "    def _parse_serve_basics(self, html_content):\n",
    "        \"\"\"Parse Serve Basics section - serves, aces, double faults breakdown\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        if not table:\n",
    "            return {}\n",
    "\n",
    "        rows = table.find_all('tr')\n",
    "        headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n",
    "\n",
    "        data_rows = []\n",
    "        for row in rows[1:]:\n",
    "            cells = [td.get_text(strip=True) for td in row.find_all('td')]\n",
    "            if len(cells) >= len(headers):\n",
    "                data_rows.append(cells)\n",
    "\n",
    "        return self._parse_serve_basics_data(headers, data_rows)\n",
    "\n",
    "    def _parse_serve_basics_data(self, headers, data_rows):\n",
    "        \"\"\"Convert Serve Basics table to structured data\"\"\"\n",
    "        stats_data = {}\n",
    "        current_set = \"Total\"\n",
    "\n",
    "        for row in data_rows:\n",
    "            if not row[0]:\n",
    "                continue\n",
    "\n",
    "            if row[0].startswith('SET'):\n",
    "                current_set = row[0]\n",
    "                continue\n",
    "\n",
    "            player_name = row[0]\n",
    "\n",
    "            if current_set not in stats_data:\n",
    "                stats_data[current_set] = {}\n",
    "\n",
    "            # Parse serve basics columns - adjust indices based on actual table structure\n",
    "            stats_data[current_set][player_name] = {\n",
    "                'serve_pts': int(row[1]) if len(row) > 1 and row[1].isdigit() else 0,\n",
    "                'aces': int(row[2]) if len(row) > 2 and row[2].isdigit() else 0,\n",
    "                'dfs': int(row[3]) if len(row) > 3 and row[3].isdigit() else 0,\n",
    "                'first_in': int(row[4]) if len(row) > 4 and row[4].isdigit() else 0,\n",
    "                'first_won': int(row[5]) if len(row) > 5 and row[5].isdigit() else 0,\n",
    "                'second_won': int(row[6]) if len(row) > 6 and row[6].isdigit() else 0\n",
    "            }\n",
    "\n",
    "        return stats_data\n",
    "\n",
    "    def _convert_serve_basics_to_jeff(self, serve_data, match_info):\n",
    "        \"\"\"Convert serve basics data to Jeff format records\"\"\"\n",
    "        jeff_records = []\n",
    "\n",
    "        for set_name, set_data in serve_data.items():\n",
    "            for player, data in set_data.items():\n",
    "                jeff_record = {\n",
    "                    'match_id': f\"{match_info['Date']}-{player.replace(' ', '_')}\",\n",
    "                    'Date': match_info['Date'],\n",
    "                    'Tournament': match_info['tournament'],\n",
    "                    'player': player,\n",
    "                    'Player_canonical': self._normalize_player_name(player),\n",
    "                    'set': set_name,\n",
    "                    'serve_pts': data['serve_pts'],\n",
    "                    'aces': data['aces'],\n",
    "                    'dfs': data['dfs'],\n",
    "                    'first_in': data['first_in'],\n",
    "                    'first_won': data['first_won'],\n",
    "                    'second_won': data['second_won']\n",
    "                }\n",
    "                jeff_records.append(jeff_record)\n",
    "\n",
    "        return jeff_records\n",
    "\n",
    "    # Existing methods unchanged\n",
    "    def _extract_stats_overview_table(self, html_content):\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        if not table:\n",
    "            return {}\n",
    "\n",
    "        rows = table.find_all('tr')\n",
    "        headers = [th.get_text(strip=True) for th in rows[0].find_all(['th', 'td'])]\n",
    "\n",
    "        data_rows = []\n",
    "        for row in rows[1:]:\n",
    "            cells = [td.get_text(strip=True) for td in row.find_all('td')]\n",
    "            if len(cells) >= len(headers):\n",
    "                data_rows.append(cells)\n",
    "\n",
    "        return self._parse_tennis_stats(headers, data_rows)\n",
    "\n",
    "    def _parse_tennis_stats(self, headers, data_rows):\n",
    "        stats_data = {}\n",
    "        current_set = \"Total\"\n",
    "\n",
    "        for row in data_rows:\n",
    "            if not row[0]:\n",
    "                continue\n",
    "\n",
    "            if row[0].startswith('SET'):\n",
    "                current_set = row[0]\n",
    "                continue\n",
    "\n",
    "            player_name = row[0]\n",
    "\n",
    "            if current_set not in stats_data:\n",
    "                stats_data[current_set] = {}\n",
    "\n",
    "            winners_text = row[8] if len(row) > 8 else \"0 (0/0)\"\n",
    "            winners_match = re.match(r'(\\d+)\\s*\\((\\d+)/(\\d+)\\)', winners_text)\n",
    "            winners_total = int(winners_match.group(1)) if winners_match else 0\n",
    "            winners_fh = int(winners_match.group(2)) if winners_match else 0\n",
    "            winners_bh = int(winners_match.group(3)) if winners_match else 0\n",
    "\n",
    "            ufe_text = row[9] if len(row) > 9 else \"0 (0/0)\"\n",
    "            ufe_match = re.match(r'(\\d+)\\s*\\((\\d+)/(\\d+)\\)', ufe_text)\n",
    "            ufe_total = int(ufe_match.group(1)) if ufe_match else 0\n",
    "            ufe_fh = int(ufe_match.group(2)) if ufe_match else 0\n",
    "            ufe_bh = int(ufe_match.group(3)) if ufe_match else 0\n",
    "\n",
    "            stats_data[current_set][player_name] = {\n",
    "                'aces_pct': row[1] if len(row) > 1 else '0%',\n",
    "                'df_pct': row[2] if len(row) > 2 else '0%',\n",
    "                'first_in_pct': row[3] if len(row) > 3 else '0%',\n",
    "                'first_won_pct': row[4] if len(row) > 4 else '0%',\n",
    "                'second_won_pct': row[5] if len(row) > 5 else '0%',\n",
    "                'bp_saved': row[6] if len(row) > 6 else '0/0',\n",
    "                'rpw_pct': row[7] if len(row) > 7 else '0%',\n",
    "                'winners': str(winners_total),\n",
    "                'winners_fh': str(winners_fh),\n",
    "                'winners_bh': str(winners_bh),\n",
    "                'ufe': str(ufe_total),\n",
    "                'ufe_fh': str(ufe_fh),\n",
    "                'ufe_bh': str(ufe_bh)\n",
    "            }\n",
    "\n",
    "        return stats_data\n",
    "\n",
    "    def _parse_match_url(self, url):\n",
    "        pattern = r'(\\d{8})-([MW])-(.+?)-(.+?)-(.+?)-(.+?)\\.html'\n",
    "        match = re.search(pattern, url)\n",
    "        if match:\n",
    "            date_str, gender, tournament, round_info, player1, player2 = match.groups()\n",
    "            return {\n",
    "                'Date': date_str,\n",
    "                'gender': 'M' if gender == 'M' else 'W',\n",
    "                'tournament': tournament.replace('_', ' '),\n",
    "                'round': round_info,\n",
    "                'player1': player1.replace('_', ' '),\n",
    "                'player2': player2.replace('_', ' ')\n",
    "            }\n",
    "        return {}\n",
    "\n",
    "    def _convert_to_jeff_format(self, stats_data, match_info):\n",
    "        jeff_records = []\n",
    "        for set_name, set_data in stats_data.items():\n",
    "            for player, data in set_data.items():\n",
    "                serve_pts = 67 if set_name == 'Total' else (40 if set_name == 'SET 1' else 27)\n",
    "\n",
    "                aces = int(float(data['aces_pct'].rstrip('%')) / 100 * serve_pts)\n",
    "                dfs = int(float(data['df_pct'].rstrip('%')) / 100 * serve_pts)\n",
    "                first_in = int(float(data['first_in_pct'].rstrip('%')) / 100 * serve_pts)\n",
    "                first_won = int(float(data['first_won_pct'].rstrip('%')) / 100 * first_in) if first_in > 0 else 0\n",
    "                second_won = int(float(data['second_won_pct'].rstrip('%')) / 100 * (serve_pts - first_in)) if (serve_pts - first_in) > 0 else 0\n",
    "\n",
    "                bp_parts = data['bp_saved'].split('/')\n",
    "                bp_saved = int(bp_parts[0])\n",
    "                bp_faced = int(bp_parts[1]) if len(bp_parts) > 1 else 0\n",
    "\n",
    "                return_pts_won = int(float(data['rpw_pct'].rstrip('%')) / 100 * serve_pts)\n",
    "\n",
    "                jeff_record = {\n",
    "                    'match_id': f\"{match_info['Date']}-{player.replace(' ', '_')}\",\n",
    "                    'Date': match_info['Date'],\n",
    "                    'Tournament': match_info['tournament'],\n",
    "                    'player': player,\n",
    "                    'Player_canonical': self._normalize_player_name(player),\n",
    "                    'set': set_name,\n",
    "                    'serve_pts': serve_pts,\n",
    "                    'aces': aces,\n",
    "                    'dfs': dfs,\n",
    "                    'first_in': first_in,\n",
    "                    'first_won': first_won,\n",
    "                    'second_won': second_won,\n",
    "                    'bp_saved': bp_saved,\n",
    "                    'bp_faced': bp_faced,\n",
    "                    'return_pts_won': return_pts_won,\n",
    "                    'winners': int(data['winners']),\n",
    "                    'winners_fh': int(data['winners_fh']),\n",
    "                    'winners_bh': int(data['winners_bh']),\n",
    "                    'unforced': int(data['ufe']),\n",
    "                    'unforced_fh': int(data['ufe_fh']),\n",
    "                    'unforced_bh': int(data['ufe_bh'])\n",
    "                }\n",
    "                jeff_records.append(jeff_record)\n",
    "        return jeff_records\n",
    "\n",
    "    def _normalize_player_name(self, name):\n",
    "        parts = name.lower().replace('.', '').split()\n",
    "        if len(parts) >= 2:\n",
    "            return f\"{parts[-1]}_{parts[0][0]}\"\n",
    "        return name.lower().replace(' ', '_')\n",
    "\n",
    "    def debug_available_sections(self, url):\n",
    "        resp = requests.get(url, headers=self.headers)\n",
    "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "        scripts = [tag.string for tag in soup.find_all(\"script\") if tag.string]\n",
    "        all_js = \"\\n\".join(scripts)\n",
    "        blocks = dict(re.findall(r\"var\\s+(\\w+)\\s*=\\s*'([\\s\\S]*?)';\", all_js, flags=re.S))\n",
    "        labels = {span.get_text(strip=True): span[\"id\"] for span in soup.select(\"span.rounds\")}\n",
    "\n",
    "        print(\"Available sections:\")\n",
    "        for label in labels.keys():\n",
    "            print(f\"- '{label}'\")\n",
    "        return labels\n",
    "\n",
    "def test_extraction_completeness(self, url):\n",
    "    \"\"\"Test all available sections and validate data structure\"\"\"\n",
    "    sections = self.debug_available_sections(url)\n",
    "\n",
    "    results = {}\n",
    "    for section_name in sections.keys():\n",
    "        try:\n",
    "            # Test each section extraction\n",
    "            extracted_data = self._test_section_extraction(url, section_name)\n",
    "            results[section_name] = len(extracted_data) > 0\n",
    "        except Exception as e:\n",
    "            results[section_name] = f\"Error: {e}\"\n",
    "\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d8d625144ae05312",
   "metadata": {},
   "source": [
    "# Test both methods\n",
    "scraper = TennisAbstractScraper()\n",
    "url = \"https://www.tennisabstract.com/charting/20250628-W-Eastbourne-F-Maya_Joint-Alexandra_Eala.html\"\n",
    "\n",
    "# Test Stats Overview\n",
    "print(\"=== STATS OVERVIEW ===\")\n",
    "overview_data = scraper.scrape_stats_overview(url)\n",
    "for record in overview_data:\n",
    "    print(record)\n",
    "\n",
    "print(\"\\n=== SERVE BASICS ===\")\n",
    "serve_data = scraper.scrape_serve_basics(url)\n",
    "for record in serve_data:\n",
    "    print(record)\n",
    "\n",
    "print(\"\\n=== SERVE INFLUENCE ===\")\n",
    "serve_infl_data = scraper.scrape_serve_influence(url)\n",
    "for record in serve_infl_data:\n",
    "    print(record)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5dd0d9a308e85d7d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
